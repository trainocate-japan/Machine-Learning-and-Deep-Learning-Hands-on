{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "6-4_(演習)PyTorchによるワインの品種分類.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNknANJwxN9BoIzOnuX/vqc"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "id": "8a83f62c",
   "cell_type": "markdown",
   "source": "<a target=\"_blank\" href=\"https://colab.research.google.com/github/trainocate-japan/Machine-Learning-and-Deep-Learning-Hands-on/blob/main/answer/6_ディープラーニング/6-4_(演習)PyTorchによるワインの品種分類.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0Ic1oCRl4eU"
   },
   "source": "# 6-3_PyTorchによるワインの品種分類\nこのノートブックでは、PyTorchで分類の予測モデルを作成します。<br>\n予測を行うテーマはwineの成分からブドウの品種を予測することです。"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWo_M2swG3es"
   },
   "source": "## ライブラリのインポート"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eqzg8G6S-ZBk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229989643,
     "user_tz": -540,
     "elapsed": 6743,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# データを処理するための基本的なライブラリ\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# scikit-learnから必要なライブラリをインポート\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets # scikit-learnからデータセットをインポートするためのライブラリ\n\n# PyTorchで使用\nimport torch\n# PyTorchのインポート設定は色々なスタイルがありますが、事前にインポートするとKerasに比べるて非常に多くのインポートを書く必要がある為、\n# 今回は使用するたびに全て記載するスタイルをとります。",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ-6ZutoLMUs"
   },
   "source": "Google Colaboratory上での出力のデフォルト設定"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tH0i-1XNLEbe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229989644,
     "user_tz": -540,
     "elapsed": 13,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# pandasのDataframeの出力\npd.set_option('display.max_columns', 500) # 表示列の最大\npd.set_option('display.max_rows', 500) # 表示行の最大\npd.set_option('display.unicode.east_asian_width', True) # 日本語出力時にヘッダのずれを解消\npd.options.display.float_format = '{:,.5f}'.format # 表示桁数の設定\n\n# ノートブックの表示桁数設定。この設定はprint文には作用せず、セルの最後に書いたものを出力する際に適用されます。\n%precision 6\n# numpy配列の指数表示禁止設定\nnp.set_printoptions(suppress=True)\n# numpy配列の表示桁数設定\nnp.set_printoptions(precision=6)",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8YM4qhJDxRA"
   },
   "source": "## データの準備"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YR4ZpobEEoSZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229989644,
     "user_tz": -540,
     "elapsed": 11,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "4f6580ce-202e-4993-c5c9-16f0873d01a3"
   },
   "source": "# データの読込\nwine_dataset = datasets.load_wine()\n# データセットのキー項目を確認\nprint(wine_dataset.keys())",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUrXjn4hQnNV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229989644,
     "user_tz": -540,
     "elapsed": 7,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "4ad345ca-17d9-4049-d022-3a3b1b350c40"
   },
   "source": "# データセットの説明はDESCRに格納されています。\nprint(wine_dataset['DESCR'])",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".. _wine_dataset:\n\nWine recognition dataset\n------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 178 (50 in each of three classes)\n    :Number of Attributes: 13 numeric, predictive attributes and the class\n    :Attribute Information:\n \t\t- Alcohol\n \t\t- Malic acid\n \t\t- Ash\n\t\t- Alcalinity of ash  \n \t\t- Magnesium\n\t\t- Total phenols\n \t\t- Flavanoids\n \t\t- Nonflavanoid phenols\n \t\t- Proanthocyanins\n\t\t- Color intensity\n \t\t- Hue\n \t\t- OD280/OD315 of diluted wines\n \t\t- Proline\n\n    - class:\n            - class_0\n            - class_1\n            - class_2\n\t\t\n    :Summary Statistics:\n    \n    ============================= ==== ===== ======= =====\n                                   Min   Max   Mean     SD\n    ============================= ==== ===== ======= =====\n    Alcohol:                      11.0  14.8    13.0   0.8\n    Malic Acid:                   0.74  5.80    2.34  1.12\n    Ash:                          1.36  3.23    2.36  0.27\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n    Magnesium:                    70.0 162.0    99.7  14.3\n    Total Phenols:                0.98  3.88    2.29  0.63\n    Flavanoids:                   0.34  5.08    2.03  1.00\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n    Proanthocyanins:              0.41  3.58    1.59  0.57\n    Colour Intensity:              1.3  13.0     5.1   2.3\n    Hue:                          0.48  1.71    0.96  0.23\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n    Proline:                       278  1680     746   315\n    ============================= ==== ===== ======= =====\n\n    :Missing Attribute Values: None\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThis is a copy of UCI ML Wine recognition datasets.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n\nThe data is the results of a chemical analysis of wines grown in the same\nregion in Italy by three different cultivators. There are thirteen different\nmeasurements taken for different constituents found in the three types of\nwine.\n\nOriginal Owners: \n\nForina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies,\nVia Brigata Salerno, 16147 Genoa, Italy.\n\nCitation:\n\nLichman, M. (2013). UCI Machine Learning Repository\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\nSchool of Information and Computer Science. \n\n.. topic:: References\n\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \n  Comparison of Classifiers in High Dimensional Settings, \n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n  Mathematics and Statistics, James Cook University of North Queensland. \n  (Also submitted to Technometrics). \n\n  The data was used with many others for comparing various \n  classifiers. The classes are separable, though only RDA \n  has achieved 100% correct classification. \n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n  (All results using the leave-one-out technique) \n\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n  Mathematics and Statistics, James Cook University of North Queensland. \n  (Also submitted to Journal of Chemometrics).\n\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPq8eVOtGkWc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229989644,
     "user_tz": -540,
     "elapsed": 5,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "ed5a3969-e459-47ea-85ff-ea18f89f1ec2"
   },
   "source": "# 目的変数となる変数名と、説明変数となる変数名を確認する\nprint(wine_dataset['target_names'])\nprint(wine_dataset['feature_names'])",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['class_0' 'class_1' 'class_2']\n['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scdbmPTaltHq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990070,
     "user_tz": -540,
     "elapsed": 429,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "f2b97211-9735-40fe-d908-f427884c7b51"
   },
   "source": "# 説明変数となるdataと目的変数となるtargetを確認する\nprint(type(wine_dataset['data']))\nprint(wine_dataset['data'].shape)\nprint(type(wine_dataset['target']))\nprint(wine_dataset['target'].shape)\n# 178行のデータがそれぞれ格納されている",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n(178, 13)\n<class 'numpy.ndarray'>\n(178,)\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "vBg_vl4VJ6-S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990070,
     "user_tz": -540,
     "elapsed": 10,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "d4d6b076-3e33-4570-a870-7e32b44e3f59"
   },
   "source": "# 説明変数と目的変数をpandasのDataFrameに変換して列名をつけて取り出す\nwine_dataset_x = pd.DataFrame(wine_dataset['data'], columns=wine_dataset['feature_names'])\nwine_dataset_y = pd.DataFrame(wine_dataset['target'], columns=['Class'])\ndisplay(wine_dataset_x.head(3))\ndisplay(wine_dataset_y.head(3))",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   alcohol  malic_acid     ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0 14.23000     1.71000 2.43000           15.60000  127.00000        2.80000   \n1 13.20000     1.78000 2.14000           11.20000  100.00000        2.65000   \n2 13.16000     2.36000 2.67000           18.60000  101.00000        2.80000   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity     hue  \\\n0     3.06000               0.28000          2.29000          5.64000 1.04000   \n1     2.76000               0.26000          1.28000          4.38000 1.05000   \n2     3.24000               0.30000          2.81000          5.68000 1.03000   \n\n   od280/od315_of_diluted_wines     proline  \n0                       3.92000 1,065.00000  \n1                       3.40000 1,050.00000  \n2                       3.17000 1,185.00000  ",
      "text/html": "\n  <div id=\"df-1ef2aa01-3d50-4216-8161-1ae89cdc0580\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23000</td>\n      <td>1.71000</td>\n      <td>2.43000</td>\n      <td>15.60000</td>\n      <td>127.00000</td>\n      <td>2.80000</td>\n      <td>3.06000</td>\n      <td>0.28000</td>\n      <td>2.29000</td>\n      <td>5.64000</td>\n      <td>1.04000</td>\n      <td>3.92000</td>\n      <td>1,065.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20000</td>\n      <td>1.78000</td>\n      <td>2.14000</td>\n      <td>11.20000</td>\n      <td>100.00000</td>\n      <td>2.65000</td>\n      <td>2.76000</td>\n      <td>0.26000</td>\n      <td>1.28000</td>\n      <td>4.38000</td>\n      <td>1.05000</td>\n      <td>3.40000</td>\n      <td>1,050.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16000</td>\n      <td>2.36000</td>\n      <td>2.67000</td>\n      <td>18.60000</td>\n      <td>101.00000</td>\n      <td>2.80000</td>\n      <td>3.24000</td>\n      <td>0.30000</td>\n      <td>2.81000</td>\n      <td>5.68000</td>\n      <td>1.03000</td>\n      <td>3.17000</td>\n      <td>1,185.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ef2aa01-3d50-4216-8161-1ae89cdc0580')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n        \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n      \n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-1ef2aa01-3d50-4216-8161-1ae89cdc0580 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-1ef2aa01-3d50-4216-8161-1ae89cdc0580');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n  "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   Class\n0      0\n1      0\n2      0",
      "text/html": "\n  <div id=\"df-6b94b462-69ea-416d-95d0-f1fa41c85a11\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b94b462-69ea-416d-95d0-f1fa41c85a11')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n        \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n      \n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-6b94b462-69ea-416d-95d0-f1fa41c85a11 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-6b94b462-69ea-416d-95d0-f1fa41c85a11');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n  "
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "BYknsfuGMTmv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990344,
     "user_tz": -540,
     "elapsed": 282,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "837d70fa-11c0-411d-c505-6875f57aa019"
   },
   "source": "# データの概要を把握します。\n# 説明変数と目的変数のDataFrameを結合します。\nwine = pd.concat([wine_dataset_x, wine_dataset_y], axis=1)\n\nprint('\\n--要約--\\n')\ndisplay(wine.info())\nprint('\\n--統計情報--\\n')\ndisplay(wine.describe())",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n--要約--\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   alcohol                       178 non-null    float64\n 1   malic_acid                    178 non-null    float64\n 2   ash                           178 non-null    float64\n 3   alcalinity_of_ash             178 non-null    float64\n 4   magnesium                     178 non-null    float64\n 5   total_phenols                 178 non-null    float64\n 6   flavanoids                    178 non-null    float64\n 7   nonflavanoid_phenols          178 non-null    float64\n 8   proanthocyanins               178 non-null    float64\n 9   color_intensity               178 non-null    float64\n 10  hue                           178 non-null    float64\n 11  od280/od315_of_diluted_wines  178 non-null    float64\n 12  proline                       178 non-null    float64\n 13  Class                         178 non-null    int64  \ndtypes: float64(13), int64(1)\nmemory usage: 19.6 KB\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "None"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n--統計情報--\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\ncount 178.00000   178.00000 178.00000          178.00000  178.00000   \nmean   13.00062     2.33635   2.36652           19.49494   99.74157   \nstd     0.81183     1.11715   0.27434            3.33956   14.28248   \nmin    11.03000     0.74000   1.36000           10.60000   70.00000   \n25%    12.36250     1.60250   2.21000           17.20000   88.00000   \n50%    13.05000     1.86500   2.36000           19.50000   98.00000   \n75%    13.67750     3.08250   2.55750           21.50000  107.00000   \nmax    14.83000     5.80000   3.23000           30.00000  162.00000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount      178.00000   178.00000             178.00000        178.00000   \nmean         2.29511     2.02927               0.36185          1.59090   \nstd          0.62585     0.99886               0.12445          0.57236   \nmin          0.98000     0.34000               0.13000          0.41000   \n25%          1.74250     1.20500               0.27000          1.25000   \n50%          2.35500     2.13500               0.34000          1.55500   \n75%          2.80000     2.87500               0.43750          1.95000   \nmax          3.88000     5.08000               0.66000          3.58000   \n\n       color_intensity       hue  od280/od315_of_diluted_wines     proline  \\\ncount        178.00000 178.00000                     178.00000   178.00000   \nmean           5.05809   0.95745                       2.61169   746.89326   \nstd            2.31829   0.22857                       0.70999   314.90747   \nmin            1.28000   0.48000                       1.27000   278.00000   \n25%            3.22000   0.78250                       1.93750   500.50000   \n50%            4.69000   0.96500                       2.78000   673.50000   \n75%            6.20000   1.12000                       3.17000   985.00000   \nmax           13.00000   1.71000                       4.00000 1,680.00000   \n\n          Class  \ncount 178.00000  \nmean    0.93820  \nstd     0.77503  \nmin     0.00000  \n25%     0.00000  \n50%     1.00000  \n75%     2.00000  \nmax     2.00000  ",
      "text/html": "\n  <div id=\"df-99843c3d-50a5-462b-8fdb-71c7ad017d7a\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n      <td>178.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.00062</td>\n      <td>2.33635</td>\n      <td>2.36652</td>\n      <td>19.49494</td>\n      <td>99.74157</td>\n      <td>2.29511</td>\n      <td>2.02927</td>\n      <td>0.36185</td>\n      <td>1.59090</td>\n      <td>5.05809</td>\n      <td>0.95745</td>\n      <td>2.61169</td>\n      <td>746.89326</td>\n      <td>0.93820</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.81183</td>\n      <td>1.11715</td>\n      <td>0.27434</td>\n      <td>3.33956</td>\n      <td>14.28248</td>\n      <td>0.62585</td>\n      <td>0.99886</td>\n      <td>0.12445</td>\n      <td>0.57236</td>\n      <td>2.31829</td>\n      <td>0.22857</td>\n      <td>0.70999</td>\n      <td>314.90747</td>\n      <td>0.77503</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>11.03000</td>\n      <td>0.74000</td>\n      <td>1.36000</td>\n      <td>10.60000</td>\n      <td>70.00000</td>\n      <td>0.98000</td>\n      <td>0.34000</td>\n      <td>0.13000</td>\n      <td>0.41000</td>\n      <td>1.28000</td>\n      <td>0.48000</td>\n      <td>1.27000</td>\n      <td>278.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.36250</td>\n      <td>1.60250</td>\n      <td>2.21000</td>\n      <td>17.20000</td>\n      <td>88.00000</td>\n      <td>1.74250</td>\n      <td>1.20500</td>\n      <td>0.27000</td>\n      <td>1.25000</td>\n      <td>3.22000</td>\n      <td>0.78250</td>\n      <td>1.93750</td>\n      <td>500.50000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.05000</td>\n      <td>1.86500</td>\n      <td>2.36000</td>\n      <td>19.50000</td>\n      <td>98.00000</td>\n      <td>2.35500</td>\n      <td>2.13500</td>\n      <td>0.34000</td>\n      <td>1.55500</td>\n      <td>4.69000</td>\n      <td>0.96500</td>\n      <td>2.78000</td>\n      <td>673.50000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.67750</td>\n      <td>3.08250</td>\n      <td>2.55750</td>\n      <td>21.50000</td>\n      <td>107.00000</td>\n      <td>2.80000</td>\n      <td>2.87500</td>\n      <td>0.43750</td>\n      <td>1.95000</td>\n      <td>6.20000</td>\n      <td>1.12000</td>\n      <td>3.17000</td>\n      <td>985.00000</td>\n      <td>2.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>14.83000</td>\n      <td>5.80000</td>\n      <td>3.23000</td>\n      <td>30.00000</td>\n      <td>162.00000</td>\n      <td>3.88000</td>\n      <td>5.08000</td>\n      <td>0.66000</td>\n      <td>3.58000</td>\n      <td>13.00000</td>\n      <td>1.71000</td>\n      <td>4.00000</td>\n      <td>1,680.00000</td>\n      <td>2.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99843c3d-50a5-462b-8fdb-71c7ad017d7a')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n        \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n      \n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-99843c3d-50a5-462b-8fdb-71c7ad017d7a button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-99843c3d-50a5-462b-8fdb-71c7ad017d7a');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n  "
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pI1jf8LMJ-1_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990344,
     "user_tz": -540,
     "elapsed": 5,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "d4857e3b-c334-4788-f29b-5db6908a607b"
   },
   "source": "# 目的変数を確認してみる\nprint(wine_dataset['target'])\n# 0,1,2は先ほど表示したfeature_namesに対応している。(今回は品種名までは書かれていない)\n# 0:class_0、1:class_1、2:class_2\n# ★PyTorchの多値分類では正解データを0,1,2,・・・というラベルの表現のまま渡すので、one-hot表現は不要",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gPBJe7AwBorC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990344,
     "user_tz": -540,
     "elapsed": 5,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# 訓練データと検証データに分割（80%を訓練用に使用）\ntrain_x, test_x, train_y, test_y = train_test_split(wine_dataset_x, wine_dataset['target'], train_size=0.8, test_size=0.2, random_state=0, stratify=wine_dataset['target']) \n# さらに訓練データを検証データに分割（訓練データの20%を検証用に使用）\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, train_size=0.8, test_size=0.2, random_state=0) ",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y5vwfnuFZWBG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990345,
     "user_tz": -540,
     "elapsed": 5,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "d9da42d8-12bc-4f29-9128-8f9109bbe568"
   },
   "source": "# 標準化を行う\n# 標準化はscikit-learnに用意されている\nscaler_x = StandardScaler()\nscaler_x.fit(train_x)\ntrain_x_scaled = scaler_x.transform(train_x)\nval_x_scaled = scaler_x.transform(val_x)\ntest_x_scaled = scaler_x.transform(test_x)\n\nprint(type(train_x_scaled))\nprint(train_x_scaled[:2])\nprint(type(val_x_scaled))\nprint(val_x_scaled[:2])\nprint(type(test_x_scaled))\nprint(test_x_scaled[:2])\n# scikit-learnのStandardScalerを使用するとnumpy配列になるので、説明変数はpandasのデータフレームからnumpyへの変換は不要\n# 目的変数もデータセットから直接取得している場合にはnumpy配列なので変換は不要",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n[[ 0.46426   1.337911 -0.959912 -0.263148 -0.654115  0.245045  0.604217\n  -0.767896 -0.171102 -0.317867 -0.222283  0.519187  0.811788]\n [ 0.670801  0.212472  1.102394  1.420004  0.412993 -1.192156 -1.18333\n   0.237188 -0.065545  1.596764 -0.968013 -1.197928 -0.045268]]\n<class 'numpy.ndarray'>\n[[ 0.646502 -0.564616 -0.272477 -1.030901  1.266679  1.362867  1.196828\n  -0.181597  1.341884  0.490824 -0.046817  1.06294   0.090057]\n [ 0.719399  2.329369 -0.127753  0.0912   -0.511834 -0.473555 -1.22219\n   0.907244 -0.980374 -0.265412 -0.222283 -0.840196 -0.64671 ]]\n<class 'numpy.ndarray'>\n[[-1.042274 -0.79685   0.523501 -0.20409   0.341852 -0.649213 -0.299271\n   0.73973  -0.962781 -0.899251  2.146507 -0.582629 -1.233117]\n [ 0.306317 -0.627141  1.645106 -1.237603  0.768695  0.484578  0.604217\n  -0.181597 -0.382217 -0.177986  0.567314  0.204382  0.34567 ]]\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFs0k_b7ocDI"
   },
   "source": "#### データをtorchtensorに変換する"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G6VCLmF-qloa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990345,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# numpy配列をテンソルに変換\n# ★多値分類では回帰とはデータの準備が異なる部分があります。 \n# 目的変数はone-hot表現にはせず、N行1列の形にもせず、1次配列の形で用意します。\ntrain_x_scaled = torch.Tensor(train_x_scaled).float()\ntrain_y = torch.Tensor(train_y).long()\nval_x_scaled = torch.Tensor(val_x_scaled).float()\nval_y = torch.Tensor(val_y).long()\ntest_x_scaled = torch.Tensor(test_x_scaled).float()\ntest_y = torch.Tensor(test_y).long()\n## データをテンソルデータセットインスタンスにする\ntrain_dataset = torch.utils.data.TensorDataset(train_x_scaled, train_y)\nval_dataset = torch.utils.data.TensorDataset(val_x_scaled, val_y)\ntest_dataset = torch.utils.data.TensorDataset(test_x_scaled, test_y)",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV-NTZKhD1Mi"
   },
   "source": "## モデルの定義"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aclNPCf9ZabB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652229990345,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# 乱数シードの固定\ntorch.manual_seed(0)\n\n# ニューラルネットワークを定義\nclass Net(torch.nn.Module):\n\n  # 必要な層や活性化関数を定義する\n  def __init__(self):\n    super(Net, self).__init__()\n    self.l1 = torch.nn.Linear(train_x.shape[1], 128)     # 中間層1\n    self.a1 = torch.nn.ReLU()  # 活性化関数1\n    self.l2 = torch.nn.Linear(128, 128)   # 中間層2\n    self.a2 = torch.nn.ReLU()  # 活性化関数2\n    self.l3 = torch.nn.Linear(128, 3)     # 出力層\n\n  # 順伝搬を定義。引数のxは、説明変数。\n  # 順番に関数を実行し、その結果を次の関数に渡していく\n  def forward(self, x):\n    x = self.l1(x)\n    x = self.a1(x)\n    x = self.l2(x)\n    x = self.a2(x)\n    x = self.l3(x)\n    return x",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOJjLVWbfE8b"
   },
   "source": "## 学習"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6R7izz7QsTcx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652230001908,
     "user_tz": -540,
     "elapsed": 11566,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "e366acd6-86cc-4040-c4a8-7d7578dfe6a5"
   },
   "source": "num_epochs = 700\n\n# データローダーの用意\n# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n# shuffleをTrueに設定することで、データをシャッフルして取り出します(★batch_sizeをデータ量に合わせて32に設定)\nbatch_size = 32\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# モデルをインスタンス化\nnet = Net()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001) # 最適化手法の用意\ncriterion = torch.nn.CrossEntropyLoss() # ★誤差関数の用意(多値分類なので、交差エントロピー誤差を使う)\n\n## 学習時に経過情報を保存する空リストを作成\ntrain_loss_list = []      # 学習データの誤差関数用リスト\nval_loss_list = []        # 検証データの誤差関数用リスト\n\n# エポック分の繰り返し\nfor epoch in range(num_epochs):\n    \n    #学習の進行状況を表示\n    print('--------')\n    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n\n    # 損失の初期化\n    train_loss = 0        # 学習データの誤差関数\n    val_loss = 0          # 検証データの誤差関数\n    \n    #=====学習パート=======\n    # 学習モードに設定\n    # PyTorchでは学習時と評価時でモードを切り替える\n    net.train()\n\n    #ミニバッチごとにデータをロードして学習\n    for x, y in train_dataloader:\n        preds = net(x)                            # 順伝搬で予測を実行\n        loss = criterion(preds, y)                # 誤差関数を計算\n        optimizer.zero_grad()                     # 勾配を初期化\n        loss.backward()                           # 勾配を計算\n        optimizer.step()                          # パラメータ更新\n        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n    #ミニバッチの平均の損失を計算\n    batch_train_loss = train_loss / len(train_dataloader)\n    \n    #=====評価パート(検証データ)=======\n    # 評価モードに設定\n    net.eval()\n    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n    with torch.no_grad():\n        for x, y in val_dataloader:\n            preds = net(x)                        # 順伝搬で予測を実行\n            loss = criterion(preds, y)            # 誤差関数を計算\n            val_loss += loss.item()               # ミニバッチごとの損失を格納    \n    #ミニバッチの平均の損失を計算\n    batch_val_loss = val_loss / len(val_dataloader)\n    \n    #エポックごとに損失を表示\n    print(\"Train_Loss: {:.4f}\".format(batch_train_loss))\n    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n    #損失をリスト化して保存\n    train_loss_list.append(batch_train_loss)\n    val_loss_list.append(batch_val_loss)",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------\nEpoch: 1/700\nTrain_Loss: 1.0410\nval_loss: 0.9546\n--------\nEpoch: 2/700\nTrain_Loss: 0.8869\nval_loss: 0.8241\n--------\nEpoch: 3/700\nTrain_Loss: 0.7407\nval_loss: 0.6926\n--------\nEpoch: 4/700\nTrain_Loss: 0.5889\nval_loss: 0.5609\n--------\nEpoch: 5/700\nTrain_Loss: 0.4645\nval_loss: 0.4381\n--------\nEpoch: 6/700\nTrain_Loss: 0.3426\nval_loss: 0.3328\n--------\nEpoch: 7/700\nTrain_Loss: 0.2379\nval_loss: 0.2521\n--------\nEpoch: 8/700\nTrain_Loss: 0.1794\nval_loss: 0.1941\n--------\nEpoch: 9/700\nTrain_Loss: 0.1239\nval_loss: 0.1556\n--------\nEpoch: 10/700\nTrain_Loss: 0.0949\nval_loss: 0.1306\n--------\nEpoch: 11/700\nTrain_Loss: 0.0697\nval_loss: 0.1164\n--------\nEpoch: 12/700\nTrain_Loss: 0.0543\nval_loss: 0.1057\n--------\nEpoch: 13/700\nTrain_Loss: 0.0454\nval_loss: 0.0984\n--------\nEpoch: 14/700\nTrain_Loss: 0.0336\nval_loss: 0.0935\n--------\nEpoch: 15/700\nTrain_Loss: 0.0293\nval_loss: 0.0911\n--------\nEpoch: 16/700\nTrain_Loss: 0.0233\nval_loss: 0.0855\n--------\nEpoch: 17/700\nTrain_Loss: 0.0219\nval_loss: 0.0814\n--------\nEpoch: 18/700\nTrain_Loss: 0.0189\nval_loss: 0.0802\n--------\nEpoch: 19/700\nTrain_Loss: 0.0158\nval_loss: 0.0826\n--------\nEpoch: 20/700\nTrain_Loss: 0.0151\nval_loss: 0.0822\n--------\nEpoch: 21/700\nTrain_Loss: 0.0139\nval_loss: 0.0836\n--------\nEpoch: 22/700\nTrain_Loss: 0.0122\nval_loss: 0.0815\n--------\nEpoch: 23/700\nTrain_Loss: 0.0111\nval_loss: 0.0781\n--------\nEpoch: 24/700\nTrain_Loss: 0.0096\nval_loss: 0.0790\n--------\nEpoch: 25/700\nTrain_Loss: 0.0094\nval_loss: 0.0795\n--------\nEpoch: 26/700\nTrain_Loss: 0.0079\nval_loss: 0.0791\n--------\nEpoch: 27/700\nTrain_Loss: 0.0073\nval_loss: 0.0788\n--------\nEpoch: 28/700\nTrain_Loss: 0.0075\nval_loss: 0.0787\n--------\nEpoch: 29/700\nTrain_Loss: 0.0065\nval_loss: 0.0814\n--------\nEpoch: 30/700\nTrain_Loss: 0.0059\nval_loss: 0.0838\n--------\nEpoch: 31/700\nTrain_Loss: 0.0055\nval_loss: 0.0850\n--------\nEpoch: 32/700\nTrain_Loss: 0.0052\nval_loss: 0.0849\n--------\nEpoch: 33/700\nTrain_Loss: 0.0050\nval_loss: 0.0854\n--------\nEpoch: 34/700\nTrain_Loss: 0.0049\nval_loss: 0.0850\n--------\nEpoch: 35/700\nTrain_Loss: 0.0043\nval_loss: 0.0828\n--------\nEpoch: 36/700\nTrain_Loss: 0.0048\nval_loss: 0.0813\n--------\nEpoch: 37/700\nTrain_Loss: 0.0037\nval_loss: 0.0815\n--------\nEpoch: 38/700\nTrain_Loss: 0.0040\nval_loss: 0.0817\n--------\nEpoch: 39/700\nTrain_Loss: 0.0034\nval_loss: 0.0842\n--------\nEpoch: 40/700\nTrain_Loss: 0.0037\nval_loss: 0.0863\n--------\nEpoch: 41/700\nTrain_Loss: 0.0031\nval_loss: 0.0863\n--------\nEpoch: 42/700\nTrain_Loss: 0.0030\nval_loss: 0.0860\n--------\nEpoch: 43/700\nTrain_Loss: 0.0031\nval_loss: 0.0874\n--------\nEpoch: 44/700\nTrain_Loss: 0.0029\nval_loss: 0.0875\n--------\nEpoch: 45/700\nTrain_Loss: 0.0029\nval_loss: 0.0889\n--------\nEpoch: 46/700\nTrain_Loss: 0.0023\nval_loss: 0.0902\n--------\nEpoch: 47/700\nTrain_Loss: 0.0022\nval_loss: 0.0903\n--------\nEpoch: 48/700\nTrain_Loss: 0.0022\nval_loss: 0.0900\n--------\nEpoch: 49/700\nTrain_Loss: 0.0022\nval_loss: 0.0886\n--------\nEpoch: 50/700\nTrain_Loss: 0.0022\nval_loss: 0.0880\n--------\nEpoch: 51/700\nTrain_Loss: 0.0022\nval_loss: 0.0875\n--------\nEpoch: 52/700\nTrain_Loss: 0.0018\nval_loss: 0.0867\n--------\nEpoch: 53/700\nTrain_Loss: 0.0019\nval_loss: 0.0869\n--------\nEpoch: 54/700\nTrain_Loss: 0.0019\nval_loss: 0.0862\n--------\nEpoch: 55/700\nTrain_Loss: 0.0018\nval_loss: 0.0859\n--------\nEpoch: 56/700\nTrain_Loss: 0.0016\nval_loss: 0.0867\n--------\nEpoch: 57/700\nTrain_Loss: 0.0015\nval_loss: 0.0874\n--------\nEpoch: 58/700\nTrain_Loss: 0.0015\nval_loss: 0.0876\n--------\nEpoch: 59/700\nTrain_Loss: 0.0014\nval_loss: 0.0883\n--------\nEpoch: 60/700\nTrain_Loss: 0.0013\nval_loss: 0.0885\n--------\nEpoch: 61/700\nTrain_Loss: 0.0014\nval_loss: 0.0889\n--------\nEpoch: 62/700\nTrain_Loss: 0.0014\nval_loss: 0.0903\n--------\nEpoch: 63/700\nTrain_Loss: 0.0013\nval_loss: 0.0911\n--------\nEpoch: 64/700\nTrain_Loss: 0.0012\nval_loss: 0.0921\n--------\nEpoch: 65/700\nTrain_Loss: 0.0011\nval_loss: 0.0933\n--------\nEpoch: 66/700\nTrain_Loss: 0.0010\nval_loss: 0.0939\n--------\nEpoch: 67/700\nTrain_Loss: 0.0012\nval_loss: 0.0943\n--------\nEpoch: 68/700\nTrain_Loss: 0.0011\nval_loss: 0.0941\n--------\nEpoch: 69/700\nTrain_Loss: 0.0011\nval_loss: 0.0939\n--------\nEpoch: 70/700\nTrain_Loss: 0.0010\nval_loss: 0.0945\n--------\nEpoch: 71/700\nTrain_Loss: 0.0010\nval_loss: 0.0950\n--------\nEpoch: 72/700\nTrain_Loss: 0.0009\nval_loss: 0.0959\n--------\nEpoch: 73/700\nTrain_Loss: 0.0010\nval_loss: 0.0956\n--------\nEpoch: 74/700\nTrain_Loss: 0.0009\nval_loss: 0.0957\n--------\nEpoch: 75/700\nTrain_Loss: 0.0009\nval_loss: 0.0957\n--------\nEpoch: 76/700\nTrain_Loss: 0.0009\nval_loss: 0.0961\n--------\nEpoch: 77/700\nTrain_Loss: 0.0007\nval_loss: 0.0967\n--------\nEpoch: 78/700\nTrain_Loss: 0.0008\nval_loss: 0.0969\n--------\nEpoch: 79/700\nTrain_Loss: 0.0007\nval_loss: 0.0977\n--------\nEpoch: 80/700\nTrain_Loss: 0.0008\nval_loss: 0.0980\n--------\nEpoch: 81/700\nTrain_Loss: 0.0008\nval_loss: 0.0980\n--------\nEpoch: 82/700\nTrain_Loss: 0.0007\nval_loss: 0.0985\n--------\nEpoch: 83/700\nTrain_Loss: 0.0008\nval_loss: 0.0984\n--------\nEpoch: 84/700\nTrain_Loss: 0.0006\nval_loss: 0.0985\n--------\nEpoch: 85/700\nTrain_Loss: 0.0006\nval_loss: 0.0982\n--------\nEpoch: 86/700\nTrain_Loss: 0.0006\nval_loss: 0.0984\n--------\nEpoch: 87/700\nTrain_Loss: 0.0006\nval_loss: 0.0983\n--------\nEpoch: 88/700\nTrain_Loss: 0.0006\nval_loss: 0.0980\n--------\nEpoch: 89/700\nTrain_Loss: 0.0006\nval_loss: 0.0980\n--------\nEpoch: 90/700\nTrain_Loss: 0.0006\nval_loss: 0.0987\n--------\nEpoch: 91/700\nTrain_Loss: 0.0005\nval_loss: 0.0990\n--------\nEpoch: 92/700\nTrain_Loss: 0.0005\nval_loss: 0.0990\n--------\nEpoch: 93/700\nTrain_Loss: 0.0005\nval_loss: 0.0991\n--------\nEpoch: 94/700\nTrain_Loss: 0.0006\nval_loss: 0.0992\n--------\nEpoch: 95/700\nTrain_Loss: 0.0005\nval_loss: 0.0991\n--------\nEpoch: 96/700\nTrain_Loss: 0.0005\nval_loss: 0.0991\n--------\nEpoch: 97/700\nTrain_Loss: 0.0005\nval_loss: 0.0995\n--------\nEpoch: 98/700\nTrain_Loss: 0.0005\nval_loss: 0.0996\n--------\nEpoch: 99/700\nTrain_Loss: 0.0005\nval_loss: 0.0997\n--------\nEpoch: 100/700\nTrain_Loss: 0.0005\nval_loss: 0.1001\n--------\nEpoch: 101/700\nTrain_Loss: 0.0004\nval_loss: 0.1007\n--------\nEpoch: 102/700\nTrain_Loss: 0.0005\nval_loss: 0.1013\n--------\nEpoch: 103/700\nTrain_Loss: 0.0004\nval_loss: 0.1017\n--------\nEpoch: 104/700\nTrain_Loss: 0.0004\nval_loss: 0.1020\n--------\nEpoch: 105/700\nTrain_Loss: 0.0004\nval_loss: 0.1023\n--------\nEpoch: 106/700\nTrain_Loss: 0.0004\nval_loss: 0.1026\n--------\nEpoch: 107/700\nTrain_Loss: 0.0005\nval_loss: 0.1030\n--------\nEpoch: 108/700\nTrain_Loss: 0.0004\nval_loss: 0.1033\n--------\nEpoch: 109/700\nTrain_Loss: 0.0004\nval_loss: 0.1033\n--------\nEpoch: 110/700\nTrain_Loss: 0.0004\nval_loss: 0.1037\n--------\nEpoch: 111/700\nTrain_Loss: 0.0004\nval_loss: 0.1037\n--------\nEpoch: 112/700\nTrain_Loss: 0.0004\nval_loss: 0.1041\n--------\nEpoch: 113/700\nTrain_Loss: 0.0004\nval_loss: 0.1043\n--------\nEpoch: 114/700\nTrain_Loss: 0.0003\nval_loss: 0.1041\n--------\nEpoch: 115/700\nTrain_Loss: 0.0003\nval_loss: 0.1041\n--------\nEpoch: 116/700\nTrain_Loss: 0.0003\nval_loss: 0.1045\n--------\nEpoch: 117/700\nTrain_Loss: 0.0003\nval_loss: 0.1048\n--------\nEpoch: 118/700\nTrain_Loss: 0.0003\nval_loss: 0.1049\n--------\nEpoch: 119/700\nTrain_Loss: 0.0003\nval_loss: 0.1048\n--------\nEpoch: 120/700\nTrain_Loss: 0.0003\nval_loss: 0.1049\n--------\nEpoch: 121/700\nTrain_Loss: 0.0003\nval_loss: 0.1048\n--------\nEpoch: 122/700\nTrain_Loss: 0.0003\nval_loss: 0.1047\n--------\nEpoch: 123/700\nTrain_Loss: 0.0003\nval_loss: 0.1053\n--------\nEpoch: 124/700\nTrain_Loss: 0.0003\nval_loss: 0.1058\n--------\nEpoch: 125/700\nTrain_Loss: 0.0003\nval_loss: 0.1060\n--------\nEpoch: 126/700\nTrain_Loss: 0.0003\nval_loss: 0.1061\n--------\nEpoch: 127/700\nTrain_Loss: 0.0003\nval_loss: 0.1064\n--------\nEpoch: 128/700\nTrain_Loss: 0.0002\nval_loss: 0.1068\n--------\nEpoch: 129/700\nTrain_Loss: 0.0003\nval_loss: 0.1070\n--------\nEpoch: 130/700\nTrain_Loss: 0.0003\nval_loss: 0.1071\n--------\nEpoch: 131/700\nTrain_Loss: 0.0003\nval_loss: 0.1073\n--------\nEpoch: 132/700\nTrain_Loss: 0.0002\nval_loss: 0.1072\n--------\nEpoch: 133/700\nTrain_Loss: 0.0002\nval_loss: 0.1074\n--------\nEpoch: 134/700\nTrain_Loss: 0.0003\nval_loss: 0.1074\n--------\nEpoch: 135/700\nTrain_Loss: 0.0002\nval_loss: 0.1071\n--------\nEpoch: 136/700\nTrain_Loss: 0.0002\nval_loss: 0.1073\n--------\nEpoch: 137/700\nTrain_Loss: 0.0002\nval_loss: 0.1075\n--------\nEpoch: 138/700\nTrain_Loss: 0.0002\nval_loss: 0.1076\n--------\nEpoch: 139/700\nTrain_Loss: 0.0002\nval_loss: 0.1079\n--------\nEpoch: 140/700\nTrain_Loss: 0.0002\nval_loss: 0.1082\n--------\nEpoch: 141/700\nTrain_Loss: 0.0003\nval_loss: 0.1083\n--------\nEpoch: 142/700\nTrain_Loss: 0.0002\nval_loss: 0.1085\n--------\nEpoch: 143/700\nTrain_Loss: 0.0002\nval_loss: 0.1085\n--------\nEpoch: 144/700\nTrain_Loss: 0.0002\nval_loss: 0.1089\n--------\nEpoch: 145/700\nTrain_Loss: 0.0002\nval_loss: 0.1089\n--------\nEpoch: 146/700\nTrain_Loss: 0.0002\nval_loss: 0.1090\n--------\nEpoch: 147/700\nTrain_Loss: 0.0002\nval_loss: 0.1092\n--------\nEpoch: 148/700\nTrain_Loss: 0.0002\nval_loss: 0.1093\n--------\nEpoch: 149/700\nTrain_Loss: 0.0002\nval_loss: 0.1094\n--------\nEpoch: 150/700\nTrain_Loss: 0.0002\nval_loss: 0.1099\n--------\nEpoch: 151/700\nTrain_Loss: 0.0002\nval_loss: 0.1102\n--------\nEpoch: 152/700\nTrain_Loss: 0.0002\nval_loss: 0.1107\n--------\nEpoch: 153/700\nTrain_Loss: 0.0002\nval_loss: 0.1111\n--------\nEpoch: 154/700\nTrain_Loss: 0.0002\nval_loss: 0.1113\n--------\nEpoch: 155/700\nTrain_Loss: 0.0002\nval_loss: 0.1113\n--------\nEpoch: 156/700\nTrain_Loss: 0.0002\nval_loss: 0.1115\n--------\nEpoch: 157/700\nTrain_Loss: 0.0002\nval_loss: 0.1119\n--------\nEpoch: 158/700\nTrain_Loss: 0.0002\nval_loss: 0.1122\n--------\nEpoch: 159/700\nTrain_Loss: 0.0002\nval_loss: 0.1127\n--------\nEpoch: 160/700\nTrain_Loss: 0.0002\nval_loss: 0.1129\n--------\nEpoch: 161/700\nTrain_Loss: 0.0002\nval_loss: 0.1132\n--------\nEpoch: 162/700\nTrain_Loss: 0.0001\nval_loss: 0.1133\n--------\nEpoch: 163/700\nTrain_Loss: 0.0002\nval_loss: 0.1133\n--------\nEpoch: 164/700\nTrain_Loss: 0.0001\nval_loss: 0.1133\n--------\nEpoch: 165/700\nTrain_Loss: 0.0001\nval_loss: 0.1133\n--------\nEpoch: 166/700\nTrain_Loss: 0.0001\nval_loss: 0.1134\n--------\nEpoch: 167/700\nTrain_Loss: 0.0002\nval_loss: 0.1137\n--------\nEpoch: 168/700\nTrain_Loss: 0.0002\nval_loss: 0.1137\n--------\nEpoch: 169/700\nTrain_Loss: 0.0001\nval_loss: 0.1135\n--------\nEpoch: 170/700\nTrain_Loss: 0.0001\nval_loss: 0.1134\n--------\nEpoch: 171/700\nTrain_Loss: 0.0001\nval_loss: 0.1137\n--------\nEpoch: 172/700\nTrain_Loss: 0.0001\nval_loss: 0.1138\n--------\nEpoch: 173/700\nTrain_Loss: 0.0002\nval_loss: 0.1140\n--------\nEpoch: 174/700\nTrain_Loss: 0.0001\nval_loss: 0.1142\n--------\nEpoch: 175/700\nTrain_Loss: 0.0002\nval_loss: 0.1144\n--------\nEpoch: 176/700\nTrain_Loss: 0.0001\nval_loss: 0.1146\n--------\nEpoch: 177/700\nTrain_Loss: 0.0001\nval_loss: 0.1149\n--------\nEpoch: 178/700\nTrain_Loss: 0.0001\nval_loss: 0.1151\n--------\nEpoch: 179/700\nTrain_Loss: 0.0001\nval_loss: 0.1156\n--------\nEpoch: 180/700\nTrain_Loss: 0.0001\nval_loss: 0.1159\n--------\nEpoch: 181/700\nTrain_Loss: 0.0001\nval_loss: 0.1159\n--------\nEpoch: 182/700\nTrain_Loss: 0.0001\nval_loss: 0.1156\n--------\nEpoch: 183/700\nTrain_Loss: 0.0001\nval_loss: 0.1156\n--------\nEpoch: 184/700\nTrain_Loss: 0.0001\nval_loss: 0.1156\n--------\nEpoch: 185/700\nTrain_Loss: 0.0001\nval_loss: 0.1156\n--------\nEpoch: 186/700\nTrain_Loss: 0.0001\nval_loss: 0.1157\n--------\nEpoch: 187/700\nTrain_Loss: 0.0001\nval_loss: 0.1159\n--------\nEpoch: 188/700\nTrain_Loss: 0.0001\nval_loss: 0.1161\n--------\nEpoch: 189/700\nTrain_Loss: 0.0001\nval_loss: 0.1162\n--------\nEpoch: 190/700\nTrain_Loss: 0.0001\nval_loss: 0.1162\n--------\nEpoch: 191/700\nTrain_Loss: 0.0001\nval_loss: 0.1164\n--------\nEpoch: 192/700\nTrain_Loss: 0.0001\nval_loss: 0.1166\n--------\nEpoch: 193/700\nTrain_Loss: 0.0001\nval_loss: 0.1167\n--------\nEpoch: 194/700\nTrain_Loss: 0.0001\nval_loss: 0.1169\n--------\nEpoch: 195/700\nTrain_Loss: 0.0001\nval_loss: 0.1172\n--------\nEpoch: 196/700\nTrain_Loss: 0.0001\nval_loss: 0.1175\n--------\nEpoch: 197/700\nTrain_Loss: 0.0001\nval_loss: 0.1178\n--------\nEpoch: 198/700\nTrain_Loss: 0.0001\nval_loss: 0.1181\n--------\nEpoch: 199/700\nTrain_Loss: 0.0001\nval_loss: 0.1184\n--------\nEpoch: 200/700\nTrain_Loss: 0.0001\nval_loss: 0.1185\n--------\nEpoch: 201/700\nTrain_Loss: 0.0001\nval_loss: 0.1187\n--------\nEpoch: 202/700\nTrain_Loss: 0.0001\nval_loss: 0.1188\n--------\nEpoch: 203/700\nTrain_Loss: 0.0001\nval_loss: 0.1188\n--------\nEpoch: 204/700\nTrain_Loss: 0.0001\nval_loss: 0.1189\n--------\nEpoch: 205/700\nTrain_Loss: 0.0001\nval_loss: 0.1191\n--------\nEpoch: 206/700\nTrain_Loss: 0.0001\nval_loss: 0.1193\n--------\nEpoch: 207/700\nTrain_Loss: 0.0001\nval_loss: 0.1195\n--------\nEpoch: 208/700\nTrain_Loss: 0.0001\nval_loss: 0.1196\n--------\nEpoch: 209/700\nTrain_Loss: 0.0001\nval_loss: 0.1197\n--------\nEpoch: 210/700\nTrain_Loss: 0.0001\nval_loss: 0.1197\n--------\nEpoch: 211/700\nTrain_Loss: 0.0001\nval_loss: 0.1198\n--------\nEpoch: 212/700\nTrain_Loss: 0.0001\nval_loss: 0.1198\n--------\nEpoch: 213/700\nTrain_Loss: 0.0001\nval_loss: 0.1199\n--------\nEpoch: 214/700\nTrain_Loss: 0.0001\nval_loss: 0.1204\n--------\nEpoch: 215/700\nTrain_Loss: 0.0001\nval_loss: 0.1207\n--------\nEpoch: 216/700\nTrain_Loss: 0.0001\nval_loss: 0.1211\n--------\nEpoch: 217/700\nTrain_Loss: 0.0001\nval_loss: 0.1211\n--------\nEpoch: 218/700\nTrain_Loss: 0.0001\nval_loss: 0.1212\n--------\nEpoch: 219/700\nTrain_Loss: 0.0001\nval_loss: 0.1213\n--------\nEpoch: 220/700\nTrain_Loss: 0.0001\nval_loss: 0.1215\n--------\nEpoch: 221/700\nTrain_Loss: 0.0001\nval_loss: 0.1216\n--------\nEpoch: 222/700\nTrain_Loss: 0.0001\nval_loss: 0.1216\n--------\nEpoch: 223/700\nTrain_Loss: 0.0001\nval_loss: 0.1217\n--------\nEpoch: 224/700\nTrain_Loss: 0.0001\nval_loss: 0.1219\n--------\nEpoch: 225/700\nTrain_Loss: 0.0001\nval_loss: 0.1223\n--------\nEpoch: 226/700\nTrain_Loss: 0.0001\nval_loss: 0.1224\n--------\nEpoch: 227/700\nTrain_Loss: 0.0001\nval_loss: 0.1225\n--------\nEpoch: 228/700\nTrain_Loss: 0.0001\nval_loss: 0.1225\n--------\nEpoch: 229/700\nTrain_Loss: 0.0001\nval_loss: 0.1228\n--------\nEpoch: 230/700\nTrain_Loss: 0.0001\nval_loss: 0.1231\n--------\nEpoch: 231/700\nTrain_Loss: 0.0001\nval_loss: 0.1234\n--------\nEpoch: 232/700\nTrain_Loss: 0.0001\nval_loss: 0.1236\n--------\nEpoch: 233/700\nTrain_Loss: 0.0001\nval_loss: 0.1236\n--------\nEpoch: 234/700\nTrain_Loss: 0.0001\nval_loss: 0.1238\n--------\nEpoch: 235/700\nTrain_Loss: 0.0001\nval_loss: 0.1239\n--------\nEpoch: 236/700\nTrain_Loss: 0.0001\nval_loss: 0.1243\n--------\nEpoch: 237/700\nTrain_Loss: 0.0001\nval_loss: 0.1245\n--------\nEpoch: 238/700\nTrain_Loss: 0.0001\nval_loss: 0.1246\n--------\nEpoch: 239/700\nTrain_Loss: 0.0001\nval_loss: 0.1247\n--------\nEpoch: 240/700\nTrain_Loss: 0.0001\nval_loss: 0.1248\n--------\nEpoch: 241/700\nTrain_Loss: 0.0001\nval_loss: 0.1248\n--------\nEpoch: 242/700\nTrain_Loss: 0.0001\nval_loss: 0.1249\n--------\nEpoch: 243/700\nTrain_Loss: 0.0001\nval_loss: 0.1251\n--------\nEpoch: 244/700\nTrain_Loss: 0.0001\nval_loss: 0.1252\n--------\nEpoch: 245/700\nTrain_Loss: 0.0001\nval_loss: 0.1253\n--------\nEpoch: 246/700\nTrain_Loss: 0.0001\nval_loss: 0.1255\n--------\nEpoch: 247/700\nTrain_Loss: 0.0001\nval_loss: 0.1256\n--------\nEpoch: 248/700\nTrain_Loss: 0.0001\nval_loss: 0.1257\n--------\nEpoch: 249/700\nTrain_Loss: 0.0001\nval_loss: 0.1259\n--------\nEpoch: 250/700\nTrain_Loss: 0.0001\nval_loss: 0.1258\n--------\nEpoch: 251/700\nTrain_Loss: 0.0001\nval_loss: 0.1257\n--------\nEpoch: 252/700\nTrain_Loss: 0.0001\nval_loss: 0.1257\n--------\nEpoch: 253/700\nTrain_Loss: 0.0001\nval_loss: 0.1256\n--------\nEpoch: 254/700\nTrain_Loss: 0.0001\nval_loss: 0.1256\n--------\nEpoch: 255/700\nTrain_Loss: 0.0001\nval_loss: 0.1257\n--------\nEpoch: 256/700\nTrain_Loss: 0.0001\nval_loss: 0.1259\n--------\nEpoch: 257/700\nTrain_Loss: 0.0001\nval_loss: 0.1260\n--------\nEpoch: 258/700\nTrain_Loss: 0.0001\nval_loss: 0.1261\n--------\nEpoch: 259/700\nTrain_Loss: 0.0001\nval_loss: 0.1262\n--------\nEpoch: 260/700\nTrain_Loss: 0.0001\nval_loss: 0.1264\n--------\nEpoch: 261/700\nTrain_Loss: 0.0001\nval_loss: 0.1263\n--------\nEpoch: 262/700\nTrain_Loss: 0.0001\nval_loss: 0.1264\n--------\nEpoch: 263/700\nTrain_Loss: 0.0001\nval_loss: 0.1264\n--------\nEpoch: 264/700\nTrain_Loss: 0.0001\nval_loss: 0.1264\n--------\nEpoch: 265/700\nTrain_Loss: 0.0000\nval_loss: 0.1264\n--------\nEpoch: 266/700\nTrain_Loss: 0.0001\nval_loss: 0.1265\n--------\nEpoch: 267/700\nTrain_Loss: 0.0000\nval_loss: 0.1267\n--------\nEpoch: 268/700\nTrain_Loss: 0.0000\nval_loss: 0.1268\n--------\nEpoch: 269/700\nTrain_Loss: 0.0001\nval_loss: 0.1269\n--------\nEpoch: 270/700\nTrain_Loss: 0.0000\nval_loss: 0.1271\n--------\nEpoch: 271/700\nTrain_Loss: 0.0000\nval_loss: 0.1272\n--------\nEpoch: 272/700\nTrain_Loss: 0.0001\nval_loss: 0.1274\n--------\nEpoch: 273/700\nTrain_Loss: 0.0001\nval_loss: 0.1274\n--------\nEpoch: 274/700\nTrain_Loss: 0.0001\nval_loss: 0.1274\n--------\nEpoch: 275/700\nTrain_Loss: 0.0001\nval_loss: 0.1275\n--------\nEpoch: 276/700\nTrain_Loss: 0.0000\nval_loss: 0.1274\n--------\nEpoch: 277/700\nTrain_Loss: 0.0000\nval_loss: 0.1274\n--------\nEpoch: 278/700\nTrain_Loss: 0.0000\nval_loss: 0.1275\n--------\nEpoch: 279/700\nTrain_Loss: 0.0000\nval_loss: 0.1276\n--------\nEpoch: 280/700\nTrain_Loss: 0.0000\nval_loss: 0.1277\n--------\nEpoch: 281/700\nTrain_Loss: 0.0000\nval_loss: 0.1277\n--------\nEpoch: 282/700\nTrain_Loss: 0.0000\nval_loss: 0.1278\n--------\nEpoch: 283/700\nTrain_Loss: 0.0000\nval_loss: 0.1279\n--------\nEpoch: 284/700\nTrain_Loss: 0.0000\nval_loss: 0.1281\n--------\nEpoch: 285/700\nTrain_Loss: 0.0000\nval_loss: 0.1284\n--------\nEpoch: 286/700\nTrain_Loss: 0.0000\nval_loss: 0.1284\n--------\nEpoch: 287/700\nTrain_Loss: 0.0000\nval_loss: 0.1286\n--------\nEpoch: 288/700\nTrain_Loss: 0.0000\nval_loss: 0.1286\n--------\nEpoch: 289/700\nTrain_Loss: 0.0000\nval_loss: 0.1285\n--------\nEpoch: 290/700\nTrain_Loss: 0.0000\nval_loss: 0.1286\n--------\nEpoch: 291/700\nTrain_Loss: 0.0000\nval_loss: 0.1288\n--------\nEpoch: 292/700\nTrain_Loss: 0.0000\nval_loss: 0.1289\n--------\nEpoch: 293/700\nTrain_Loss: 0.0000\nval_loss: 0.1291\n--------\nEpoch: 294/700\nTrain_Loss: 0.0000\nval_loss: 0.1293\n--------\nEpoch: 295/700\nTrain_Loss: 0.0000\nval_loss: 0.1294\n--------\nEpoch: 296/700\nTrain_Loss: 0.0000\nval_loss: 0.1295\n--------\nEpoch: 297/700\nTrain_Loss: 0.0000\nval_loss: 0.1295\n--------\nEpoch: 298/700\nTrain_Loss: 0.0000\nval_loss: 0.1297\n--------\nEpoch: 299/700\nTrain_Loss: 0.0000\nval_loss: 0.1297\n--------\nEpoch: 300/700\nTrain_Loss: 0.0000\nval_loss: 0.1298\n--------\nEpoch: 301/700\nTrain_Loss: 0.0000\nval_loss: 0.1300\n--------\nEpoch: 302/700\nTrain_Loss: 0.0000\nval_loss: 0.1300\n--------\nEpoch: 303/700\nTrain_Loss: 0.0000\nval_loss: 0.1301\n--------\nEpoch: 304/700\nTrain_Loss: 0.0000\nval_loss: 0.1302\n--------\nEpoch: 305/700\nTrain_Loss: 0.0000\nval_loss: 0.1304\n--------\nEpoch: 306/700\nTrain_Loss: 0.0000\nval_loss: 0.1307\n--------\nEpoch: 307/700\nTrain_Loss: 0.0000\nval_loss: 0.1309\n--------\nEpoch: 308/700\nTrain_Loss: 0.0000\nval_loss: 0.1310\n--------\nEpoch: 309/700\nTrain_Loss: 0.0000\nval_loss: 0.1311\n--------\nEpoch: 310/700\nTrain_Loss: 0.0000\nval_loss: 0.1313\n--------\nEpoch: 311/700\nTrain_Loss: 0.0000\nval_loss: 0.1316\n--------\nEpoch: 312/700\nTrain_Loss: 0.0000\nval_loss: 0.1317\n--------\nEpoch: 313/700\nTrain_Loss: 0.0000\nval_loss: 0.1318\n--------\nEpoch: 314/700\nTrain_Loss: 0.0000\nval_loss: 0.1320\n--------\nEpoch: 315/700\nTrain_Loss: 0.0000\nval_loss: 0.1321\n--------\nEpoch: 316/700\nTrain_Loss: 0.0000\nval_loss: 0.1321\n--------\nEpoch: 317/700\nTrain_Loss: 0.0000\nval_loss: 0.1323\n--------\nEpoch: 318/700\nTrain_Loss: 0.0000\nval_loss: 0.1323\n--------\nEpoch: 319/700\nTrain_Loss: 0.0000\nval_loss: 0.1324\n--------\nEpoch: 320/700\nTrain_Loss: 0.0000\nval_loss: 0.1325\n--------\nEpoch: 321/700\nTrain_Loss: 0.0000\nval_loss: 0.1325\n--------\nEpoch: 322/700\nTrain_Loss: 0.0000\nval_loss: 0.1325\n--------\nEpoch: 323/700\nTrain_Loss: 0.0000\nval_loss: 0.1326\n--------\nEpoch: 324/700\nTrain_Loss: 0.0000\nval_loss: 0.1326\n--------\nEpoch: 325/700\nTrain_Loss: 0.0000\nval_loss: 0.1327\n--------\nEpoch: 326/700\nTrain_Loss: 0.0000\nval_loss: 0.1329\n--------\nEpoch: 327/700\nTrain_Loss: 0.0000\nval_loss: 0.1330\n--------\nEpoch: 328/700\nTrain_Loss: 0.0000\nval_loss: 0.1330\n--------\nEpoch: 329/700\nTrain_Loss: 0.0000\nval_loss: 0.1330\n--------\nEpoch: 330/700\nTrain_Loss: 0.0000\nval_loss: 0.1330\n--------\nEpoch: 331/700\nTrain_Loss: 0.0000\nval_loss: 0.1331\n--------\nEpoch: 332/700\nTrain_Loss: 0.0000\nval_loss: 0.1331\n--------\nEpoch: 333/700\nTrain_Loss: 0.0000\nval_loss: 0.1331\n--------\nEpoch: 334/700\nTrain_Loss: 0.0000\nval_loss: 0.1331\n--------\nEpoch: 335/700\nTrain_Loss: 0.0000\nval_loss: 0.1332\n--------\nEpoch: 336/700\nTrain_Loss: 0.0000\nval_loss: 0.1332\n--------\nEpoch: 337/700\nTrain_Loss: 0.0000\nval_loss: 0.1333\n--------\nEpoch: 338/700\nTrain_Loss: 0.0000\nval_loss: 0.1335\n--------\nEpoch: 339/700\nTrain_Loss: 0.0000\nval_loss: 0.1336\n--------\nEpoch: 340/700\nTrain_Loss: 0.0000\nval_loss: 0.1337\n--------\nEpoch: 341/700\nTrain_Loss: 0.0000\nval_loss: 0.1339\n--------\nEpoch: 342/700\nTrain_Loss: 0.0000\nval_loss: 0.1342\n--------\nEpoch: 343/700\nTrain_Loss: 0.0000\nval_loss: 0.1344\n--------\nEpoch: 344/700\nTrain_Loss: 0.0000\nval_loss: 0.1345\n--------\nEpoch: 345/700\nTrain_Loss: 0.0000\nval_loss: 0.1347\n--------\nEpoch: 346/700\nTrain_Loss: 0.0000\nval_loss: 0.1348\n--------\nEpoch: 347/700\nTrain_Loss: 0.0000\nval_loss: 0.1349\n--------\nEpoch: 348/700\nTrain_Loss: 0.0000\nval_loss: 0.1349\n--------\nEpoch: 349/700\nTrain_Loss: 0.0000\nval_loss: 0.1351\n--------\nEpoch: 350/700\nTrain_Loss: 0.0000\nval_loss: 0.1353\n--------\nEpoch: 351/700\nTrain_Loss: 0.0000\nval_loss: 0.1353\n--------\nEpoch: 352/700\nTrain_Loss: 0.0000\nval_loss: 0.1353\n--------\nEpoch: 353/700\nTrain_Loss: 0.0000\nval_loss: 0.1354\n--------\nEpoch: 354/700\nTrain_Loss: 0.0000\nval_loss: 0.1354\n--------\nEpoch: 355/700\nTrain_Loss: 0.0000\nval_loss: 0.1355\n--------\nEpoch: 356/700\nTrain_Loss: 0.0000\nval_loss: 0.1355\n--------\nEpoch: 357/700\nTrain_Loss: 0.0000\nval_loss: 0.1356\n--------\nEpoch: 358/700\nTrain_Loss: 0.0000\nval_loss: 0.1357\n--------\nEpoch: 359/700\nTrain_Loss: 0.0000\nval_loss: 0.1357\n--------\nEpoch: 360/700\nTrain_Loss: 0.0000\nval_loss: 0.1359\n--------\nEpoch: 361/700\nTrain_Loss: 0.0000\nval_loss: 0.1360\n--------\nEpoch: 362/700\nTrain_Loss: 0.0000\nval_loss: 0.1363\n--------\nEpoch: 363/700\nTrain_Loss: 0.0000\nval_loss: 0.1365\n--------\nEpoch: 364/700\nTrain_Loss: 0.0000\nval_loss: 0.1366\n--------\nEpoch: 365/700\nTrain_Loss: 0.0000\nval_loss: 0.1367\n--------\nEpoch: 366/700\nTrain_Loss: 0.0000\nval_loss: 0.1368\n--------\nEpoch: 367/700\nTrain_Loss: 0.0000\nval_loss: 0.1369\n--------\nEpoch: 368/700\nTrain_Loss: 0.0000\nval_loss: 0.1370\n--------\nEpoch: 369/700\nTrain_Loss: 0.0000\nval_loss: 0.1370\n--------\nEpoch: 370/700\nTrain_Loss: 0.0000\nval_loss: 0.1371\n--------\nEpoch: 371/700\nTrain_Loss: 0.0000\nval_loss: 0.1371\n--------\nEpoch: 372/700\nTrain_Loss: 0.0000\nval_loss: 0.1372\n--------\nEpoch: 373/700\nTrain_Loss: 0.0000\nval_loss: 0.1372\n--------\nEpoch: 374/700\nTrain_Loss: 0.0000\nval_loss: 0.1372\n--------\nEpoch: 375/700\nTrain_Loss: 0.0000\nval_loss: 0.1374\n--------\nEpoch: 376/700\nTrain_Loss: 0.0000\nval_loss: 0.1374\n--------\nEpoch: 377/700\nTrain_Loss: 0.0000\nval_loss: 0.1375\n--------\nEpoch: 378/700\nTrain_Loss: 0.0000\nval_loss: 0.1374\n--------\nEpoch: 379/700\nTrain_Loss: 0.0000\nval_loss: 0.1374\n--------\nEpoch: 380/700\nTrain_Loss: 0.0000\nval_loss: 0.1375\n--------\nEpoch: 381/700\nTrain_Loss: 0.0000\nval_loss: 0.1376\n--------\nEpoch: 382/700\nTrain_Loss: 0.0000\nval_loss: 0.1376\n--------\nEpoch: 383/700\nTrain_Loss: 0.0000\nval_loss: 0.1377\n--------\nEpoch: 384/700\nTrain_Loss: 0.0000\nval_loss: 0.1378\n--------\nEpoch: 385/700\nTrain_Loss: 0.0000\nval_loss: 0.1378\n--------\nEpoch: 386/700\nTrain_Loss: 0.0000\nval_loss: 0.1378\n--------\nEpoch: 387/700\nTrain_Loss: 0.0000\nval_loss: 0.1379\n--------\nEpoch: 388/700\nTrain_Loss: 0.0000\nval_loss: 0.1381\n--------\nEpoch: 389/700\nTrain_Loss: 0.0000\nval_loss: 0.1382\n--------\nEpoch: 390/700\nTrain_Loss: 0.0000\nval_loss: 0.1383\n--------\nEpoch: 391/700\nTrain_Loss: 0.0000\nval_loss: 0.1384\n--------\nEpoch: 392/700\nTrain_Loss: 0.0000\nval_loss: 0.1383\n--------\nEpoch: 393/700\nTrain_Loss: 0.0000\nval_loss: 0.1384\n--------\nEpoch: 394/700\nTrain_Loss: 0.0000\nval_loss: 0.1384\n--------\nEpoch: 395/700\nTrain_Loss: 0.0000\nval_loss: 0.1386\n--------\nEpoch: 396/700\nTrain_Loss: 0.0000\nval_loss: 0.1388\n--------\nEpoch: 397/700\nTrain_Loss: 0.0000\nval_loss: 0.1389\n--------\nEpoch: 398/700\nTrain_Loss: 0.0000\nval_loss: 0.1392\n--------\nEpoch: 399/700\nTrain_Loss: 0.0000\nval_loss: 0.1394\n--------\nEpoch: 400/700\nTrain_Loss: 0.0000\nval_loss: 0.1394\n--------\nEpoch: 401/700\nTrain_Loss: 0.0000\nval_loss: 0.1396\n--------\nEpoch: 402/700\nTrain_Loss: 0.0000\nval_loss: 0.1397\n--------\nEpoch: 403/700\nTrain_Loss: 0.0000\nval_loss: 0.1397\n--------\nEpoch: 404/700\nTrain_Loss: 0.0000\nval_loss: 0.1398\n--------\nEpoch: 405/700\nTrain_Loss: 0.0000\nval_loss: 0.1398\n--------\nEpoch: 406/700\nTrain_Loss: 0.0000\nval_loss: 0.1399\n--------\nEpoch: 407/700\nTrain_Loss: 0.0000\nval_loss: 0.1401\n--------\nEpoch: 408/700\nTrain_Loss: 0.0000\nval_loss: 0.1402\n--------\nEpoch: 409/700\nTrain_Loss: 0.0000\nval_loss: 0.1403\n--------\nEpoch: 410/700\nTrain_Loss: 0.0000\nval_loss: 0.1403\n--------\nEpoch: 411/700\nTrain_Loss: 0.0000\nval_loss: 0.1404\n--------\nEpoch: 412/700\nTrain_Loss: 0.0000\nval_loss: 0.1404\n--------\nEpoch: 413/700\nTrain_Loss: 0.0000\nval_loss: 0.1406\n--------\nEpoch: 414/700\nTrain_Loss: 0.0000\nval_loss: 0.1408\n--------\nEpoch: 415/700\nTrain_Loss: 0.0000\nval_loss: 0.1409\n--------\nEpoch: 416/700\nTrain_Loss: 0.0000\nval_loss: 0.1410\n--------\nEpoch: 417/700\nTrain_Loss: 0.0000\nval_loss: 0.1410\n--------\nEpoch: 418/700\nTrain_Loss: 0.0000\nval_loss: 0.1409\n--------\nEpoch: 419/700\nTrain_Loss: 0.0000\nval_loss: 0.1409\n--------\nEpoch: 420/700\nTrain_Loss: 0.0000\nval_loss: 0.1411\n--------\nEpoch: 421/700\nTrain_Loss: 0.0000\nval_loss: 0.1412\n--------\nEpoch: 422/700\nTrain_Loss: 0.0000\nval_loss: 0.1411\n--------\nEpoch: 423/700\nTrain_Loss: 0.0000\nval_loss: 0.1411\n--------\nEpoch: 424/700\nTrain_Loss: 0.0000\nval_loss: 0.1411\n--------\nEpoch: 425/700\nTrain_Loss: 0.0000\nval_loss: 0.1411\n--------\nEpoch: 426/700\nTrain_Loss: 0.0000\nval_loss: 0.1411\n--------\nEpoch: 427/700\nTrain_Loss: 0.0000\nval_loss: 0.1412\n--------\nEpoch: 428/700\nTrain_Loss: 0.0000\nval_loss: 0.1412\n--------\nEpoch: 429/700\nTrain_Loss: 0.0000\nval_loss: 0.1412\n--------\nEpoch: 430/700\nTrain_Loss: 0.0000\nval_loss: 0.1413\n--------\nEpoch: 431/700\nTrain_Loss: 0.0000\nval_loss: 0.1414\n--------\nEpoch: 432/700\nTrain_Loss: 0.0000\nval_loss: 0.1416\n--------\nEpoch: 433/700\nTrain_Loss: 0.0000\nval_loss: 0.1416\n--------\nEpoch: 434/700\nTrain_Loss: 0.0000\nval_loss: 0.1417\n--------\nEpoch: 435/700\nTrain_Loss: 0.0000\nval_loss: 0.1417\n--------\nEpoch: 436/700\nTrain_Loss: 0.0000\nval_loss: 0.1418\n--------\nEpoch: 437/700\nTrain_Loss: 0.0000\nval_loss: 0.1418\n--------\nEpoch: 438/700\nTrain_Loss: 0.0000\nval_loss: 0.1417\n--------\nEpoch: 439/700\nTrain_Loss: 0.0000\nval_loss: 0.1418\n--------\nEpoch: 440/700\nTrain_Loss: 0.0000\nval_loss: 0.1417\n--------\nEpoch: 441/700\nTrain_Loss: 0.0000\nval_loss: 0.1418\n--------\nEpoch: 442/700\nTrain_Loss: 0.0000\nval_loss: 0.1420\n--------\nEpoch: 443/700\nTrain_Loss: 0.0000\nval_loss: 0.1421\n--------\nEpoch: 444/700\nTrain_Loss: 0.0000\nval_loss: 0.1421\n--------\nEpoch: 445/700\nTrain_Loss: 0.0000\nval_loss: 0.1422\n--------\nEpoch: 446/700\nTrain_Loss: 0.0000\nval_loss: 0.1423\n--------\nEpoch: 447/700\nTrain_Loss: 0.0000\nval_loss: 0.1423\n--------\nEpoch: 448/700\nTrain_Loss: 0.0000\nval_loss: 0.1424\n--------\nEpoch: 449/700\nTrain_Loss: 0.0000\nval_loss: 0.1423\n--------\nEpoch: 450/700\nTrain_Loss: 0.0000\nval_loss: 0.1423\n--------\nEpoch: 451/700\nTrain_Loss: 0.0000\nval_loss: 0.1424\n--------\nEpoch: 452/700\nTrain_Loss: 0.0000\nval_loss: 0.1425\n--------\nEpoch: 453/700\nTrain_Loss: 0.0000\nval_loss: 0.1425\n--------\nEpoch: 454/700\nTrain_Loss: 0.0000\nval_loss: 0.1425\n--------\nEpoch: 455/700\nTrain_Loss: 0.0000\nval_loss: 0.1426\n--------\nEpoch: 456/700\nTrain_Loss: 0.0000\nval_loss: 0.1427\n--------\nEpoch: 457/700\nTrain_Loss: 0.0000\nval_loss: 0.1427\n--------\nEpoch: 458/700\nTrain_Loss: 0.0000\nval_loss: 0.1427\n--------\nEpoch: 459/700\nTrain_Loss: 0.0000\nval_loss: 0.1427\n--------\nEpoch: 460/700\nTrain_Loss: 0.0000\nval_loss: 0.1428\n--------\nEpoch: 461/700\nTrain_Loss: 0.0000\nval_loss: 0.1428\n--------\nEpoch: 462/700\nTrain_Loss: 0.0000\nval_loss: 0.1429\n--------\nEpoch: 463/700\nTrain_Loss: 0.0000\nval_loss: 0.1429\n--------\nEpoch: 464/700\nTrain_Loss: 0.0000\nval_loss: 0.1430\n--------\nEpoch: 465/700\nTrain_Loss: 0.0000\nval_loss: 0.1431\n--------\nEpoch: 466/700\nTrain_Loss: 0.0000\nval_loss: 0.1431\n--------\nEpoch: 467/700\nTrain_Loss: 0.0000\nval_loss: 0.1431\n--------\nEpoch: 468/700\nTrain_Loss: 0.0000\nval_loss: 0.1432\n--------\nEpoch: 469/700\nTrain_Loss: 0.0000\nval_loss: 0.1432\n--------\nEpoch: 470/700\nTrain_Loss: 0.0000\nval_loss: 0.1433\n--------\nEpoch: 471/700\nTrain_Loss: 0.0000\nval_loss: 0.1434\n--------\nEpoch: 472/700\nTrain_Loss: 0.0000\nval_loss: 0.1434\n--------\nEpoch: 473/700\nTrain_Loss: 0.0000\nval_loss: 0.1435\n--------\nEpoch: 474/700\nTrain_Loss: 0.0000\nval_loss: 0.1436\n--------\nEpoch: 475/700\nTrain_Loss: 0.0000\nval_loss: 0.1437\n--------\nEpoch: 476/700\nTrain_Loss: 0.0000\nval_loss: 0.1437\n--------\nEpoch: 477/700\nTrain_Loss: 0.0000\nval_loss: 0.1438\n--------\nEpoch: 478/700\nTrain_Loss: 0.0000\nval_loss: 0.1439\n--------\nEpoch: 479/700\nTrain_Loss: 0.0000\nval_loss: 0.1439\n--------\nEpoch: 480/700\nTrain_Loss: 0.0000\nval_loss: 0.1440\n--------\nEpoch: 481/700\nTrain_Loss: 0.0000\nval_loss: 0.1441\n--------\nEpoch: 482/700\nTrain_Loss: 0.0000\nval_loss: 0.1441\n--------\nEpoch: 483/700\nTrain_Loss: 0.0000\nval_loss: 0.1441\n--------\nEpoch: 484/700\nTrain_Loss: 0.0000\nval_loss: 0.1442\n--------\nEpoch: 485/700\nTrain_Loss: 0.0000\nval_loss: 0.1442\n--------\nEpoch: 486/700\nTrain_Loss: 0.0000\nval_loss: 0.1443\n--------\nEpoch: 487/700\nTrain_Loss: 0.0000\nval_loss: 0.1443\n--------\nEpoch: 488/700\nTrain_Loss: 0.0000\nval_loss: 0.1444\n--------\nEpoch: 489/700\nTrain_Loss: 0.0000\nval_loss: 0.1445\n--------\nEpoch: 490/700\nTrain_Loss: 0.0000\nval_loss: 0.1445\n--------\nEpoch: 491/700\nTrain_Loss: 0.0000\nval_loss: 0.1446\n--------\nEpoch: 492/700\nTrain_Loss: 0.0000\nval_loss: 0.1446\n--------\nEpoch: 493/700\nTrain_Loss: 0.0000\nval_loss: 0.1448\n--------\nEpoch: 494/700\nTrain_Loss: 0.0000\nval_loss: 0.1449\n--------\nEpoch: 495/700\nTrain_Loss: 0.0000\nval_loss: 0.1449\n--------\nEpoch: 496/700\nTrain_Loss: 0.0000\nval_loss: 0.1450\n--------\nEpoch: 497/700\nTrain_Loss: 0.0000\nval_loss: 0.1451\n--------\nEpoch: 498/700\nTrain_Loss: 0.0000\nval_loss: 0.1451\n--------\nEpoch: 499/700\nTrain_Loss: 0.0000\nval_loss: 0.1451\n--------\nEpoch: 500/700\nTrain_Loss: 0.0000\nval_loss: 0.1451\n--------\nEpoch: 501/700\nTrain_Loss: 0.0000\nval_loss: 0.1451\n--------\nEpoch: 502/700\nTrain_Loss: 0.0000\nval_loss: 0.1453\n--------\nEpoch: 503/700\nTrain_Loss: 0.0000\nval_loss: 0.1454\n--------\nEpoch: 504/700\nTrain_Loss: 0.0000\nval_loss: 0.1455\n--------\nEpoch: 505/700\nTrain_Loss: 0.0000\nval_loss: 0.1457\n--------\nEpoch: 506/700\nTrain_Loss: 0.0000\nval_loss: 0.1458\n--------\nEpoch: 507/700\nTrain_Loss: 0.0000\nval_loss: 0.1458\n--------\nEpoch: 508/700\nTrain_Loss: 0.0000\nval_loss: 0.1459\n--------\nEpoch: 509/700\nTrain_Loss: 0.0000\nval_loss: 0.1460\n--------\nEpoch: 510/700\nTrain_Loss: 0.0000\nval_loss: 0.1461\n--------\nEpoch: 511/700\nTrain_Loss: 0.0000\nval_loss: 0.1463\n--------\nEpoch: 512/700\nTrain_Loss: 0.0000\nval_loss: 0.1464\n--------\nEpoch: 513/700\nTrain_Loss: 0.0000\nval_loss: 0.1465\n--------\nEpoch: 514/700\nTrain_Loss: 0.0000\nval_loss: 0.1466\n--------\nEpoch: 515/700\nTrain_Loss: 0.0000\nval_loss: 0.1466\n--------\nEpoch: 516/700\nTrain_Loss: 0.0000\nval_loss: 0.1467\n--------\nEpoch: 517/700\nTrain_Loss: 0.0000\nval_loss: 0.1468\n--------\nEpoch: 518/700\nTrain_Loss: 0.0000\nval_loss: 0.1468\n--------\nEpoch: 519/700\nTrain_Loss: 0.0000\nval_loss: 0.1469\n--------\nEpoch: 520/700\nTrain_Loss: 0.0000\nval_loss: 0.1471\n--------\nEpoch: 521/700\nTrain_Loss: 0.0000\nval_loss: 0.1471\n--------\nEpoch: 522/700\nTrain_Loss: 0.0000\nval_loss: 0.1472\n--------\nEpoch: 523/700\nTrain_Loss: 0.0000\nval_loss: 0.1473\n--------\nEpoch: 524/700\nTrain_Loss: 0.0000\nval_loss: 0.1474\n--------\nEpoch: 525/700\nTrain_Loss: 0.0000\nval_loss: 0.1473\n--------\nEpoch: 526/700\nTrain_Loss: 0.0000\nval_loss: 0.1473\n--------\nEpoch: 527/700\nTrain_Loss: 0.0000\nval_loss: 0.1474\n--------\nEpoch: 528/700\nTrain_Loss: 0.0000\nval_loss: 0.1475\n--------\nEpoch: 529/700\nTrain_Loss: 0.0000\nval_loss: 0.1475\n--------\nEpoch: 530/700\nTrain_Loss: 0.0000\nval_loss: 0.1475\n--------\nEpoch: 531/700\nTrain_Loss: 0.0000\nval_loss: 0.1476\n--------\nEpoch: 532/700\nTrain_Loss: 0.0000\nval_loss: 0.1476\n--------\nEpoch: 533/700\nTrain_Loss: 0.0000\nval_loss: 0.1476\n--------\nEpoch: 534/700\nTrain_Loss: 0.0000\nval_loss: 0.1476\n--------\nEpoch: 535/700\nTrain_Loss: 0.0000\nval_loss: 0.1477\n--------\nEpoch: 536/700\nTrain_Loss: 0.0000\nval_loss: 0.1477\n--------\nEpoch: 537/700\nTrain_Loss: 0.0000\nval_loss: 0.1477\n--------\nEpoch: 538/700\nTrain_Loss: 0.0000\nval_loss: 0.1478\n--------\nEpoch: 539/700\nTrain_Loss: 0.0000\nval_loss: 0.1478\n--------\nEpoch: 540/700\nTrain_Loss: 0.0000\nval_loss: 0.1479\n--------\nEpoch: 541/700\nTrain_Loss: 0.0000\nval_loss: 0.1480\n--------\nEpoch: 542/700\nTrain_Loss: 0.0000\nval_loss: 0.1481\n--------\nEpoch: 543/700\nTrain_Loss: 0.0000\nval_loss: 0.1482\n--------\nEpoch: 544/700\nTrain_Loss: 0.0000\nval_loss: 0.1482\n--------\nEpoch: 545/700\nTrain_Loss: 0.0000\nval_loss: 0.1483\n--------\nEpoch: 546/700\nTrain_Loss: 0.0000\nval_loss: 0.1484\n--------\nEpoch: 547/700\nTrain_Loss: 0.0000\nval_loss: 0.1485\n--------\nEpoch: 548/700\nTrain_Loss: 0.0000\nval_loss: 0.1486\n--------\nEpoch: 549/700\nTrain_Loss: 0.0000\nval_loss: 0.1487\n--------\nEpoch: 550/700\nTrain_Loss: 0.0000\nval_loss: 0.1487\n--------\nEpoch: 551/700\nTrain_Loss: 0.0000\nval_loss: 0.1488\n--------\nEpoch: 552/700\nTrain_Loss: 0.0000\nval_loss: 0.1488\n--------\nEpoch: 553/700\nTrain_Loss: 0.0000\nval_loss: 0.1489\n--------\nEpoch: 554/700\nTrain_Loss: 0.0000\nval_loss: 0.1489\n--------\nEpoch: 555/700\nTrain_Loss: 0.0000\nval_loss: 0.1489\n--------\nEpoch: 556/700\nTrain_Loss: 0.0000\nval_loss: 0.1489\n--------\nEpoch: 557/700\nTrain_Loss: 0.0000\nval_loss: 0.1489\n--------\nEpoch: 558/700\nTrain_Loss: 0.0000\nval_loss: 0.1490\n--------\nEpoch: 559/700\nTrain_Loss: 0.0000\nval_loss: 0.1491\n--------\nEpoch: 560/700\nTrain_Loss: 0.0000\nval_loss: 0.1491\n--------\nEpoch: 561/700\nTrain_Loss: 0.0000\nval_loss: 0.1492\n--------\nEpoch: 562/700\nTrain_Loss: 0.0000\nval_loss: 0.1492\n--------\nEpoch: 563/700\nTrain_Loss: 0.0000\nval_loss: 0.1493\n--------\nEpoch: 564/700\nTrain_Loss: 0.0000\nval_loss: 0.1492\n--------\nEpoch: 565/700\nTrain_Loss: 0.0000\nval_loss: 0.1492\n--------\nEpoch: 566/700\nTrain_Loss: 0.0000\nval_loss: 0.1493\n--------\nEpoch: 567/700\nTrain_Loss: 0.0000\nval_loss: 0.1493\n--------\nEpoch: 568/700\nTrain_Loss: 0.0000\nval_loss: 0.1493\n--------\nEpoch: 569/700\nTrain_Loss: 0.0000\nval_loss: 0.1493\n--------\nEpoch: 570/700\nTrain_Loss: 0.0000\nval_loss: 0.1494\n--------\nEpoch: 571/700\nTrain_Loss: 0.0000\nval_loss: 0.1494\n--------\nEpoch: 572/700\nTrain_Loss: 0.0000\nval_loss: 0.1495\n--------\nEpoch: 573/700\nTrain_Loss: 0.0000\nval_loss: 0.1495\n--------\nEpoch: 574/700\nTrain_Loss: 0.0000\nval_loss: 0.1496\n--------\nEpoch: 575/700\nTrain_Loss: 0.0000\nval_loss: 0.1497\n--------\nEpoch: 576/700\nTrain_Loss: 0.0000\nval_loss: 0.1498\n--------\nEpoch: 577/700\nTrain_Loss: 0.0000\nval_loss: 0.1499\n--------\nEpoch: 578/700\nTrain_Loss: 0.0000\nval_loss: 0.1498\n--------\nEpoch: 579/700\nTrain_Loss: 0.0000\nval_loss: 0.1499\n--------\nEpoch: 580/700\nTrain_Loss: 0.0000\nval_loss: 0.1500\n--------\nEpoch: 581/700\nTrain_Loss: 0.0000\nval_loss: 0.1501\n--------\nEpoch: 582/700\nTrain_Loss: 0.0000\nval_loss: 0.1502\n--------\nEpoch: 583/700\nTrain_Loss: 0.0000\nval_loss: 0.1503\n--------\nEpoch: 584/700\nTrain_Loss: 0.0000\nval_loss: 0.1504\n--------\nEpoch: 585/700\nTrain_Loss: 0.0000\nval_loss: 0.1505\n--------\nEpoch: 586/700\nTrain_Loss: 0.0000\nval_loss: 0.1505\n--------\nEpoch: 587/700\nTrain_Loss: 0.0000\nval_loss: 0.1506\n--------\nEpoch: 588/700\nTrain_Loss: 0.0000\nval_loss: 0.1507\n--------\nEpoch: 589/700\nTrain_Loss: 0.0000\nval_loss: 0.1507\n--------\nEpoch: 590/700\nTrain_Loss: 0.0000\nval_loss: 0.1508\n--------\nEpoch: 591/700\nTrain_Loss: 0.0000\nval_loss: 0.1510\n--------\nEpoch: 592/700\nTrain_Loss: 0.0000\nval_loss: 0.1510\n--------\nEpoch: 593/700\nTrain_Loss: 0.0000\nval_loss: 0.1512\n--------\nEpoch: 594/700\nTrain_Loss: 0.0000\nval_loss: 0.1513\n--------\nEpoch: 595/700\nTrain_Loss: 0.0000\nval_loss: 0.1513\n--------\nEpoch: 596/700\nTrain_Loss: 0.0000\nval_loss: 0.1514\n--------\nEpoch: 597/700\nTrain_Loss: 0.0000\nval_loss: 0.1515\n--------\nEpoch: 598/700\nTrain_Loss: 0.0000\nval_loss: 0.1516\n--------\nEpoch: 599/700\nTrain_Loss: 0.0000\nval_loss: 0.1517\n--------\nEpoch: 600/700\nTrain_Loss: 0.0000\nval_loss: 0.1517\n--------\nEpoch: 601/700\nTrain_Loss: 0.0000\nval_loss: 0.1518\n--------\nEpoch: 602/700\nTrain_Loss: 0.0000\nval_loss: 0.1518\n--------\nEpoch: 603/700\nTrain_Loss: 0.0000\nval_loss: 0.1519\n--------\nEpoch: 604/700\nTrain_Loss: 0.0000\nval_loss: 0.1519\n--------\nEpoch: 605/700\nTrain_Loss: 0.0000\nval_loss: 0.1520\n--------\nEpoch: 606/700\nTrain_Loss: 0.0000\nval_loss: 0.1520\n--------\nEpoch: 607/700\nTrain_Loss: 0.0000\nval_loss: 0.1521\n--------\nEpoch: 608/700\nTrain_Loss: 0.0000\nval_loss: 0.1522\n--------\nEpoch: 609/700\nTrain_Loss: 0.0000\nval_loss: 0.1524\n--------\nEpoch: 610/700\nTrain_Loss: 0.0000\nval_loss: 0.1525\n--------\nEpoch: 611/700\nTrain_Loss: 0.0000\nval_loss: 0.1526\n--------\nEpoch: 612/700\nTrain_Loss: 0.0000\nval_loss: 0.1527\n--------\nEpoch: 613/700\nTrain_Loss: 0.0000\nval_loss: 0.1527\n--------\nEpoch: 614/700\nTrain_Loss: 0.0000\nval_loss: 0.1527\n--------\nEpoch: 615/700\nTrain_Loss: 0.0000\nval_loss: 0.1529\n--------\nEpoch: 616/700\nTrain_Loss: 0.0000\nval_loss: 0.1529\n--------\nEpoch: 617/700\nTrain_Loss: 0.0000\nval_loss: 0.1530\n--------\nEpoch: 618/700\nTrain_Loss: 0.0000\nval_loss: 0.1530\n--------\nEpoch: 619/700\nTrain_Loss: 0.0000\nval_loss: 0.1531\n--------\nEpoch: 620/700\nTrain_Loss: 0.0000\nval_loss: 0.1531\n--------\nEpoch: 621/700\nTrain_Loss: 0.0000\nval_loss: 0.1531\n--------\nEpoch: 622/700\nTrain_Loss: 0.0000\nval_loss: 0.1531\n--------\nEpoch: 623/700\nTrain_Loss: 0.0000\nval_loss: 0.1532\n--------\nEpoch: 624/700\nTrain_Loss: 0.0000\nval_loss: 0.1532\n--------\nEpoch: 625/700\nTrain_Loss: 0.0000\nval_loss: 0.1533\n--------\nEpoch: 626/700\nTrain_Loss: 0.0000\nval_loss: 0.1534\n--------\nEpoch: 627/700\nTrain_Loss: 0.0000\nval_loss: 0.1534\n--------\nEpoch: 628/700\nTrain_Loss: 0.0000\nval_loss: 0.1535\n--------\nEpoch: 629/700\nTrain_Loss: 0.0000\nval_loss: 0.1536\n--------\nEpoch: 630/700\nTrain_Loss: 0.0000\nval_loss: 0.1537\n--------\nEpoch: 631/700\nTrain_Loss: 0.0000\nval_loss: 0.1537\n--------\nEpoch: 632/700\nTrain_Loss: 0.0000\nval_loss: 0.1538\n--------\nEpoch: 633/700\nTrain_Loss: 0.0000\nval_loss: 0.1538\n--------\nEpoch: 634/700\nTrain_Loss: 0.0000\nval_loss: 0.1538\n--------\nEpoch: 635/700\nTrain_Loss: 0.0000\nval_loss: 0.1538\n--------\nEpoch: 636/700\nTrain_Loss: 0.0000\nval_loss: 0.1538\n--------\nEpoch: 637/700\nTrain_Loss: 0.0000\nval_loss: 0.1539\n--------\nEpoch: 638/700\nTrain_Loss: 0.0000\nval_loss: 0.1540\n--------\nEpoch: 639/700\nTrain_Loss: 0.0000\nval_loss: 0.1541\n--------\nEpoch: 640/700\nTrain_Loss: 0.0000\nval_loss: 0.1541\n--------\nEpoch: 641/700\nTrain_Loss: 0.0000\nval_loss: 0.1542\n--------\nEpoch: 642/700\nTrain_Loss: 0.0000\nval_loss: 0.1543\n--------\nEpoch: 643/700\nTrain_Loss: 0.0000\nval_loss: 0.1543\n--------\nEpoch: 644/700\nTrain_Loss: 0.0000\nval_loss: 0.1544\n--------\nEpoch: 645/700\nTrain_Loss: 0.0000\nval_loss: 0.1545\n--------\nEpoch: 646/700\nTrain_Loss: 0.0000\nval_loss: 0.1546\n--------\nEpoch: 647/700\nTrain_Loss: 0.0000\nval_loss: 0.1546\n--------\nEpoch: 648/700\nTrain_Loss: 0.0000\nval_loss: 0.1547\n--------\nEpoch: 649/700\nTrain_Loss: 0.0000\nval_loss: 0.1548\n--------\nEpoch: 650/700\nTrain_Loss: 0.0000\nval_loss: 0.1549\n--------\nEpoch: 651/700\nTrain_Loss: 0.0000\nval_loss: 0.1549\n--------\nEpoch: 652/700\nTrain_Loss: 0.0000\nval_loss: 0.1550\n--------\nEpoch: 653/700\nTrain_Loss: 0.0000\nval_loss: 0.1550\n--------\nEpoch: 654/700\nTrain_Loss: 0.0000\nval_loss: 0.1551\n--------\nEpoch: 655/700\nTrain_Loss: 0.0000\nval_loss: 0.1550\n--------\nEpoch: 656/700\nTrain_Loss: 0.0000\nval_loss: 0.1551\n--------\nEpoch: 657/700\nTrain_Loss: 0.0000\nval_loss: 0.1551\n--------\nEpoch: 658/700\nTrain_Loss: 0.0000\nval_loss: 0.1552\n--------\nEpoch: 659/700\nTrain_Loss: 0.0000\nval_loss: 0.1553\n--------\nEpoch: 660/700\nTrain_Loss: 0.0000\nval_loss: 0.1554\n--------\nEpoch: 661/700\nTrain_Loss: 0.0000\nval_loss: 0.1555\n--------\nEpoch: 662/700\nTrain_Loss: 0.0000\nval_loss: 0.1556\n--------\nEpoch: 663/700\nTrain_Loss: 0.0000\nval_loss: 0.1556\n--------\nEpoch: 664/700\nTrain_Loss: 0.0000\nval_loss: 0.1557\n--------\nEpoch: 665/700\nTrain_Loss: 0.0000\nval_loss: 0.1557\n--------\nEpoch: 666/700\nTrain_Loss: 0.0000\nval_loss: 0.1557\n--------\nEpoch: 667/700\nTrain_Loss: 0.0000\nval_loss: 0.1558\n--------\nEpoch: 668/700\nTrain_Loss: 0.0000\nval_loss: 0.1558\n--------\nEpoch: 669/700\nTrain_Loss: 0.0000\nval_loss: 0.1559\n--------\nEpoch: 670/700\nTrain_Loss: 0.0000\nval_loss: 0.1560\n--------\nEpoch: 671/700\nTrain_Loss: 0.0000\nval_loss: 0.1561\n--------\nEpoch: 672/700\nTrain_Loss: 0.0000\nval_loss: 0.1561\n--------\nEpoch: 673/700\nTrain_Loss: 0.0000\nval_loss: 0.1562\n--------\nEpoch: 674/700\nTrain_Loss: 0.0000\nval_loss: 0.1562\n--------\nEpoch: 675/700\nTrain_Loss: 0.0000\nval_loss: 0.1562\n--------\nEpoch: 676/700\nTrain_Loss: 0.0000\nval_loss: 0.1563\n--------\nEpoch: 677/700\nTrain_Loss: 0.0000\nval_loss: 0.1564\n--------\nEpoch: 678/700\nTrain_Loss: 0.0000\nval_loss: 0.1565\n--------\nEpoch: 679/700\nTrain_Loss: 0.0000\nval_loss: 0.1565\n--------\nEpoch: 680/700\nTrain_Loss: 0.0000\nval_loss: 0.1566\n--------\nEpoch: 681/700\nTrain_Loss: 0.0000\nval_loss: 0.1566\n--------\nEpoch: 682/700\nTrain_Loss: 0.0000\nval_loss: 0.1567\n--------\nEpoch: 683/700\nTrain_Loss: 0.0000\nval_loss: 0.1567\n--------\nEpoch: 684/700\nTrain_Loss: 0.0000\nval_loss: 0.1567\n--------\nEpoch: 685/700\nTrain_Loss: 0.0000\nval_loss: 0.1568\n--------\nEpoch: 686/700\nTrain_Loss: 0.0000\nval_loss: 0.1568\n--------\nEpoch: 687/700\nTrain_Loss: 0.0000\nval_loss: 0.1569\n--------\nEpoch: 688/700\nTrain_Loss: 0.0000\nval_loss: 0.1569\n--------\nEpoch: 689/700\nTrain_Loss: 0.0000\nval_loss: 0.1569\n--------\nEpoch: 690/700\nTrain_Loss: 0.0000\nval_loss: 0.1569\n--------\nEpoch: 691/700\nTrain_Loss: 0.0000\nval_loss: 0.1570\n--------\nEpoch: 692/700\nTrain_Loss: 0.0000\nval_loss: 0.1570\n--------\nEpoch: 693/700\nTrain_Loss: 0.0000\nval_loss: 0.1570\n--------\nEpoch: 694/700\nTrain_Loss: 0.0000\nval_loss: 0.1571\n--------\nEpoch: 695/700\nTrain_Loss: 0.0000\nval_loss: 0.1571\n--------\nEpoch: 696/700\nTrain_Loss: 0.0000\nval_loss: 0.1572\n--------\nEpoch: 697/700\nTrain_Loss: 0.0000\nval_loss: 0.1573\n--------\nEpoch: 698/700\nTrain_Loss: 0.0000\nval_loss: 0.1574\n--------\nEpoch: 699/700\nTrain_Loss: 0.0000\nval_loss: 0.1574\n--------\nEpoch: 700/700\nTrain_Loss: 0.0000\nval_loss: 0.1574\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5nX2NznfKXC"
   },
   "source": "## 評価"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "RvUNa3-5rjBM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652230002811,
     "user_tz": -540,
     "elapsed": 914,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "0c928d42-6cf6-4afe-91bc-514b43ca99f6"
   },
   "source": "# この後精度改善のために何度か確認するので、関数化しておきます\ndef myevaluete():\n\n  # 誤差関数の可視化\n  fig = plt.figure() # グラフの描画領域全体のオブジェクトを取得\n  fig.set_figheight(8) # 縦の幅を指定\n  fig.set_figwidth(12) # 横の幅を指定\n  plt.plot(train_loss_list, color='b', label='train_Loss')\n  plt.plot( val_loss_list, color='m', label='val_loss')\n  plt.xlabel('epoch')\n  plt.ylabel('loss')\n  plt.legend()\n  plt.show()\n\n  net.eval() # モデルを評価モードにする\n\n  # テストデータ用のデータローダを用意\n  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n  y_pred = None\n  with torch.no_grad():\n      # ミニバッチで取り出しながら、最初のバッチはy_predに設定し、\n      # 2回目以降はそこへnumpy配列で接続していく\n      for inputs, labels in test_dataloader:\n          outputs = net(inputs)\n          if y_pred is None:\n              y_pred = outputs.data.numpy()\n          else:\n              y_pred = np.concatenate([y_pred, outputs.data.numpy()])\n\n  # np.argmaxで最も確率が高い値を0,1,2に変換する\n  accuracy = accuracy_score(test_y, np.argmax(y_pred, axis=1))\n  print('テストデータに対する予測精度：{}\\n'.format(accuracy))\n\nmyevaluete()",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 864x576 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHgCAYAAACMxVqsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xcdZ3n//fn1KmqTnc6125yJwlOIDBGLmYQRGbxMiyigldAwR0Zf8vIoILLsiIDrIPD7szuPhzHnwzIqIvDD1GEwclvzA6K4DioICEm3AkxkNDccg/pJN3VVfXZP86pvqWTVFXX6erOeT0fj3pUnVOnT33rax7N209/zveYuwsAAABAdYJmDwAAAACYSAjQAAAAQA0I0AAAAEANCNAAAABADQjQAAAAQA0I0AAAAEANwmYPoFYdHR2+aNGiZg8DAAAAh7nHHntsq7t3Dt8/4QL0okWLtGrVqmYPAwAAAIc5M9s40n5aOAAAAIAaEKABAACAGhCgAQAAgBpMuB5oAACAtOrr61NXV5d6enqaPZTDSktLi+bPn69sNlvV8QRoAACACaKrq0vt7e1atGiRzKzZwzksuLu2bdumrq4uLV68uKqfoYUDAABggujp6dHMmTMJzw1kZpo5c2ZNVX0CNAAAwARCeG68WueUAA0AAADUgAANAACAqu3cuVN/93d/V/PPnX322dq5c2fNP/epT31Kd999d80/lyQCNAAAAKp2oABdLBYP+nMrV67UtGnTkhrWmGIVDgAAgAnoiiukNWsae84TTpC+9rWDH3P11Vfrd7/7nU444QRls1m1tLRo+vTpevbZZ7Vu3Tp98IMf1EsvvaSenh5dfvnluuSSSyRJixYt0qpVq9Td3a33vve9esc73qFf/epXmjdvnv7pn/5JkyZNqnqcPT09uvTSS7Vq1SqFYaivfvWreuc736mnnnpKF198sQqFgsrlsu655x7NnTtX5513nrq6ulQqlXTdddfp/PPPH800EaABAABQvb/6q7/Sk08+qTVr1ujnP/+53ve+9+nJJ5/sXwLuO9/5jmbMmKF9+/bpD/7gD/SRj3xEM2fOHHKO559/Xnfeeaf+/u//Xuedd57uueceXXTRRVWP4aabbpKZ6YknntCzzz6rM888U+vWrdMtt9yiyy+/XBdeeKEKhYJKpZJWrlypuXPn6sc//rEkadeuXaOeAwI0AADABHSoSvFYOfnkk4esn/z1r39d9957ryTppZde0vPPP79fgF68eLFOOOEESdJb3/pWvfjiizV95kMPPaTPfe5zkqSlS5dq4cKFWrdunU499VTdeOON6urq0oc//GEtWbJEy5Yt05VXXqkvfvGLev/736/TTz99FN82Qg80AAAA6tbW1tb/+uc//7nuv/9+/frXv9batWt14oknjri+cj6f73+dyWQO2T9drU984hNasWKFJk2apLPPPlsPPPCAjj76aK1evVrLli3TtddeqxtuuGHUn0MFGgAAAFVrb2/X7t27R3xv165dmj59ulpbW/Xss8/q4YcfTmQMp59+uu644w69613v0rp167Rp0yYdc8wx2rBhg4466ih9/vOf16ZNm/T4449r6dKlmjFjhi666CJNmzZN3/rWt0b9+QRoAAAAVG3mzJk67bTT9OY3v1mTJk3SrFmz+t8766yzdMstt+jYY4/VMccco1NOOaUhn/mnf/qnuuKKKyRJCxYs0IMPPqhLL71Uy5YtUxiGuu2225TP53XXXXfp9ttvVzab1ezZs3XNNdfo0Ucf1VVXXaUgCJTNZnXzzTePejzm7qM+yVhavny5r1q1akw/s1iUurultjYpmx3TjwYAAOj3zDPP6Nhjj232MA5LI82tmT3m7suHH0sPdBV+9Stp+nTp3/6t2SMBAABAs9HCUYVMJnpuUH87AAAAhrnsssv0y1/+csi+yy+/XBdffHGTRnRgBOgqhPEsEaABAACScdNNNzV7CFWjhaMKlQBdKjV3HAAAAGg+AnQVqEADAACgggBdBQI0AAAAKgjQVSBAAwAAoIIAXQUCNAAAQH0mT558wPdefPFFvfnNbx7D0TQGAboKBGgAAABUsIxdFQjQAABgvHn+iufVvaa7oeecfMJkLfnakoMec/XVV2vBggW67LLLJElf/vKXFYahHnzwQe3YsUN9fX36y7/8S5177rk1fXZPT48uvfRSrVq1SmEY6qtf/are+c536qmnntLFF1+sQqGgcrmse+65R3PnztV5552nrq4ulUolXXfddTr//PPr/t61IkBXgQANAAAQOf/883XFFVf0B+i77rpL9913nz7/+c9rypQp2rp1q0455RSdc845MrOqz3vTTTfJzPTEE0/o2Wef1Zlnnql169bplltu0eWXX64LL7xQhUJBpVJJK1eu1Ny5c/XjH/9YkrRr165EvuuBEKCrwJ0IAQDAeHOoSnFSTjzxRG3evFmvvPKKtmzZounTp2v27Nn6whe+oF/84hcKgkAvv/yyXn/9dc2ePbvq8z700EP63Oc+J0launSpFi5cqHXr1unUU0/VjTfeqK6uLn34wx/WkiVLtGzZMl155ZX64he/qPe///06/fTTk/q6I6IHugpUoAEAAAZ87GMf0913360f/OAHOv/883XHHXdoy5Yteuyxx7RmzRrNmjVLPT09DfmsT3ziE1qxYoUmTZqks88+Ww888ICOPvporV69WsuWLdO1116rG264oSGfVa3EArSZfcfMNpvZkwd438zs62a23sweN7OTkhrLaHEnQgAAgAHnn3++vv/97+vuu+/Wxz72Me3atUtHHHGEstmsHnzwQW3cuLHmc55++um64447JEnr1q3Tpk2bdMwxx2jDhg066qij9PnPf17nnnuuHn/8cb3yyitqbW3VRRddpKuuukqrV69u9Fc8qCRbOG6T9A1J/3CA998raUn8eJukm+PncYcKNAAAwIDf//3f1+7duzVv3jzNmTNHF154oT7wgQ9o2bJlWr58uZYuXVrzOf/sz/5Ml156qZYtW6YwDHXbbbcpn8/rrrvu0u23365sNqvZs2frmmuu0aOPPqqrrrpKQRAom83q5ptvTuBbHpi5e3InN1sk6Z/dfb8F/szsm5J+7u53xtvPSTrD3V892DmXL1/uq1atSmC0B1YsStms9JWvSNdeO6YfDQAA0O+ZZ57Rscce2+xhHJZGmlsze8zdlw8/tpk90PMkvTRouyvetx8zu8TMVpnZqi1btozJ4AbjIkIAAABUTIhVONz9Vkm3SlEFeqw/3ywK0QRoAACA2j3xxBP65Cc/OWRfPp/XI4880qQRjU4zA/TLkhYM2p4f7xuXwpAADQAAUI9ly5ZpzZo1zR5GwzSzhWOFpP8Qr8ZxiqRdh+p/biYCNAAAGA+SvH4trWqd08Qq0GZ2p6QzJHWYWZek/yopK0nufouklZLOlrRe0l5JFyc1lkYgQAMAgGZraWnRtm3bNHPmzJru8ocDc3dt27ZNLS0tVf9MYgHa3T9+iPdd0mVJfX6j0QMNAACabf78+erq6lIzFlU4nLW0tGj+/PlVHz8hLiIcD6hAAwCAZstms1q8eHGzh5F63Mq7SmHInQgBAABAgK4aFWgAAABIBOiqEaABAAAgEaCrRoAGAACARICuGgEaAAAAEgG6agRoAAAASAToqhGgAQAAIBGgq0aABgAAgESArhp3IgQAAIBEgK5K384+Hb17h7I9fc0eCgAAAJqMAF2FPU/s0aeeXKvOXd3NHgoAAACajABdBQstelH05g4EAAAATUeAroJlowDtBGgAAIDUI0BXYaACXW7uQAAAANB0BOgqBNl4mkpUoAEAANKOAF0FeqABAABQQYCuQqUHmgo0AAAACNBV6K9AE6ABAABSjwBdhUoF2gjQAAAAqUeArkJ/gC4ToAEAANKOAF2FSguHlVjGDgAAIO0I0FVgGTsAAABUEKCr0F+BpoUDAAAg9QjQVaAHGgAAABUE6CpYYHJJAS0cAAAAqUeArpJnjAo0AAAACNDV8sAUyFVmIQ4AAIBUI0BXyTOmUGWVSs0eCQAAAJqJAF0lD0wZuYrFZo8EAAAAzUSArpJnAoVyKtAAAAApR4CuVoYKNAAAAAjQ1cuYQgI0AABA6hGgq+RUoAEAACACdPUI0AAAABABumoWRsvYEaABAADSjQBdrZAKNAAAAAjQ1QsDLiIEAAAAAbpaRg80AAAARICuHi0cAAAAEAG6ahYHaO5ECAAAkG4E6CpZlhupAAAAgABdNaOFAwAAACJAVy2qQLMONAAAQNoRoKsUZAMq0AAAACBAV4sWDgAAAEgE6KoFOS4iBAAAAAG6alSgAQAAIBGgq0YFGgAAABIBumpBlhupAAAAgABdtagCzTJ2AAAAaUeArlImSw80AAAACNBVC3KsAw0AAAACdNUyeVNGUrHgzR4KAAAAmogAXaUga5II0AAAAGlHgK5SJh8F6FIvARoAACDNCNBVyuTiAE0FGgAAINUI0FWqBOhyodzkkQAAAKCZCNBV6m/hoAINAACQagToKoX5aKrKfQRoAACANCNAV6myCkeZCjQAAECqEaCrFOQI0AAAACBAV83COEDTwgEAAJBqBOgqWZYADQAAAAJ01QjQAAAAkAjQVetv4WAdaAAAgFRLNECb2Vlm9pyZrTezq0d4/0gze9DMfmtmj5vZ2UmOZzSCbDRVTgUaAAAg1RIL0GaWkXSTpPdKOk7Sx83suGGHXSvpLnc/UdIFkv4uqfGMVn8FukiABgAASLMkK9AnS1rv7hvcvSDp+5LOHXaMS5oSv54q6ZUExzMqlR5oKtAAAADpFiZ47nmSXhq03SXpbcOO+bKkn5jZ5yS1SXpPguMZlUoF2qlAAwAApFqzLyL8uKTb3H2+pLMl3W5m+43JzC4xs1VmtmrLli1jPkhpoAItAjQAAECqJRmgX5a0YND2/HjfYJ+WdJckufuvJbVI6hh+Ine/1d2Xu/vyzs7OhIZ7cFSgAQAAICUboB+VtMTMFptZTtFFgiuGHbNJ0rslycyOVRSgm1NiPoSBHmiWsQMAAEizxAK0uxclfVbSfZKeUbTaxlNmdoOZnRMfdqWk/2hmayXdKelT7j4uS7xUoAEAACAlexGh3H2lpJXD9l0/6PXTkk5LcgyNUlkHWiUCNAAAQJo1+yLCCYOLCAEAACARoKtWaeEgQAMAAKQbAbpK/RVoWjgAAABSjQBdpf4KNAEaAAAg1QjQVapUoK3EMnYAAABpRoCuEhVoAAAASAToqlmmUoEmQAMAAKQZAbpKZqaSmVQmQAMAAKQZAboGHhgVaAAAgJQjQNegHJiMCjQAAECqEaBrQAUaAAAABOgalANTUGYZOwAAgDQjQNfAaeEAAABIPQJ0DTwTKHACNAAAQJoRoGtABRoAAAAE6Bp4xpShAg0AAJBqBOhaUIEGAABIPQJ0DahAAwAAgABdi4wpkIuV7AAAANKLAF2L0BSqrGKx2QMBAABAsxCga5ExhXICNAAAQIoRoGuRCZQhQAMAAKQaAboWoRGgAQAAUo4AXYtMFKBLpWYPBAAAAM1CgK6BZemBBgAASDsCdA0sQwsHAABA2hGga5FlGTsAAIC0I0DXwLiIEAAAIPUI0DWwLMvYAQAApB0BugZByEWEAAAAaUeAroFlaeEAAABIOwJ0DYwKNAAAQOoRoGsQUIEGAABIvbDZA5hIgpzJVOZOhAAAAClGgK5BkDVJUrHPJVlzBwMAAICmoIWjBkEumq5irzd5JAAAAGgWAnQN+ivQBGgAAIDUIkDXIMgRoAEAANKOAF2DTFyBLhGgAQAAUosAXYMgTwUaAAAg7QjQNRioQJebPBIAAAA0CwG6BpUKdKlABRoAACCtCNA1yMQXEZYJ0AAAAKlFgK5BGK8DTQUaAAAgvQjQNci0UIEGAABIOwJ0DSotHKU+AjQAAEBaEaBrEMYXEZZZxg4AACC1CNA1qARoKtAAAADpRYCuQdjfA8060AAAAGlFgK5BfwsHFWgAAIDUIkDXIJOPpssJ0AAAAKlFgK5B/41UCNAAAACpRYCugWWjAO1FAjQAAEBaEaBrYGEcoKlAAwAApBYBugaVCjQtHAAAAOlFgK7BQAsHy9gBAACkFQG6BrRwAAAAgABdgyAbL2PHRYQAAACpRYCuQaUCLQI0AABAahGga8AydgAAACBA14AKNAAAAAjQNahUoFUiQAMAAKQVAboGAxVolrEDAABIKwJ0DcxMJYkKNAAAQIoRoGtUNiNAAwAApBgBukZlCwjQAAAAKUaArlEpMBkBGgAAILUSDdBmdpaZPWdm683s6gMcc56ZPW1mT5nZ95IcTyM4LRwAAACpFiZ1YjPLSLpJ0h9J6pL0qJmtcPenBx2zRNKXJJ3m7jvM7IikxtMoZSrQAAAAqZZkBfpkSevdfYO7FyR9X9K5w475j5JucvcdkuTumxMcT0OUzWRllrEDAABIqyQD9DxJLw3a7or3DXa0pKPN7Jdm9rCZnZXgeBqCCjQAAEC6JdbCUcPnL5F0hqT5kn5hZsvcfefgg8zsEkmXSNKRRx451mMcwgOTlQnQAAAAaZVkBfplSQsGbc+P9w3WJWmFu/e5+wuS1ikK1EO4+63uvtzdl3d2diY24Gp4ECggQAMAAKRWkgH6UUlLzGyxmeUkXSBpxbBjfqSo+iwz61DU0rEhwTGNGhVoAACAdEssQLt7UdJnJd0n6RlJd7n7U2Z2g5mdEx92n6RtZva0pAclXeXu25IaUyOUM6bACdAAAABplWgPtLuvlLRy2L7rB712Sf8pfkwMVKABAABSjTsR1sgzRg80AABAihGgaxQFaNaBBgAASCsCdI0sYzJ6oAEAAFKLAF0jDwNlCNAAAACpRYCukWWMAA0AAJBiBOhahSxjBwAAkGYE6FplTBm5uI4QAAAgnQjQNbJsFKD7+po9EgAAADQDAbpGFppClVUsNnskAAAAaIZE70R4OLLQFFCBBgAASC0q0DWykBYOAACANCNA18iyAQEaAAAgxQjQNQpyppAADQAAkFoE6BpFFxE6FxECAACkFAG6RpUKdKHAzVQAAADSiABdoyBrkqS+HgI0AABAGhGga5TJEaABAADSjABdI4sr0EUCNAAAQCpVFaDN7HIzm2KRb5vZajM7M+nBjUeZXDRlVKABAADSqdoK9J+4+xuSzpQ0XdInJf1VYqMaxyotHFSgAQAA0qnaAG3x89mSbnf3pwbtS5VMPu6B7iVAAwAApFG1AfoxM/uJogB9n5m1SyonN6zxq1KBLlGBBgAASKWwyuM+LekESRvcfa+ZzZB0cXLDGr8yOVOfpL7eVP7/BwAAgNSrtgJ9qqTn3H2nmV0k6VpJu5Ib1vhVaeEo0sIBAACQStUG6Jsl7TWz4yVdKel3kv4hsVGNY2GeFg4AAIA0qzZAF93dJZ0r6RvufpOk9uSGNX6FLdGUUYEGAABIp2p7oHeb2ZcULV93upkFkrLJDWv86r+IsECABgAASKNqK9DnS+pVtB70a5LmS/qfiY1qHAtbCNAAAABpVlWAjkPzHZKmmtn7JfW4eyp7oLOVHmhaOAAAAFKp2lt5nyfpN5I+Juk8SY+Y2UeTHNh4FU6iAg0AAJBm1fZA/7mkP3D3zZJkZp2S7pd0d1IDG6/CuAe6XGAdaAAAgDSqtgc6qITn2LYafvawkq1UoPuoQAMAAKRRtRXofzGz+yTdGW+fL2llMkMa37ItlQo0ARoAACCNqgrQ7n6VmX1E0mnxrlvd/d7khjV+VdaBJkADAACkU7UVaLn7PZLuSXAsE4Jlowq008IBAACQSgcN0Ga2W9JISdEkubtPSWRU45iFcQsHARoAACCVDhqg3T2Vt+s+mEoFmgANAACQTqlcSWM0KhVo72MZOwAAgDQiQNeICjQAAEC6EaBrVKlAq0iABgAASCMCdI2CbLyMHQEaAAAglQjQNeqvQNPCAQAAkEoE6Br1rwNNBRoAACCVCNA1ssBUlqQSARoAACCNCNB1KJtJRZaxAwAASCMCdB1KZnIq0AAAAKlEgK6Dm8nogQYAAEglAnQdSkEglQnQAAAAaUSAroMHJqOFAwAAIJUI0HUom7EKBwAAQEoRoOtABRoAACC9CNB18IzJ6IEGAABIJQJ0HTwwWZl1oAEAANKIAF0HD0wBLRwAAACpRICug2dM5gRoAACANCJA1yMTKKAHGgAAIJUI0HXwjBGgAQAAUooAXY/QFDgXEQIAAKQRAboOHgbK0AMNAACQSgToemRMGXeRoQEAANKHAF2PrClUWSwFDQAAkD4E6DpYGCiUq6+v2SMBAADAWCNA1yNryqpMgAYAAEghAnQdLGvKUIEGAABIJQJ0HSwbKCtXsdjskQAAAGCsEaDrYPFFhFSgAQAA0ocAXQfLcREhAABAWiUaoM3sLDN7zszWm9nVBznuI2bmZrY8yfE0SpAzAjQAAEBKJRagzSwj6SZJ75V0nKSPm9lxIxzXLulySY8kNZZGC7JRgC70cicVAACAtEmyAn2ypPXuvsHdC5K+L+ncEY77iqS/ltST4FgaKshF01bsIUADAACkTZIBep6klwZtd8X7+pnZSZIWuPuPExxHwwV5kyT17eNWhAAAAGnTtIsIzSyQ9FVJV1Zx7CVmtsrMVm3ZsiX5wR1CpQLdt5cKNAAAQNokGaBflrRg0Pb8eF9Fu6Q3S/q5mb0o6RRJK0a6kNDdb3X35e6+vLOzM8EhVyfTElegaeEAAABInSQD9KOSlpjZYjPLSbpA0orKm+6+y9073H2Ruy+S9LCkc9x9VYJjaohMLgrQpR5aOAAAANImsQDt7kVJn5V0n6RnJN3l7k+Z2Q1mdk5SnzsWMvm4hYMKNAAAQOqESZ7c3VdKWjls3/UHOPaMJMfSSJm8qSSpyEWEAAAAqcOdCOsQTmIZOwAAgLQiQNehchFhiRupAAAApA4Bug5hnosIAQAA0ooAXYdKCwcVaAAAgPQhQNch7G/hoAINAACQNgToOmSpQAMAAKQWAboOlQp0uUCABgAASBsCdB2yk+IATQsHAABA6hCg65BrjaaNCjQAAED6EKDr0F+BLlCBBgAASBsCdB2ylQp0HxVoAACAtCFA1yET30jFaeEAAABIHQJ0HSxXCdC0cAAAAKQNAboOQTaaNqeFAwAAIHUI0HXor0AXqUADAACkDQG6DhbGAZoKNAAAQOoQoOtgZirKJAI0AABA6hCg61Q0k2jhAAAASB0CdJ1KFtDCAQAAkEIE6DqVzGRUoAEAAFKHAF2nUhBIRSrQAAAAaUOArlM5oAINAACQRgToOpUygQICNAAAQOoQoOtUCgIZLRwAAACpQ4CuUzkTKChRgQYAAEgbAnSdyhlTpkyABgAASBsCdJ3KYaAMFWgAAIDUIUDXybMBFWgAAIAUIkDXycNAmTIXEQIAAKQNAbpOng0UUoEGAABIHQJ0nTwMFDoBGgAAIG0I0PXKmbIEaAAAgNQhQNfJslSgAQAA0ogAXa98oKy4iBAAACBtCNB1slygUK4yt/MGAABIFQJ0nSwXTV1vN20cAAAAaUKArlOQN0lSgQANAACQKgToOgUtVKABAADSiABdJ8tHU0cFGgAAIF0I0HXKxBXowh4uIgQAAEgTAnSdKi0cfXuoQAMAAKQJAbpO4aS4Ar2XAA0AAJAmBOg6hZOiVTioQAMAAKQLAbpOlR7oIhVoAACAVCFA1ylsjXug93IRIQAAQJoQoOtUCdClfVSgAQAA0oQAXadsHKCLBGgAAIBUIUDXKdsaXURIDzQAAEC6EKDrlG2LWzh6CNAAAABpQoCuU64/QHMRIQAAQJoQoOtUCdDlXirQAAAAaUKArlOuPQ7QXEQIAACQKgToOuVbTL0K5D2lZg8FAAAAY4gAXadsVupRQAUaAAAgZQjQdcrlpF5lJCrQAAAAqUKArlOlAi2WsQMAAEgVAnSd+ivQvVSgAQAA0oQAXacwlHoVSCxjBwAAkCoE6DqZScUMPdAAAABpQ4AehWIYKChQgQYAAEgTAvQolMKMrEAFGgAAIE0I0KNQzgXKFKlAAwAApAkBehTKuYzCIhVoAACANCFAj0Y+IEADAACkDAF6NFoyyrrLS97skQAAAGCMEKBHwVqi6SvtowoNAACQFgToUQhaM5Kk8l4uJAQAAEiLRAO0mZ1lZs+Z2Xozu3qE9/+TmT1tZo+b2c/MbGGS42m0oDWuQO+lAg0AAJAWiQVoM8tIuknSeyUdJ+njZnbcsMN+K2m5u79F0t2S/kdS40lC2EYFGgAAIG2SrECfLGm9u29w94Kk70s6d/AB7v6gu++NNx+WND/B8TRcdnI0fcVuKtAAAABpkWSAnifppUHbXfG+A/m0pP+T4HgaLmyPKtD7dlKBBgAASIuw2QOQJDO7SNJySf/uAO9fIukSSTryyCPHcGQHl58SBei926lAAwAApEWSFeiXJS0YtD0/3jeEmb1H0p9LOsfde0c6kbvf6u7L3X15Z2dnIoOtR25KNH09VKABAABSI8kA/aikJWa22Mxyki6QtGLwAWZ2oqRvKgrPmxMcSyJapkYV6J4dVKABAADSIrEA7e5FSZ+VdJ+kZyTd5e5PmdkNZnZOfNj/lDRZ0g/NbI2ZrTjA6callo6oA6Z3e7HJIwEAAMBYSbQH2t1XSlo5bN/1g16/J8nPT1prR1SBLhCgAQAAUoM7EY5C65RA+xSobycBGgAAIC0I0KPQ2irtUajiG/RAAwAApAUBehTa2qIAXX6DCjQAAEBaEKBHobVV6lYo302ABgAASAsC9ChELRwZaQ8BGgAAIC0I0KNQ6YG2vQRoAACAtCBAj0I2K+2zUJl9BGgAAIC0IECPUm82VNjDKhwAAABpQYAepb58qEyprHKh3OyhAAAAYAwQoEepmI/uRljcRRsHAABAGiR6K+80KE2KprC4s6hcZ67JowEAABg77i7vc5V7y/JC/NznKheGPve/Lkav+5/j1+W+Ed6rPJddi65b1OyvOgQBepS8EqCpQAMAgIR4KQqn5cKgoFqIQung197rQ44pF8oq90QP7/X+1+We6OcGB98D/Xxl35CQXBg4LnEmAvThptyelST1be1r8h66zUMAABiqSURBVEgAAEAS3D0KjIPD51g94pCrRl5qZVLQEijIB7K8KcjFr3M25DnTllE4Ixz6fm6En6nsyweybLydMwXZ6Ll/XzZ6bWH8XjiwfdD3Qmvgl28MAvQolaZGbRt9rxOgAQColZcH/Xm/MEL1szDsvd4Rjh1ehR12ngPtr+b85d5o32hZzqLQeoBHZkpG2SOyQ/fno+dKoO0PpoOC60jBdr/jh53Tsiaz8RdKJxIC9Cj5tKgCXXit0OSRAABQHXeXl1ze6yrtKanUXYqe49flPeVo396SynvLKu0rqbyvPPR1T9yzWop6VVWKw3Apfl3ygcfg6m3v0AprUi0AQ8Lk8KA56DkzOaMwF+63vz+AZk3BpAMH36oe+UAWEFgPJwToUcpPC9VjgQqvE6ABAAO8VEUv6cH6S0fqd62zT3Wkc6uO3BpMCgYelUpmJv4zeyZ6KNDA64yiIDploJo6+FH5s3/lMVLAPdj+/fbFVVjLUGFFsgjQo9TaKu2wHBVoABgjXh5Uzdw3qCI6vCo6QiW0f7voA5XQ3mEXWPVF/aZedqms6P19wz6r59AhV42+x5ZpxD7VEf90336QPtVh/a6ZtoyCtkCZyRll2jIDz5X9kzIKWuPASygFJBGgR621VdruOSrQAFKtXIwD6LBQul84PcD75b1l9W3rU9+2PpV2DbQT9LcM7B0IsI3oRx1RfGFVpaoqkywwWd6iEDmo+hpODw944dV+ldMDhdyDXYw10rlDbt0AjBcE6FFqa5O2eU6F1/Y2eygAsB93j3pZdw9UTss9cSgdXMXdU1JpV0nFnUUVdxWj5zeK+wXXA4XgRlRbM1MyynZkFU4No77U6aEy86LgmmnNRH/2HxRih4fazKToGAuj1oFKG0GllWDwtoU2tIUg/jkqrACqQYAepdZW6WXlVHhtZ7OHAmACqvTJjrhSwEirDfSWBy722j008PYH4GEhuNZwm5mcUTgtVGZK/Gf8SYHCqeF+KwP0L4E1bF9/IK32vXwcgrNUWAFMDAToUWptlbYrp+L2osqFsoIc/wEADndecpX2xisXxNXdvu19Km4rRm0Ildfbo5aE4vZof2lPab+LuxqxtmtmShR4w6mhwmmh8gvyantzWxSCp8bvTQn3X0lg0HamdSA00yoAAAdHgB6l1lZpq6K1oHtf6dWkRZOaPCIg3Ur7oiqs9w0sr9V/8VhxWPDtHli2a/CjfwmvAxxT3nfo1JuZnFE4M1R2RlbZmVnlF+SVac9Ut5LACKsODFn3tS3ov9grnBJGLQoAgDFDgB6ltjapS62SpL3P7iVAAyMo9ZSiKuz2vqGtBruiR6m7JMtafw+rl3zgArI90Vq0+90AYV95YI3a+Lm4s1hVuD2QoDVeiWDyoJUI2jPKzckN3TfomKAtUNgeKpwRKjszGz3PyPLXKAA4jBGgR6m1VXqxEqCf2auZZ81s8oiA5JSLZRV3FPtbEga3Jwx/PXi7vPcQoTajEft0LWf9PbjDq7L9LQdzoyW2MpPiVoUZYf8KCUMuHovXqR2yZNfgR2uGGx0AAKpCgB6lqVOlN5STT81q7zOsxIHxr9wbtScUdxf72xXKPWUVXi9oz9N7tPeZvVElN67yFncWVXpj4KK1A8pI2RnZ/kpsy5EtCk+IXlce4fQo3IZTB/p1M1MzyrRkorV948+0jCloC+jFBQCMSwToUeroiJ4Lc1q15+k9zR0MUsfLruKuuOr7ep/2vbBPPRt61PtSrwqvFVR4raDiG8WoHWJP1A7hfQdZQ9eklqNalJ2ZVdASKJwWqmVxi8L2MGpVmDbQ0xvOjMPxjOh1OCUcVQXXgqiFIzMpU/c5AAAYCwToUaoE6O4ZrWp7eovcnXVEUbXSnpJ6XooCb++m3uh1V6+K26IKcKUVob/Xt7ukvh1xH3F31Ec8UutDdlZW+Tl55Wbn1HJUy5C7ioXtoTLtcdtCe7y/JVB2ZlaTjp5EgAUA4BAI0KM0fboUBNKWaZM1c8er6nmxR5MWcyFhGri7ijuKKrxaUO+rvdq3fp96XuhRaXe8Pm/cIuG90coPxV1FSdHqDKXdJfVt7lOpe1j6NSk3K6dsR1aWN+19bq+8z6OQ2xotNdaysEXh8VEIDqeGynZko0dnVi2LWtSyqEWZFkIwAABJIUCPUiYjzZghbWqdoqWS3njkDQL0YaBciC6W86Krb2ufel/pVeGVgnpf7tXedXu1e9Vu9bzYs98thS1vCqeE+1V3c7Nzal0aXWxa2l2KVnY4IqfsEVGvcH5BXvkj88rPzbN6AwAA4xwBugE6OqTfeZuCSYF2P7Jbsy6Y1ewhpZ67q7izqMIrBe373b5oJYj4tsP9tyMedCvivm19Az3D26Ol1Q7UK5yfn9fkt05Wx7kdys3JRa0Sc3JqWdii/JF5WngAADjMEaAboKND2rwtUPtb2/XGw280eziHFS+7Cq8VVNpbUu/GXpW6SwpaAvW+2queF3rkBZe7Sy5ZaOp9uVd71u7Rnqf3yAsHuVhO0fGVWwpnZ2aVm53T5BMmKzsjq8zUjFqObJGF8Xtzc8rPjYIyFWIAANKNAN0AHR3S+vXStA9N08YbN6qwpaBcZ67Zw5pQiruK6n6iW3se36Putd3a89Qe7Xtun4q7i/u1SfSzKATLotfe58odkVPb8W2a/575ys3LKTc7p0lHTVK2Mztw++J8vKYwd28DAAB1IEA3QGen9PDDUseHOrTxKxu1bcU2zfn0nGYPa0yU9kQrQVjeBgJqvHZvsbuo3at2q7itGN1SueRSWerb3qferl6Ve8vq2dCj7se71buxt/+c4fRQbW9uU8eHOxROiZZRy7RllF+YV9geqrSvpNwRObUsblGQHagGswIKAAAYCwToBujokLZuldqOn6z8wrw2f3/zYRGgK+0T4dRQXnL1dvVqzxN7tOfJPVG1+Ik96nmhRxpeIA6koCVQuacsHeAGdJaLAnd+QV5T3z5VbZ9p0+TjJ2vyWyYrNzdXVxAmPAMAgLFAgG6Ajg6pWJTeeMM095K5euHPX1D32m5NPn5ys4dWlXKxrN2/2R1Vi98oKjM5o92P7taOn+xQ39a+/X8gI7Ue3ar25e2a/anZys3O9V+MN/iRmZzRlJOnKL8gH91gIxPfLKM9o9zs+kIyAABAsxGgG6CzM3revFlafOlcbfrvm/Tc//Ocjr//eIVTx2aKvew13wWuuLuozXduVtffdGnvs0NvQ57tzGrGWTPUfnK7ynvL0cV0s7KavGyyWpe2KshzIR0AAEgnAnQDzJ4dPb/+unT00Vkd+71j9eSHntTGGzfqTf/jTYl9rpddr/3v1/TC9S+o8EpB+SPzWnjNQnV+tFPZmdkRf2bfhn3q/m23utd2q+trXSrtLqltWZuOveNYTTtjmrIdWRV3FZWdmR3VbZkBAAAOVwToBpgVL/v8+uvRc8cHOtRxbode/c6rWvyVxTVVa92ji+36NvdFbQ6DQmzxjaJeu+01yaVdv9ylHT/doeLOoqacNkVzPj1HO+7foXWfWad1l65T+1vblZubU+uxrWpZ0KLtP9muXf+2S8Udxf7zzXjfDC28dqGmvG3KkHYKVhABAAA4MAJ0A1QC9GuvDeyb+5m52vqPW/Xad1/T3Evmqtxb1o77dyg3O6f2t7bvdw5316a/3qRN/22TSntKUlnKL8zrLSvforbj2lTYWtDad6/Vnsf3SJKCtkCzPj5L098zXZ3ndcrMtOjLi/TGI29ox093aMdPd2jf7/Zp+8rt8qIrPz+vzo92qu0tbZr69qlqOapF2WkjV6kBAABwYAToBujokIJgoAItSdPfM11TTpuiF65/Qdv/Zbu23rtVUnSr52X//zLN+KMZQ87x4l+8qI1/sVEzPzBTbW9pU25WTpv+2yatfttqzThrhnb/drcKLxe07P8s05STpyjIB8q0ZYacwwLT1FOnauqpU7Xo+kWSomXm+rb2cYc8AACABiFAN0AmE11IODhAm5mW/L9LtPbda7X13q2ac8kctZ/Urpe/8bIeP/Nxtb+tXX2v9ym/MK/s9Ky2/mirZv/JbB3zrWP6g27HOR16/vLn1b22W/k5eS35xhLNPGtmbWNry+wXtAEAAFA/AnSDzJ49tIVDktpPbNcpL5yifev39bdtTDllilafslrF7UVNOXWKejb1qHtNt2ZfPFtHf/PoIVXiloUtWvajZWP5NQAAAHAIBOgGmTVraAW6IpwaDul5nnz8ZL39tbcr055hlQsAAIAJiMV8G+RAAXok4dSQ8AwAADBBEaAbZM4c6dVXpfIBbl0NAACAwwMBukHe9CapUJC6upo9EgAAACSJAN0gRx8dPa9b19xxAAAAIFkE6AYhQAMAAKQDAbpB5syR2toI0AAAAIc7AnSDmEVVaAI0AADA4Y0A3UBLl0pr10ruzR4JAAAAkkKAbqA//EPplVek9eubPRIAAAAkhQDdQO96V/T8s581dxwAAABIDgG6gZYskebPlx58sNkjAQAAQFII0A1kJp16qrRqVbNHAgAAgKQQoBvsxBOlDRuknTubPRIAAAAkgQDdYCedFD2vWdPccQAAACAZBOgGO/HE6Hn16uaOAwAAAMkgQDfYEUdIv/d70ve+J5XLzR4NAAAAGo0AnYDrrpMee0y6995mjwQAAACNRoBOwIUXSosWSbfc0uyRAAAAoNEI0AnIZKSLL5buv5+LCQEAAA43BOiE/MmfSDNmSKecIj30ULNHAwAAgEYhQCdk/nzpqaei5wsukLZubfaIAAAA0AgE6ATNni3ddZe0ZYv0vvdJP/qR5N7sUQEAAGA0CNAJO+kk6dvfljZulD70Iek974nuVAgAAICJiQA9Bi66SHr5Zenmm6VHHpGOOUb6whekBx6gIg0AADDRJBqgzewsM3vOzNab2dUjvJ83sx/E7z9iZouSHE8zZTLSZz4jrVsXrdDxta9J7363dMYZ0jXXSH/7t9KrrzZ7lAAAADgU84RKoGaWkbRO0h9J6pL0qKSPu/vTg475M0lvcffPmNkFkj7k7ucf7LzLly/3VatWJTLmsfTqq9I990j/639JL70U3bUwCKQlS6I7GXZ2RhcgTp0qzZolLV0qzZsXBfEjjpDMmv0NAAAADm9m9pi7L99vf4IB+lRJX3b3fx9vf0mS3P2/DzrmvviYX5tZKOk1SZ1+kEEdLgF6sHJZeu456Yc/lNaujXqkt22LQnaxuP/xnZ1Se7uUz0u5XBS6Z8+Wstlo+1CPWo7LZqPQHgTRI5MZ2EeIBwAAh7MDBegwwc+cJ+mlQdtdkt52oGPcvWhmuyTNlJSqRd+CQDr2WOn664fu371b2rtXevJJadeuqFJdKklPPy319EiFgrRvn7R6tbRzZ7RdeYyFwcE6CKJAXXkM3x5p30jHDH9I1R1zKLWE/bSek++TvnMebt8niXM2+/ugefjfafwIAmnlymaPYqgkA3TDmNklki6RpCOPPLLJoxk77e3RY9as2n7OPapcV8J0X9/QcD38caj33aMqeakUPfr6ovOXy0P3uw99lMuH3jfSMZVH5bsc7FHtfNQyd2k8J98nfeccD9+nmuPTPEdoHv53Gl+CcbjkRZIB+mVJCwZtz4/3jXRMV9zCMVXStuEncvdbJd0qRS0ciYz2MGI20H7R1tbs0QAAABxeksz0j0paYmaLzSwn6QJJK4Yds0LSH8evPyrpgYP1PwMAAADNllgFOu5p/qyk+yRlJH3H3Z8ysxskrXL3FZK+Lel2M1svabuikA0AAACMW4n2QLv7Skkrh+27ftDrHkkfS3IMAAAAQCONw7ZsAAAAYPwiQAMAAAA1IEADAAAANSBAAwAAADUgQAMAAAA1IEADAAAANSBAAwAAADUgQAMAAAA1IEADAAAANSBAAwAAADUgQAMAAAA1IEADAAAANSBAAwAAADUgQAMAAAA1IEADAAAANTB3b/YYamJmWyRtbNLHd0ja2qTPPtwxt8lhbpPD3CaHuU0Oc5sc5jY5zZrbhe7eOXznhAvQzWRmq9x9ebPHcThibpPD3CaHuU0Oc5sc5jY5zG1yxtvc0sIBAAAA1IAADQAAANSAAF2bW5s9gMMYc5sc5jY5zG1ymNvkMLfJYW6TM67mlh5oAAAAoAZUoAEAAIAaEKCrYGZnmdlzZrbezK5u9ngmGjP7jpltNrMnB+2bYWY/NbPn4+fp8X4zs6/Hc/24mZ3UvJGPf2a2wMweNLOnzewpM7s83s/8jpKZtZjZb8xsbTy3fxHvX2xmj8Rz+AMzy8X78/H2+vj9Rc0c/0RgZhkz+62Z/XO8zdw2gJm9aGZPmNkaM1sV7+N3QgOY2TQzu9vMnjWzZ8zsVOZ29MzsmPjfa+XxhpldMZ7nlgB9CGaWkXSTpPdKOk7Sx83suOaOasK5TdJZw/ZdLeln7r5E0s/ibSma5yXx4xJJN4/RGCeqoqQr3f04SadIuiz+98n8jl6vpHe5+/GSTpB0lpmdIumvJf2Nu/+epB2SPh0f/2lJO+L9fxMfh4O7XNIzg7aZ28Z5p7ufMGjZL34nNMbfSvoXd18q6XhF/36Z21Fy9+fif68nSHqrpL2S7tU4nlsC9KGdLGm9u29w94Kk70s6t8ljmlDc/ReStg/bfa6k78avvyvpg4P2/4NHHpY0zczmjM1IJx53f9XdV8evdyv6ZT5PzO+oxXPUHW9m44dLepeku+P9w+e2Mud3S3q3mdkYDXfCMbP5kt4n6Vvxtom5TRK/E0bJzKZK+kNJ35Ykdy+4+04xt432bkm/c/eNGsdzS4A+tHmSXhq03RXvw+jMcvdX49evSZoVv2a+6xT/WftESY+I+W2IuMVgjaTNkn4q6XeSdrp7MT5k8Pz1z238/i5JM8d2xBPK1yT9F0nleHummNtGcUk/MbPHzOySeB+/E0ZvsaQtkv533Hr0LTNrE3PbaBdIujN+PW7nlgCNpvNoKRiWgxkFM5ss6R5JV7j7G4PfY37r5+6l+E+K8xX9NWppk4d0WDCz90va7O6PNXssh6l3uPtJiv7MfZmZ/eHgN/mdULdQ0kmSbnb3EyXt0UBLgSTmdrTi6x7OkfTD4e+Nt7klQB/ay5IWDNqeH+/D6Lxe+XNL/Lw53s9818jMsorC8x3u/o/xbua3geI/0z4o6VRFfyoM47cGz1//3MbvT5W0bYyHOlGcJukcM3tRUVvcuxT1ljK3DeDuL8fPmxX1kZ4sfic0QpekLnd/JN6+W1GgZm4b572SVrv76/H2uJ1bAvShPSppSXx1eE7RnxZWNHlMh4MVkv44fv3Hkv5p0P7/EF9he4qkXYP+fINh4j7Qb0t6xt2/Ougt5neUzKzTzKbFrydJ+iNFPeYPSvpofNjwua3M+UclPeAstD8id/+Su89390WKfqc+4O4XirkdNTNrM7P2ymtJZ0p6UvxOGDV3f03SS2Z2TLzr3ZKeFnPbSB/XQPuGNI7nlhupVMHMzlbUr5eR9B13v7HJQ5pQzOxOSWdI6pD0uqT/KulHku6SdKSkjZLOc/ftcSD8hqJVO/ZKutjdVzVj3BOBmb1D0r9JekIDvaTXKOqDZn5HwczeouiilYyiYsNd7n6DmR2lqGo6Q9JvJV3k7r1m1iLpdkV96NslXeDuG5oz+onDzM6Q9J/d/f3M7ejFc3hvvBlK+p6732hmM8XvhFEzsxMUXfiak7RB0sWKfz+IuR2V+P/wbZJ0lLvviveN23+3BGgAAACgBrRwAAAAADUgQAMAAAA1IEADAAAANSBAAwAAADUgQAMAAAA1IEADAGRmZ5jZPzd7HAAwERCgAQAAgBoQoAFgAjGzi8zsN2a2xsy+aWYZM+s2s78xs6fM7Gdm1hkfe4KZPWxmj5vZvWY2Pd7/e2Z2v5mtNbPVZvam+PSTzexuM3vWzO6Ib1YAABiGAA0AE4SZHSvpfEmnufsJkkqSLpTUJmmVu/++pH9VdLdPSfoHSV9097coultlZf8dkm5y9+MlvV1S5Ra4J0q6QtJxko6SdFriXwoAJqCw2QMAAFTt3ZLeKunRuDg8SdJmRbdx/0F8zP8n6R/NbKqkae7+r/H+70r6oZm1S5rn7vdKkrv3SFJ8vt+4e1e8vUbSIkkPJf+1AGBiIUADwMRhkr7r7l8astPsumHHeZ3n7x30uiT+GwEAI6KFAwAmjp9J+qiZHSFJZjbDzBYq+l3+0fiYT0h6yN13SdphZqfH+z8p6V/dfbekLjP7YHyOvJm1jum3AIAJjuoCAEwQ7v60mV0r6SdmFkjqk3SZpD2STo7f26yoT1qS/ljSLXFA3iDp4nj/JyV908xuiM/xsTH8GgAw4Zl7vX/pAwCMB2bW7e6Tmz0OAEgLWjgAAACAGlCBBgAAAGpABRoAAACoAQEaAAAAqAEBGgAAAKgBARoAAACoAQEaAAAAqAEBGgAAAKjB/wWmEoHznVWWjwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "テストデータに対する予測精度：1.0\n\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGESjKehZJbZ"
   },
   "source": "# ニューラルネットワークモデルを改良する"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZKt3A2lQrTDM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652230002811,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# Early-Stopping機能を実装したクラス\nclass EarlyStopping:\n\n  def __init__(self, patience=0):\n    self._step = 0              # lossが改善しなかった連続回数をカウントする。先頭の_は内部的であることを表わしています\n    self._loss = float('inf')   # そこまでで最も改善が見られたlossの値を格納する\n    self.patience = patience    # 引数で指定する、何回改善されなかったら早期終了するかの回数\n\n  def __call__(self, loss):\n    if self._loss < loss:\n      self._step += 1   # lossが改善しなければ_stepを1増やす\n      if self._step >= self.patience:    # patienceの回数改善しなかったら早期終了する\n        print('early stopping')\n        return True\n    else:               # lossが改善した場合は_stepを0にして_lossを更新する\n      self._step = 0\n      self._loss = loss\n    return False",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RPAPSofarWkb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652230002811,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    }
   },
   "source": "# 乱数シードの固定\ntorch.manual_seed(0)\n\n# ニューラルネットワークを定義\nclass Net(torch.nn.Module):\n\n  # 必要な層や活性化関数を定義する\n  def __init__(self):\n    super(Net, self).__init__()\n    self.l1 = torch.nn.Linear(train_x.shape[1], 128)     # 中間層1\n    self.a1 = torch.nn.ReLU()             # 活性化関数1\n    self.d1 = torch.nn.Dropout(0.2)       # ★ドロップアウト層1★\n    self.l2 = torch.nn.Linear(128, 128)   # 中間層2\n    self.a2 = torch.nn.ReLU()             # 活性化関数2\n    self.d2 = torch.nn.Dropout(0.2)       # ★ドロップアウト層2★\n    self.l3 = torch.nn.Linear(128, 3)     # 出力層\n\n  # 順伝搬を定義。引数のxは、説明変数。\n  # 順番に関数を実行し、その結果を次の関数に渡していく\n  def forward(self, x):\n    x = self.l1(x)\n    x = self.a1(x)\n    x = self.d1(x) # ★ドロップアウト層1★\n    x = self.l2(x)\n    x = self.a2(x)\n    x = self.d2(x) # ★ドロップアウト層2★\n    x = self.l3(x)\n    return x",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cdJnAbAL3KV3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652230006890,
     "user_tz": -540,
     "elapsed": 4082,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "fdf96f1d-4e1c-4f11-cf62-943ddfc99564"
   },
   "source": "num_epochs = 700\n\n# データローダーの用意\n# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n# shuffleをTrueに設定することで、データをシャッフルして取り出します\nbatch_size = 32\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# ★早期終了のインスタンスを準備★\nes = EarlyStopping(patience=10)\n\n# モデルをインスタンス化\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01) # 最適化手法の用意\ncriterion = torch.nn.CrossEntropyLoss() # ★誤差関数の用意(多値分類なので、交差エントロピー誤差を使う)\n\n## 学習時に経過情報を保存する空リストを作成\ntrain_loss_list = []      # 学習データの誤差関数用リスト\nval_loss_list = []        # 検証データの誤差関数用リスト\n\n# エポック分の繰り返し\nfor epoch in range(num_epochs):\n    \n    #学習の進行状況を表示\n    print('--------')\n    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n\n    # 損失の初期化\n    train_loss = 0        # 学習データの誤差関数\n    val_loss = 0          # 検証データの誤差関数\n    \n    #=====学習パート=======\n    # 学習モードに設定\n    # PyTorchでは学習時と評価時でモードを切り替える\n    net.train()\n\n    #ミニバッチごとにデータをロードして学習\n    for x, y in train_dataloader:\n        preds = net(x)                            # 順伝搬で予測を実行\n        loss = criterion(preds, y)                # 誤差関数を計算\n        optimizer.zero_grad()                     # 勾配を初期化\n        loss.backward()                           # 勾配を計算\n        optimizer.step()                          # パラメータ更新\n        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n    #ミニバッチの平均の損失を計算\n    batch_train_loss = train_loss / len(train_dataloader)\n    \n    #=====評価パート(検証データ)=======\n    # 評価モードに設定\n    net.eval()\n    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n    with torch.no_grad():\n        for x, y in val_dataloader:\n            preds = net(x)                        # 順伝搬で予測を実行\n            loss = criterion(preds, y)            # 誤差関数を計算\n            val_loss += loss.item()               # ミニバッチごとの損失を格納    \n    #ミニバッチの平均の損失を計算\n    batch_val_loss = val_loss / len(val_dataloader)\n    \n    #エポックごとに損失を表示\n    print(\"Train_Loss: {:.4f}\".format(batch_train_loss))\n    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n    #損失をリスト化して保存\n    train_loss_list.append(batch_train_loss)\n    val_loss_list.append(batch_val_loss)\n\n    # ★早期終了判定★\n    if es(batch_val_loss):\n      break",
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------\nEpoch: 1/700\nTrain_Loss: 1.0931\nval_loss: 1.0833\n--------\nEpoch: 2/700\nTrain_Loss: 1.0699\nval_loss: 1.0674\n--------\nEpoch: 3/700\nTrain_Loss: 1.0595\nval_loss: 1.0520\n--------\nEpoch: 4/700\nTrain_Loss: 1.0496\nval_loss: 1.0373\n--------\nEpoch: 5/700\nTrain_Loss: 1.0293\nval_loss: 1.0216\n--------\nEpoch: 6/700\nTrain_Loss: 1.0099\nval_loss: 1.0071\n--------\nEpoch: 7/700\nTrain_Loss: 1.0104\nval_loss: 0.9925\n--------\nEpoch: 8/700\nTrain_Loss: 0.9938\nval_loss: 0.9788\n--------\nEpoch: 9/700\nTrain_Loss: 0.9681\nval_loss: 0.9648\n--------\nEpoch: 10/700\nTrain_Loss: 0.9542\nval_loss: 0.9514\n--------\nEpoch: 11/700\nTrain_Loss: 0.9302\nval_loss: 0.9363\n--------\nEpoch: 12/700\nTrain_Loss: 0.9310\nval_loss: 0.9218\n--------\nEpoch: 13/700\nTrain_Loss: 0.9076\nval_loss: 0.9067\n--------\nEpoch: 14/700\nTrain_Loss: 0.9028\nval_loss: 0.8924\n--------\nEpoch: 15/700\nTrain_Loss: 0.8752\nval_loss: 0.8775\n--------\nEpoch: 16/700\nTrain_Loss: 0.8640\nval_loss: 0.8633\n--------\nEpoch: 17/700\nTrain_Loss: 0.8424\nval_loss: 0.8492\n--------\nEpoch: 18/700\nTrain_Loss: 0.8257\nval_loss: 0.8345\n--------\nEpoch: 19/700\nTrain_Loss: 0.8172\nval_loss: 0.8202\n--------\nEpoch: 20/700\nTrain_Loss: 0.8028\nval_loss: 0.8054\n--------\nEpoch: 21/700\nTrain_Loss: 0.7951\nval_loss: 0.7903\n--------\nEpoch: 22/700\nTrain_Loss: 0.7590\nval_loss: 0.7740\n--------\nEpoch: 23/700\nTrain_Loss: 0.7624\nval_loss: 0.7591\n--------\nEpoch: 24/700\nTrain_Loss: 0.7278\nval_loss: 0.7436\n--------\nEpoch: 25/700\nTrain_Loss: 0.7348\nval_loss: 0.7288\n--------\nEpoch: 26/700\nTrain_Loss: 0.6946\nval_loss: 0.7134\n--------\nEpoch: 27/700\nTrain_Loss: 0.6949\nval_loss: 0.6986\n--------\nEpoch: 28/700\nTrain_Loss: 0.6739\nval_loss: 0.6836\n--------\nEpoch: 29/700\nTrain_Loss: 0.6648\nval_loss: 0.6690\n--------\nEpoch: 30/700\nTrain_Loss: 0.6328\nval_loss: 0.6540\n--------\nEpoch: 31/700\nTrain_Loss: 0.6389\nval_loss: 0.6391\n--------\nEpoch: 32/700\nTrain_Loss: 0.6096\nval_loss: 0.6245\n--------\nEpoch: 33/700\nTrain_Loss: 0.5759\nval_loss: 0.6096\n--------\nEpoch: 34/700\nTrain_Loss: 0.5742\nval_loss: 0.5950\n--------\nEpoch: 35/700\nTrain_Loss: 0.5661\nval_loss: 0.5808\n--------\nEpoch: 36/700\nTrain_Loss: 0.5348\nval_loss: 0.5665\n--------\nEpoch: 37/700\nTrain_Loss: 0.5215\nval_loss: 0.5527\n--------\nEpoch: 38/700\nTrain_Loss: 0.5219\nval_loss: 0.5391\n--------\nEpoch: 39/700\nTrain_Loss: 0.5003\nval_loss: 0.5259\n--------\nEpoch: 40/700\nTrain_Loss: 0.4869\nval_loss: 0.5130\n--------\nEpoch: 41/700\nTrain_Loss: 0.4787\nval_loss: 0.5003\n--------\nEpoch: 42/700\nTrain_Loss: 0.4705\nval_loss: 0.4877\n--------\nEpoch: 43/700\nTrain_Loss: 0.4559\nval_loss: 0.4756\n--------\nEpoch: 44/700\nTrain_Loss: 0.4488\nval_loss: 0.4639\n--------\nEpoch: 45/700\nTrain_Loss: 0.4333\nval_loss: 0.4525\n--------\nEpoch: 46/700\nTrain_Loss: 0.4073\nval_loss: 0.4413\n--------\nEpoch: 47/700\nTrain_Loss: 0.4127\nval_loss: 0.4304\n--------\nEpoch: 48/700\nTrain_Loss: 0.4129\nval_loss: 0.4200\n--------\nEpoch: 49/700\nTrain_Loss: 0.4041\nval_loss: 0.4099\n--------\nEpoch: 50/700\nTrain_Loss: 0.3684\nval_loss: 0.3995\n--------\nEpoch: 51/700\nTrain_Loss: 0.3555\nval_loss: 0.3900\n--------\nEpoch: 52/700\nTrain_Loss: 0.3675\nval_loss: 0.3806\n--------\nEpoch: 53/700\nTrain_Loss: 0.3426\nval_loss: 0.3711\n--------\nEpoch: 54/700\nTrain_Loss: 0.3530\nval_loss: 0.3623\n--------\nEpoch: 55/700\nTrain_Loss: 0.3499\nval_loss: 0.3541\n--------\nEpoch: 56/700\nTrain_Loss: 0.3204\nval_loss: 0.3458\n--------\nEpoch: 57/700\nTrain_Loss: 0.3126\nval_loss: 0.3379\n--------\nEpoch: 58/700\nTrain_Loss: 0.3052\nval_loss: 0.3302\n--------\nEpoch: 59/700\nTrain_Loss: 0.2927\nval_loss: 0.3227\n--------\nEpoch: 60/700\nTrain_Loss: 0.2892\nval_loss: 0.3158\n--------\nEpoch: 61/700\nTrain_Loss: 0.2781\nval_loss: 0.3090\n--------\nEpoch: 62/700\nTrain_Loss: 0.2809\nval_loss: 0.3022\n--------\nEpoch: 63/700\nTrain_Loss: 0.2487\nval_loss: 0.2958\n--------\nEpoch: 64/700\nTrain_Loss: 0.2555\nval_loss: 0.2896\n--------\nEpoch: 65/700\nTrain_Loss: 0.2598\nval_loss: 0.2836\n--------\nEpoch: 66/700\nTrain_Loss: 0.2480\nval_loss: 0.2779\n--------\nEpoch: 67/700\nTrain_Loss: 0.2456\nval_loss: 0.2722\n--------\nEpoch: 68/700\nTrain_Loss: 0.2433\nval_loss: 0.2670\n--------\nEpoch: 69/700\nTrain_Loss: 0.2389\nval_loss: 0.2617\n--------\nEpoch: 70/700\nTrain_Loss: 0.2229\nval_loss: 0.2567\n--------\nEpoch: 71/700\nTrain_Loss: 0.2351\nval_loss: 0.2518\n--------\nEpoch: 72/700\nTrain_Loss: 0.2192\nval_loss: 0.2472\n--------\nEpoch: 73/700\nTrain_Loss: 0.2203\nval_loss: 0.2426\n--------\nEpoch: 74/700\nTrain_Loss: 0.2103\nval_loss: 0.2382\n--------\nEpoch: 75/700\nTrain_Loss: 0.2099\nval_loss: 0.2341\n--------\nEpoch: 76/700\nTrain_Loss: 0.1981\nval_loss: 0.2299\n--------\nEpoch: 77/700\nTrain_Loss: 0.1941\nval_loss: 0.2262\n--------\nEpoch: 78/700\nTrain_Loss: 0.1987\nval_loss: 0.2219\n--------\nEpoch: 79/700\nTrain_Loss: 0.2201\nval_loss: 0.2182\n--------\nEpoch: 80/700\nTrain_Loss: 0.1977\nval_loss: 0.2147\n--------\nEpoch: 81/700\nTrain_Loss: 0.1996\nval_loss: 0.2115\n--------\nEpoch: 82/700\nTrain_Loss: 0.1735\nval_loss: 0.2082\n--------\nEpoch: 83/700\nTrain_Loss: 0.1745\nval_loss: 0.2049\n--------\nEpoch: 84/700\nTrain_Loss: 0.1653\nval_loss: 0.2021\n--------\nEpoch: 85/700\nTrain_Loss: 0.1753\nval_loss: 0.1994\n--------\nEpoch: 86/700\nTrain_Loss: 0.1747\nval_loss: 0.1965\n--------\nEpoch: 87/700\nTrain_Loss: 0.1512\nval_loss: 0.1937\n--------\nEpoch: 88/700\nTrain_Loss: 0.1507\nval_loss: 0.1914\n--------\nEpoch: 89/700\nTrain_Loss: 0.1957\nval_loss: 0.1884\n--------\nEpoch: 90/700\nTrain_Loss: 0.1638\nval_loss: 0.1859\n--------\nEpoch: 91/700\nTrain_Loss: 0.1539\nval_loss: 0.1829\n--------\nEpoch: 92/700\nTrain_Loss: 0.1674\nval_loss: 0.1801\n--------\nEpoch: 93/700\nTrain_Loss: 0.1500\nval_loss: 0.1779\n--------\nEpoch: 94/700\nTrain_Loss: 0.1634\nval_loss: 0.1756\n--------\nEpoch: 95/700\nTrain_Loss: 0.1419\nval_loss: 0.1735\n--------\nEpoch: 96/700\nTrain_Loss: 0.1412\nval_loss: 0.1715\n--------\nEpoch: 97/700\nTrain_Loss: 0.1471\nval_loss: 0.1697\n--------\nEpoch: 98/700\nTrain_Loss: 0.1393\nval_loss: 0.1679\n--------\nEpoch: 99/700\nTrain_Loss: 0.1329\nval_loss: 0.1664\n--------\nEpoch: 100/700\nTrain_Loss: 0.1353\nval_loss: 0.1647\n--------\nEpoch: 101/700\nTrain_Loss: 0.1364\nval_loss: 0.1632\n--------\nEpoch: 102/700\nTrain_Loss: 0.1331\nval_loss: 0.1616\n--------\nEpoch: 103/700\nTrain_Loss: 0.1175\nval_loss: 0.1596\n--------\nEpoch: 104/700\nTrain_Loss: 0.1226\nval_loss: 0.1581\n--------\nEpoch: 105/700\nTrain_Loss: 0.1267\nval_loss: 0.1568\n--------\nEpoch: 106/700\nTrain_Loss: 0.1041\nval_loss: 0.1554\n--------\nEpoch: 107/700\nTrain_Loss: 0.1343\nval_loss: 0.1545\n--------\nEpoch: 108/700\nTrain_Loss: 0.1167\nval_loss: 0.1531\n--------\nEpoch: 109/700\nTrain_Loss: 0.1280\nval_loss: 0.1510\n--------\nEpoch: 110/700\nTrain_Loss: 0.1178\nval_loss: 0.1494\n--------\nEpoch: 111/700\nTrain_Loss: 0.1074\nval_loss: 0.1479\n--------\nEpoch: 112/700\nTrain_Loss: 0.1147\nval_loss: 0.1461\n--------\nEpoch: 113/700\nTrain_Loss: 0.0932\nval_loss: 0.1446\n--------\nEpoch: 114/700\nTrain_Loss: 0.1114\nval_loss: 0.1430\n--------\nEpoch: 115/700\nTrain_Loss: 0.1034\nval_loss: 0.1418\n--------\nEpoch: 116/700\nTrain_Loss: 0.1083\nval_loss: 0.1403\n--------\nEpoch: 117/700\nTrain_Loss: 0.1122\nval_loss: 0.1398\n--------\nEpoch: 118/700\nTrain_Loss: 0.1239\nval_loss: 0.1392\n--------\nEpoch: 119/700\nTrain_Loss: 0.0954\nval_loss: 0.1382\n--------\nEpoch: 120/700\nTrain_Loss: 0.1050\nval_loss: 0.1376\n--------\nEpoch: 121/700\nTrain_Loss: 0.1029\nval_loss: 0.1368\n--------\nEpoch: 122/700\nTrain_Loss: 0.1032\nval_loss: 0.1363\n--------\nEpoch: 123/700\nTrain_Loss: 0.0915\nval_loss: 0.1352\n--------\nEpoch: 124/700\nTrain_Loss: 0.1141\nval_loss: 0.1332\n--------\nEpoch: 125/700\nTrain_Loss: 0.0959\nval_loss: 0.1322\n--------\nEpoch: 126/700\nTrain_Loss: 0.0970\nval_loss: 0.1310\n--------\nEpoch: 127/700\nTrain_Loss: 0.0967\nval_loss: 0.1300\n--------\nEpoch: 128/700\nTrain_Loss: 0.0800\nval_loss: 0.1289\n--------\nEpoch: 129/700\nTrain_Loss: 0.0743\nval_loss: 0.1279\n--------\nEpoch: 130/700\nTrain_Loss: 0.0880\nval_loss: 0.1273\n--------\nEpoch: 131/700\nTrain_Loss: 0.0865\nval_loss: 0.1267\n--------\nEpoch: 132/700\nTrain_Loss: 0.0800\nval_loss: 0.1258\n--------\nEpoch: 133/700\nTrain_Loss: 0.0945\nval_loss: 0.1258\n--------\nEpoch: 134/700\nTrain_Loss: 0.0906\nval_loss: 0.1246\n--------\nEpoch: 135/700\nTrain_Loss: 0.0782\nval_loss: 0.1235\n--------\nEpoch: 136/700\nTrain_Loss: 0.0804\nval_loss: 0.1229\n--------\nEpoch: 137/700\nTrain_Loss: 0.0861\nval_loss: 0.1219\n--------\nEpoch: 138/700\nTrain_Loss: 0.0831\nval_loss: 0.1206\n--------\nEpoch: 139/700\nTrain_Loss: 0.0912\nval_loss: 0.1199\n--------\nEpoch: 140/700\nTrain_Loss: 0.1083\nval_loss: 0.1192\n--------\nEpoch: 141/700\nTrain_Loss: 0.0732\nval_loss: 0.1188\n--------\nEpoch: 142/700\nTrain_Loss: 0.0887\nval_loss: 0.1178\n--------\nEpoch: 143/700\nTrain_Loss: 0.0900\nval_loss: 0.1172\n--------\nEpoch: 144/700\nTrain_Loss: 0.0896\nval_loss: 0.1165\n--------\nEpoch: 145/700\nTrain_Loss: 0.0770\nval_loss: 0.1158\n--------\nEpoch: 146/700\nTrain_Loss: 0.1003\nval_loss: 0.1147\n--------\nEpoch: 147/700\nTrain_Loss: 0.0956\nval_loss: 0.1147\n--------\nEpoch: 148/700\nTrain_Loss: 0.0793\nval_loss: 0.1146\n--------\nEpoch: 149/700\nTrain_Loss: 0.0769\nval_loss: 0.1131\n--------\nEpoch: 150/700\nTrain_Loss: 0.0740\nval_loss: 0.1121\n--------\nEpoch: 151/700\nTrain_Loss: 0.0669\nval_loss: 0.1114\n--------\nEpoch: 152/700\nTrain_Loss: 0.0801\nval_loss: 0.1108\n--------\nEpoch: 153/700\nTrain_Loss: 0.0694\nval_loss: 0.1105\n--------\nEpoch: 154/700\nTrain_Loss: 0.0721\nval_loss: 0.1091\n--------\nEpoch: 155/700\nTrain_Loss: 0.0641\nval_loss: 0.1090\n--------\nEpoch: 156/700\nTrain_Loss: 0.0810\nval_loss: 0.1094\n--------\nEpoch: 157/700\nTrain_Loss: 0.0705\nval_loss: 0.1089\n--------\nEpoch: 158/700\nTrain_Loss: 0.0691\nval_loss: 0.1085\n--------\nEpoch: 159/700\nTrain_Loss: 0.0645\nval_loss: 0.1083\n--------\nEpoch: 160/700\nTrain_Loss: 0.0774\nval_loss: 0.1077\n--------\nEpoch: 161/700\nTrain_Loss: 0.0680\nval_loss: 0.1076\n--------\nEpoch: 162/700\nTrain_Loss: 0.0633\nval_loss: 0.1068\n--------\nEpoch: 163/700\nTrain_Loss: 0.0519\nval_loss: 0.1061\n--------\nEpoch: 164/700\nTrain_Loss: 0.0641\nval_loss: 0.1060\n--------\nEpoch: 165/700\nTrain_Loss: 0.0640\nval_loss: 0.1064\n--------\nEpoch: 166/700\nTrain_Loss: 0.0578\nval_loss: 0.1067\n--------\nEpoch: 167/700\nTrain_Loss: 0.0665\nval_loss: 0.1061\n--------\nEpoch: 168/700\nTrain_Loss: 0.0597\nval_loss: 0.1056\n--------\nEpoch: 169/700\nTrain_Loss: 0.0510\nval_loss: 0.1051\n--------\nEpoch: 170/700\nTrain_Loss: 0.0606\nval_loss: 0.1042\n--------\nEpoch: 171/700\nTrain_Loss: 0.0582\nval_loss: 0.1044\n--------\nEpoch: 172/700\nTrain_Loss: 0.0596\nval_loss: 0.1039\n--------\nEpoch: 173/700\nTrain_Loss: 0.0647\nval_loss: 0.1032\n--------\nEpoch: 174/700\nTrain_Loss: 0.0566\nval_loss: 0.1026\n--------\nEpoch: 175/700\nTrain_Loss: 0.0727\nval_loss: 0.1026\n--------\nEpoch: 176/700\nTrain_Loss: 0.0670\nval_loss: 0.1020\n--------\nEpoch: 177/700\nTrain_Loss: 0.0537\nval_loss: 0.1017\n--------\nEpoch: 178/700\nTrain_Loss: 0.0648\nval_loss: 0.1017\n--------\nEpoch: 179/700\nTrain_Loss: 0.0544\nval_loss: 0.1015\n--------\nEpoch: 180/700\nTrain_Loss: 0.0468\nval_loss: 0.1011\n--------\nEpoch: 181/700\nTrain_Loss: 0.0539\nval_loss: 0.0996\n--------\nEpoch: 182/700\nTrain_Loss: 0.0671\nval_loss: 0.0981\n--------\nEpoch: 183/700\nTrain_Loss: 0.0708\nval_loss: 0.0992\n--------\nEpoch: 184/700\nTrain_Loss: 0.0580\nval_loss: 0.0991\n--------\nEpoch: 185/700\nTrain_Loss: 0.0585\nval_loss: 0.0983\n--------\nEpoch: 186/700\nTrain_Loss: 0.0714\nval_loss: 0.0977\n--------\nEpoch: 187/700\nTrain_Loss: 0.0507\nval_loss: 0.0975\n--------\nEpoch: 188/700\nTrain_Loss: 0.0481\nval_loss: 0.0972\n--------\nEpoch: 189/700\nTrain_Loss: 0.0715\nval_loss: 0.0981\n--------\nEpoch: 190/700\nTrain_Loss: 0.0626\nval_loss: 0.0978\n--------\nEpoch: 191/700\nTrain_Loss: 0.0450\nval_loss: 0.0973\n--------\nEpoch: 192/700\nTrain_Loss: 0.0623\nval_loss: 0.0954\n--------\nEpoch: 193/700\nTrain_Loss: 0.0565\nval_loss: 0.0955\n--------\nEpoch: 194/700\nTrain_Loss: 0.0514\nval_loss: 0.0945\n--------\nEpoch: 195/700\nTrain_Loss: 0.0695\nval_loss: 0.0955\n--------\nEpoch: 196/700\nTrain_Loss: 0.0649\nval_loss: 0.0969\n--------\nEpoch: 197/700\nTrain_Loss: 0.0473\nval_loss: 0.0969\n--------\nEpoch: 198/700\nTrain_Loss: 0.0596\nval_loss: 0.0961\n--------\nEpoch: 199/700\nTrain_Loss: 0.0548\nval_loss: 0.0955\n--------\nEpoch: 200/700\nTrain_Loss: 0.0579\nval_loss: 0.0941\n--------\nEpoch: 201/700\nTrain_Loss: 0.0410\nval_loss: 0.0936\n--------\nEpoch: 202/700\nTrain_Loss: 0.0612\nval_loss: 0.0929\n--------\nEpoch: 203/700\nTrain_Loss: 0.0454\nval_loss: 0.0926\n--------\nEpoch: 204/700\nTrain_Loss: 0.0580\nval_loss: 0.0926\n--------\nEpoch: 205/700\nTrain_Loss: 0.0551\nval_loss: 0.0928\n--------\nEpoch: 206/700\nTrain_Loss: 0.0553\nval_loss: 0.0926\n--------\nEpoch: 207/700\nTrain_Loss: 0.0531\nval_loss: 0.0927\n--------\nEpoch: 208/700\nTrain_Loss: 0.0469\nval_loss: 0.0921\n--------\nEpoch: 209/700\nTrain_Loss: 0.0631\nval_loss: 0.0921\n--------\nEpoch: 210/700\nTrain_Loss: 0.0563\nval_loss: 0.0917\n--------\nEpoch: 211/700\nTrain_Loss: 0.0449\nval_loss: 0.0918\n--------\nEpoch: 212/700\nTrain_Loss: 0.0584\nval_loss: 0.0919\n--------\nEpoch: 213/700\nTrain_Loss: 0.0628\nval_loss: 0.0927\n--------\nEpoch: 214/700\nTrain_Loss: 0.0548\nval_loss: 0.0923\n--------\nEpoch: 215/700\nTrain_Loss: 0.0500\nval_loss: 0.0927\n--------\nEpoch: 216/700\nTrain_Loss: 0.0472\nval_loss: 0.0920\n--------\nEpoch: 217/700\nTrain_Loss: 0.0557\nval_loss: 0.0908\n--------\nEpoch: 218/700\nTrain_Loss: 0.0418\nval_loss: 0.0908\n--------\nEpoch: 219/700\nTrain_Loss: 0.0321\nval_loss: 0.0908\n--------\nEpoch: 220/700\nTrain_Loss: 0.0423\nval_loss: 0.0907\n--------\nEpoch: 221/700\nTrain_Loss: 0.0431\nval_loss: 0.0912\n--------\nEpoch: 222/700\nTrain_Loss: 0.0389\nval_loss: 0.0915\n--------\nEpoch: 223/700\nTrain_Loss: 0.0509\nval_loss: 0.0929\n--------\nEpoch: 224/700\nTrain_Loss: 0.0456\nval_loss: 0.0928\n--------\nEpoch: 225/700\nTrain_Loss: 0.0469\nval_loss: 0.0931\n--------\nEpoch: 226/700\nTrain_Loss: 0.0452\nval_loss: 0.0933\n--------\nEpoch: 227/700\nTrain_Loss: 0.0451\nval_loss: 0.0923\n--------\nEpoch: 228/700\nTrain_Loss: 0.0431\nval_loss: 0.0916\n--------\nEpoch: 229/700\nTrain_Loss: 0.0453\nval_loss: 0.0923\n--------\nEpoch: 230/700\nTrain_Loss: 0.0498\nval_loss: 0.0923\nearly stopping\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "ogecq8SSsVBo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652230007395,
     "user_tz": -540,
     "elapsed": 516,
     "user": {
      "displayName": "廣岡雅人",
      "userId": "09611567210500329356"
     }
    },
    "outputId": "ec546afa-ef02-4462-fd86-4aab7c79e11a"
   },
   "source": "myevaluete()",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 864x576 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "テストデータに対する予測精度：1.0\n\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u17Oo46jc3yC"
   },
   "source": "※ ニューラルネットワークの構成によっては、結果が97.2%もしくは100%になるかもしれません。これはテストデータを切り出す際に全体の20%で指定をすると36件のデータがテストデータとなり、そのうちの35件を正しく予測できると97.2%になるため、35件あてられたか、36件あてられたかの違いです。<br>\n分類問題の精度は正解率なので、0か1のやや極端な値です。ギリギリ正解だったのか、余裕で正解だったのかは考慮されていません。そういった関係から直感的にはわかりにくいものの、誤差関数のほうが評価指標として優先される場合もあります。"
  }
 ]
}