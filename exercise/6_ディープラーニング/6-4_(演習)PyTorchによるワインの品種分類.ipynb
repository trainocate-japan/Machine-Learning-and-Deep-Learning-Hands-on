{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ef8970",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/trainocate-japan/Machine-Learning-and-Deep-Learning-Hands-on/blob/main/exercise/6_ディープラーニング/6-4_(演習)PyTorchによるワインの品種分類.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0Ic1oCRl4eU"
   },
   "source": [
    "# 6-3_PyTorchによるワインの品種分類\n",
    "このノートブックでは、PyTorchで分類の予測モデルを作成します。<br>\n",
    "予測を行うテーマはwineの成分からブドウの品種を予測することです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWo_M2swG3es"
   },
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eqzg8G6S-ZBk"
   },
   "outputs": [],
   "source": [
    "# データを処理するための基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learnから必要なライブラリをインポート\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets # scikit-learnからデータセットをインポートするためのライブラリ\n",
    "\n",
    "# PyTorchで使用\n",
    "import torch\n",
    "# PyTorchのインポート設定は色々なスタイルがありますが、事前にインポートするとKerasに比べるて非常に多くのインポートを書く必要がある為、\n",
    "# 今回は使用するたびに全て記載するスタイルをとります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8YM4qhJDxRA"
   },
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR4ZpobEEoSZ"
   },
   "outputs": [],
   "source": [
    "# データの読込\n",
    "wine_dataset = datasets.load_wine()\n",
    "# データセットのキー項目を確認\n",
    "print(wine_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUrXjn4hQnNV"
   },
   "outputs": [],
   "source": [
    "# データセットの説明はDESCRに格納されています。\n",
    "print(wine_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPq8eVOtGkWc"
   },
   "outputs": [],
   "source": [
    "# 目的変数となる変数名と、説明変数となる変数名を確認する\n",
    "print(wine_dataset['target_names'])\n",
    "print(wine_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scdbmPTaltHq"
   },
   "outputs": [],
   "source": [
    "# 説明変数となるdataと目的変数となるtargetを確認する\n",
    "print(type(wine_dataset['data']))\n",
    "print(wine_dataset['data'].shape)\n",
    "print(type(wine_dataset['target']))\n",
    "print(wine_dataset['target'].shape)\n",
    "# 178行のデータがそれぞれ格納されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBg_vl4VJ6-S"
   },
   "outputs": [],
   "source": [
    "# 説明変数と目的変数をpandasのDataFrameに変換して列名をつけて取り出す\n",
    "wine_dataset_x = pd.DataFrame(wine_dataset['data'], columns=wine_dataset['feature_names'])\n",
    "wine_dataset_y = pd.DataFrame(wine_dataset['target'], columns=['Class'])\n",
    "display(wine_dataset_x.head(3))\n",
    "display(wine_dataset_y.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYknsfuGMTmv"
   },
   "outputs": [],
   "source": [
    "# データの概要を把握します。\n",
    "# 説明変数と目的変数のDataFrameを結合します。\n",
    "wine = pd.concat([wine_dataset_x, wine_dataset_y], axis=1)\n",
    "\n",
    "print('\\n--要約--\\n')\n",
    "display(wine.info())\n",
    "print('\\n--統計情報--\\n')\n",
    "display(wine.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI1jf8LMJ-1_"
   },
   "outputs": [],
   "source": [
    "# 目的変数を確認してみる\n",
    "print(wine_dataset['target'])\n",
    "# 0,1,2は先ほど表示したfeature_namesに対応している。(今回は品種名までは書かれていない)\n",
    "# 0:class_0、1:class_1、2:class_2\n",
    "# ★PyTorchの多値分類では正解データを0,1,2,・・・というラベルの表現のまま渡すので、one-hot表現は不要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPBJe7AwBorC"
   },
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割（80%を訓練用に使用）\n",
    "train_x, test_x, train_y, test_y = train_test_split(wine_dataset_x, wine_dataset['target'], train_size=0.8, test_size=0.2, random_state=0, stratify=wine_dataset['target']) \n",
    "# さらに訓練データを検証データに分割（訓練データの20%を検証用に使用）\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, train_size=0.8, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5vwfnuFZWBG"
   },
   "outputs": [],
   "source": [
    "# 標準化を行う\n",
    "# 標準化はscikit-learnに用意されている\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(train_x)\n",
    "train_x_scaled = scaler_x.transform(train_x)\n",
    "val_x_scaled = scaler_x.transform(val_x)\n",
    "test_x_scaled = scaler_x.transform(test_x)\n",
    "\n",
    "print(type(train_x_scaled))\n",
    "print(train_x_scaled[:2])\n",
    "print(type(val_x_scaled))\n",
    "print(val_x_scaled[:2])\n",
    "print(type(test_x_scaled))\n",
    "print(test_x_scaled[:2])\n",
    "# scikit-learnのStandardScalerを使用するとnumpy配列になるので、説明変数はpandasのデータフレームからnumpyへの変換は不要\n",
    "# 目的変数もデータセットから直接取得している場合にはnumpy配列なので変換は不要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFs0k_b7ocDI"
   },
   "source": [
    "#### データをtorchtensorに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6VCLmF-qloa"
   },
   "outputs": [],
   "source": [
    "# numpy配列をテンソルに変換\n",
    "# ★多値分類では回帰とはデータの準備が異なる部分があります。 \n",
    "# 目的変数はone-hot表現にはせず、N行1列の形にもせず、1次配列の形で用意します。\n",
    "train_x_scaled = torch.Tensor(train_x_scaled).float()\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "val_x_scaled = torch.Tensor(val_x_scaled).float()\n",
    "val_y = torch.Tensor(val_y).long()\n",
    "test_x_scaled = torch.Tensor(test_x_scaled).float()\n",
    "test_y = torch.Tensor(test_y).long()\n",
    "## データをテンソルデータセットインスタンスにする\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x_scaled, train_y)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x_scaled, val_y)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x_scaled, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV-NTZKhD1Mi"
   },
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aclNPCf9ZabB"
   },
   "outputs": [],
   "source": [
    "# 乱数シードの固定\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ニューラルネットワークを定義\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "  # 必要な層や活性化関数を定義する\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.l1 = torch.nn.Linear(train_x.shape[1], 128)     # 中間層1\n",
    "    self.a1 = torch.nn.ReLU()  # 活性化関数1\n",
    "    self.l2 = torch.nn.Linear(128, 128)   # 中間層2\n",
    "    self.a2 = torch.nn.ReLU()  # 活性化関数2\n",
    "    self.l3 = torch.nn.Linear(128, 3)     # 出力層\n",
    "\n",
    "  # 順伝搬を定義。引数のxは、説明変数。\n",
    "  # 順番に関数を実行し、その結果を次の関数に渡していく\n",
    "  def forward(self, x):\n",
    "    x = self.l1(x)\n",
    "    x = self.a1(x)\n",
    "    x = self.l2(x)\n",
    "    x = self.a2(x)\n",
    "    x = self.l3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOJjLVWbfE8b"
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R7izz7QsTcx"
   },
   "outputs": [],
   "source": [
    "num_epochs = 700\n",
    "\n",
    "# データローダーの用意\n",
    "# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n",
    "# shuffleをTrueに設定することで、データをシャッフルして取り出します(★batch_sizeをデータ量に合わせて32に設定)\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# モデルをインスタンス化\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001) # 最適化手法の用意\n",
    "criterion = torch.nn.CrossEntropyLoss() # ★誤差関数の用意(多値分類なので、交差エントロピー誤差を使う)\n",
    "\n",
    "## 学習時に経過情報を保存する空リストを作成\n",
    "train_loss_list = []      # 学習データの誤差関数用リスト\n",
    "val_loss_list = []        # 検証データの誤差関数用リスト\n",
    "\n",
    "# エポック分の繰り返し\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #学習の進行状況を表示\n",
    "    print('--------')\n",
    "    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "    # 損失の初期化\n",
    "    train_loss = 0        # 学習データの誤差関数\n",
    "    val_loss = 0          # 検証データの誤差関数\n",
    "    \n",
    "    #=====学習パート=======\n",
    "    # 学習モードに設定\n",
    "    # PyTorchでは学習時と評価時でモードを切り替える\n",
    "    net.train()\n",
    "\n",
    "    #ミニバッチごとにデータをロードして学習\n",
    "    for x, y in train_dataloader:\n",
    "        preds = net(x)                            # 順伝搬で予測を実行\n",
    "        loss = criterion(preds, y)                # 誤差関数を計算\n",
    "        optimizer.zero_grad()                     # 勾配を初期化\n",
    "        loss.backward()                           # 勾配を計算\n",
    "        optimizer.step()                          # パラメータ更新\n",
    "        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n",
    "    #ミニバッチの平均の損失を計算\n",
    "    batch_train_loss = train_loss / len(train_dataloader)\n",
    "    \n",
    "    #=====評価パート(検証データ)=======\n",
    "    # 評価モードに設定\n",
    "    net.eval()\n",
    "    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dataloader:\n",
    "            preds = net(x)                        # 順伝搬で予測を実行\n",
    "            loss = criterion(preds, y)            # 誤差関数を計算\n",
    "            val_loss += loss.item()               # ミニバッチごとの損失を格納    \n",
    "    #ミニバッチの平均の損失を計算\n",
    "    batch_val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    #エポックごとに損失を表示\n",
    "    print(\"Train_Loss: {:.4f}\".format(batch_train_loss))\n",
    "    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n",
    "    #損失をリスト化して保存\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    val_loss_list.append(batch_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5nX2NznfKXC"
   },
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvUNa3-5rjBM"
   },
   "outputs": [],
   "source": [
    "# この後精度改善のために何度か確認するので、関数化しておきます\n",
    "def myevaluete():\n",
    "\n",
    "  # 誤差関数の可視化\n",
    "  fig = plt.figure() # グラフの描画領域全体のオブジェクトを取得\n",
    "  fig.set_figheight(8) # 縦の幅を指定\n",
    "  fig.set_figwidth(12) # 横の幅を指定\n",
    "  plt.plot(train_loss_list, color='b', label='train_Loss')\n",
    "  plt.plot( val_loss_list, color='m', label='val_loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  net.eval() # モデルを評価モードにする\n",
    "\n",
    "  # テストデータ用のデータローダを用意\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "  y_pred = None\n",
    "  with torch.no_grad():\n",
    "      # ミニバッチで取り出しながら、最初のバッチはy_predに設定し、\n",
    "      # 2回目以降はそこへnumpy配列で接続していく\n",
    "      for inputs, labels in test_dataloader:\n",
    "          outputs = net(inputs)\n",
    "          if y_pred is None:\n",
    "              y_pred = outputs.data.numpy()\n",
    "          else:\n",
    "              y_pred = np.concatenate([y_pred, outputs.data.numpy()])\n",
    "\n",
    "  # np.argmaxで最も確率が高い値を0,1,2に変換する\n",
    "  accuracy = accuracy_score(test_y, np.argmax(y_pred, axis=1))\n",
    "  print('テストデータに対する予測精度：{}\\n'.format(accuracy))\n",
    "\n",
    "myevaluete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGESjKehZJbZ"
   },
   "source": [
    "# ニューラルネットワークモデルを改良する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKt3A2lQrTDM"
   },
   "outputs": [],
   "source": [
    "# Early-Stopping機能を実装したクラス\n",
    "class EarlyStopping:\n",
    "\n",
    "  def __init__(self, patience=0):\n",
    "    self._step = 0              # lossが改善しなかった連続回数をカウントする。先頭の_は内部的であることを表わしています\n",
    "    self._loss = float('inf')   # そこまでで最も改善が見られたlossの値を格納する\n",
    "    self.patience = patience    # 引数で指定する、何回改善されなかったら早期終了するかの回数\n",
    "\n",
    "  def __call__(self, loss):\n",
    "    if self._loss < loss:\n",
    "      self._step += 1   # lossが改善しなければ_stepを1増やす\n",
    "      if self._step >= self.patience:    # patienceの回数改善しなかったら早期終了する\n",
    "        print('early stopping')\n",
    "        return True\n",
    "    else:               # lossが改善した場合は_stepを0にして_lossを更新する\n",
    "      self._step = 0\n",
    "      self._loss = loss\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPAPSofarWkb"
   },
   "outputs": [],
   "source": [
    "# 乱数シードの固定\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ニューラルネットワークを定義\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "  # 必要な層や活性化関数を定義する\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.l1 = torch.nn.Linear(train_x.shape[1], 128)     # 中間層1\n",
    "    self.a1 = torch.nn.ReLU()             # 活性化関数1\n",
    "    self.d1 = torch.nn.Dropout(0.2)       # ★ドロップアウト層1★\n",
    "    self.l2 = torch.nn.Linear(128, 128)   # 中間層2\n",
    "    self.a2 = torch.nn.ReLU()             # 活性化関数2\n",
    "    self.d2 = torch.nn.Dropout(0.2)       # ★ドロップアウト層2★\n",
    "    self.l3 = torch.nn.Linear(128, 3)     # 出力層\n",
    "\n",
    "  # 順伝搬を定義。引数のxは、説明変数。\n",
    "  # 順番に関数を実行し、その結果を次の関数に渡していく\n",
    "  def forward(self, x):\n",
    "    x = self.l1(x)\n",
    "    x = self.a1(x)\n",
    "    x = self.d1(x) # ★ドロップアウト層1★\n",
    "    x = self.l2(x)\n",
    "    x = self.a2(x)\n",
    "    x = self.d2(x) # ★ドロップアウト層2★\n",
    "    x = self.l3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdJnAbAL3KV3"
   },
   "outputs": [],
   "source": [
    "num_epochs = 700\n",
    "\n",
    "# データローダーの用意\n",
    "# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n",
    "# shuffleをTrueに設定することで、データをシャッフルして取り出します\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ★早期終了のインスタンスを準備★\n",
    "es = EarlyStopping(patience=10)\n",
    "\n",
    "# モデルをインスタンス化\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01) # 最適化手法の用意\n",
    "criterion = torch.nn.CrossEntropyLoss() # ★誤差関数の用意(多値分類なので、交差エントロピー誤差を使う)\n",
    "\n",
    "## 学習時に経過情報を保存する空リストを作成\n",
    "train_loss_list = []      # 学習データの誤差関数用リスト\n",
    "val_loss_list = []        # 検証データの誤差関数用リスト\n",
    "\n",
    "# エポック分の繰り返し\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #学習の進行状況を表示\n",
    "    print('--------')\n",
    "    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "    # 損失の初期化\n",
    "    train_loss = 0        # 学習データの誤差関数\n",
    "    val_loss = 0          # 検証データの誤差関数\n",
    "    \n",
    "    #=====学習パート=======\n",
    "    # 学習モードに設定\n",
    "    # PyTorchでは学習時と評価時でモードを切り替える\n",
    "    net.train()\n",
    "\n",
    "    #ミニバッチごとにデータをロードして学習\n",
    "    for x, y in train_dataloader:\n",
    "        preds = net(x)                            # 順伝搬で予測を実行\n",
    "        loss = criterion(preds, y)                # 誤差関数を計算\n",
    "        optimizer.zero_grad()                     # 勾配を初期化\n",
    "        loss.backward()                           # 勾配を計算\n",
    "        optimizer.step()                          # パラメータ更新\n",
    "        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n",
    "    #ミニバッチの平均の損失を計算\n",
    "    batch_train_loss = train_loss / len(train_dataloader)\n",
    "    \n",
    "    #=====評価パート(検証データ)=======\n",
    "    # 評価モードに設定\n",
    "    net.eval()\n",
    "    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dataloader:\n",
    "            preds = net(x)                        # 順伝搬で予測を実行\n",
    "            loss = criterion(preds, y)            # 誤差関数を計算\n",
    "            val_loss += loss.item()               # ミニバッチごとの損失を格納    \n",
    "    #ミニバッチの平均の損失を計算\n",
    "    batch_val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    #エポックごとに損失を表示\n",
    "    print(\"Train_Loss: {:.4f}\".format(batch_train_loss))\n",
    "    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n",
    "    #損失をリスト化して保存\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    val_loss_list.append(batch_val_loss)\n",
    "\n",
    "    # ★早期終了判定★\n",
    "    if es(batch_val_loss):\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogecq8SSsVBo"
   },
   "outputs": [],
   "source": [
    "myevaluete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u17Oo46jc3yC"
   },
   "source": [
    "※ ニューラルネットワークの構成によっては、結果が97.2%もしくは100%になるかもしれません。これはテストデータを切り出す際に全体の20%で指定をすると36件のデータがテストデータとなり、そのうちの35件を正しく予測できると97.2%になるため、35件あてられたか、36件あてられたかの違いです。<br>\n",
    "分類問題の精度は正解率なので、0か1のやや極端な値です。ギリギリ正解だったのか、余裕で正解だったのかは考慮されていません。そういった関係から直感的にはわかりにくいものの、誤差関数のほうが評価指標として優先される場合もあります。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNknANJwxN9BoIzOnuX/vqc",
   "collapsed_sections": [],
   "name": "6-4_(演習)PyTorchによるワインの品種分類.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
