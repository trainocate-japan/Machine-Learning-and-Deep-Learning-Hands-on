{
 "cells": [
  {
   "id": "35b1cd1c",
   "cell_type": "markdown",
   "source": "<a target=\"_blank\" href=\"https://colab.research.google.com/github/trainocate-japan/Machine-Learning-and-Deep-Learning-Hands-on/blob/main/exercise/6_ディープラーニング/6-1_TensorFlow_Kerasによる自動車の燃費予測.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0Ic1oCRl4eU"
   },
   "source": "# 6-1_TensorFlow/Kerasによる自動車の燃費予測\nこのノートブックでは、TensorFlow / Kerasで回帰の予測モデルを作成します。<br>\n予測を行うテーマは1970年代後半から1980年台初めの自動車の燃費を予測することです。\n\n[Keras公式ドキュメント](https://keras.io/ja/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWo_M2swG3es"
   },
   "source": "## ライブラリのインポート"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Eqzg8G6S-ZBk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Collecting tensorflow\n  Downloading tensorflow-2.12.0-cp39-cp39-win_amd64.whl (1.9 kB)\nCollecting tensorflow-intel==2.12.0; platform_system == \"Windows\"\n  Downloading tensorflow_intel-2.12.0-cp39-cp39-win_amd64.whl (272.8 MB)\nCollecting tensorboard<2.13,>=2.12\n  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\nCollecting google-pasta>=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting astunparse>=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Downloading protobuf-4.22.1-cp39-cp39-win_amd64.whl (420 kB)\nCollecting libclang>=13.0.0\n  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\nCollecting wrapt<1.15,>=1.11.0\n  Using cached wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\nRequirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages (from tensorflow-intel==2.12.0; platform_system == \"Windows\"->tensorflow) (4.0.1)\nRequirement already satisfied: six>=1.12.0 in c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages (from tensorflow-intel==2.12.0; platform_system == \"Windows\"->tensorflow) (1.16.0)\nCollecting termcolor>=1.1.0\n  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\nRequirement already satisfied: packaging in c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages (from tensorflow-intel==2.12.0; platform_system == \"Windows\"->tensorflow) (21.3)\nCollecting tensorflow-estimator<2.13,>=2.12.0\n  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\nRequirement already satisfied: setuptools in c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages (from tensorflow-intel==2.12.0; platform_system == \"Windows\"->tensorflow) (49.2.1)\nCollecting flatbuffers>=2.0\n  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\nCollecting opt-einsum>=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages (from tensorflow-intel==2.12.0; platform_system == \"Windows\"->tensorflow) (1.22.2)\nCollecting keras<2.13,>=2.12.0\n  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\nCollecting gast<=0.4.0,>=0.2.1\n  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting h5py>=2.9.0\n  Downloading h5py-3.8.0-cp39-cp39-win_amd64.whl (2.6 MB)\nCollecting grpcio<2.0,>=1.24.3\n  Downloading grpcio-1.53.0-cp39-cp39-win_amd64.whl (4.0 MB)\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\nCollecting jax>=0.3.15\n  Downloading jax-0.4.8.tar.gz (1.2 MB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n    Preparing wheel metadata: started\n    Preparing wheel metadata: finished with status 'error'\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "    ERROR: Command errored out with exit status 1:\n     command: 'c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\scripts\\python.exe' 'c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\fujitama\\AppData\\Local\\Temp\\tmp_zv_ati5'\n         cwd: C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-install-z7d3q154\\jax\n    Complete output (13 lines):\n    Error in sitecustomize; set PYTHONVERBOSE for traceback:\n    SyntaxError: (unicode error) 'utf-8' codec can't decode byte 0x83 in position 0: invalid start byte (sitecustomize.py, line 7)\n    running dist_info\n    creating C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\n    writing C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\PKG-INFO\n    writing dependency_links to C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\dependency_links.txt\n    writing requirements to C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\requires.txt\n    writing top-level names to C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\top_level.txt\n    writing manifest file 'C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\SOURCES.txt'\n    reading manifest file 'C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\SOURCES.txt'\n    writing manifest file 'C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.egg-info\\SOURCES.txt'\n    creating 'C:\\Users\\fujitama\\AppData\\Local\\Temp\\pip-modern-metadata-0h9fk6d7\\jax.dist-info'\n    error: invalid command 'bdist_wheel'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: 'c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\scripts\\python.exe' 'c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\fujitama\\AppData\\Local\\Temp\\tmp_zv_ati5' Check the logs for full command output.\nWARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\nYou should consider upgrading via the 'c:\\users\\fujitama\\onedrive - トレノケート株式会社\\デスクトップ\\doc_mac0025g\\mlho\\_venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install tensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# TensorFlow/Kerasで使用\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": "# データを処理するための基本的なライブラリ\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# scikit-learnから必要なライブラリをインポート\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n!pip install tensorflow\n# TensorFlow/Kerasで使用\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping # 早期終了\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ-6ZutoLMUs"
   },
   "source": "Google Colaboratory上での出力のデフォルト設定"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH0i-1XNLEbe"
   },
   "outputs": [],
   "source": "# pandasのDataframeの出力\npd.set_option('display.max_columns', 500) # 表示列の最大\npd.set_option('display.max_rows', 500) # 表示行の最大\npd.set_option('display.unicode.east_asian_width', True) # 日本語出力時にヘッダのずれを解消\npd.options.display.float_format = '{:,.5f}'.format # 表示桁数の設定\n\n# ノートブックの表示桁数設定。この設定はprint文には作用せず、セルの最後に書いたものを出力する際に適用されます。\n%precision 3\n# numpy配列の指数表示禁止設定\nnp.set_printoptions(suppress=True)\n# numpy配列の表示桁数設定\nnp.set_printoptions(precision=3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8YM4qhJDxRA"
   },
   "source": "## データの準備\n今回使用するデータはUCI Machine Learning Repositoryから公開されているAuto MPG データセットのコピーです。<br>\ndownloaded from : https://archive.ics.uci.edu/ml/datasets/auto+mpg"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVdGz4qH40Bd"
   },
   "source": "#### データを取り込む\n- pandasのread_csvメソッドを使用して、mlho/data/auto_mpg.csvファイルを読み込みます\n- 読み込んだものは変数df_auto_mpgに代入します"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Id6S1tvV1jB5"
   },
   "outputs": [],
   "source": "# csvファイルを読み込みます\ndf_auto_mpg = pd.read_csv(\"/content/drive/MyDrive/mlho/data/auto_mpg.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzbVEi9b48VC"
   },
   "source": "#### データを確認する\n- MPG : 燃費（目的変数）\n- Cylinders : シリンダーの数\n- Displacement : 排気量\n- Horsepower : 馬力\n- Weight : 重量\n- Acceleration : 加速度\n- Model Year : モデル年\n- USA, Europe, Japan ： 生産国のOne-Hot表現"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yL_ybysA1-di"
   },
   "outputs": [],
   "source": "# 読み込んだデータを確認します\ndf_auto_mpg.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stmJSzwi4aNi"
   },
   "outputs": [],
   "source": "# df_auto_mpgのデータ要約を確認\ndf_auto_mpg.info()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owaePxYS4dOm"
   },
   "outputs": [],
   "source": "# df_auto_mpgの統計情報を確認\ndf_auto_mpg.describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhL3S_Yl2qM8"
   },
   "source": "今回のデータセットには、Horsepowerに値の含まれていない「欠損値」が含まれており、そのままではニューラルネットワークで学習をすることができません。<br>\n欠損値の補い方は、データによりいくつかの方法がありますが、今回は単純に削除します。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c6AszHW-yUV"
   },
   "outputs": [],
   "source": "# DataFrameのdropnaメソッドを使用して、欠損値の含まれる行を削除する\ndf_auto_mpg = df_auto_mpg.dropna()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biF93jpy-nre"
   },
   "outputs": [],
   "source": "# 再度、df_auto_mpgのデータ要約を確認\ndf_auto_mpg.info()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGUVHryfSJRb"
   },
   "source": "#### 説明変数と目的変数を切り出す"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBFUVORI31wK"
   },
   "outputs": [],
   "source": "# 目的変数にするMPG以外をすべて説明変数にする\nx = df_auto_mpg.drop(columns='MPG')\nx.tail(4)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvSJMzA1BAs3"
   },
   "outputs": [],
   "source": "# 目的変数はMPG\ny = df_auto_mpg[\"MPG\"]\ny.tail(4)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7tKbJFv5J-L"
   },
   "source": "#### データを訓練データと検証データに分割する"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPBJe7AwBorC"
   },
   "outputs": [],
   "source": "# 訓練データと検証データに分割（80%を訓練用に使用）\ntrain_x, val_x, train_y, val_y = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=5) "
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzqhkwc7e1JU"
   },
   "source": "### データのスケールを揃える\n回帰の場合はデータにより目的変数も標準化した方が良いケースがある為目的変数も標準化する。"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMcQInA3fBPF"
   },
   "source": "説明変数を標準化"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5RdUSMnEHgd"
   },
   "outputs": [],
   "source": "# 訓練データ説明変数の各列の平均を計算する\ntrain_x_mean = train_x.mean()\ntrain_x_mean.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihn7BIKVEkdG"
   },
   "outputs": [],
   "source": "# 訓練データ説明変数の各列の標準偏差を計算する\ntrain_x_std = train_x.std()\ntrain_x_std.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0l0nNthDPq6"
   },
   "outputs": [],
   "source": "# 訓練データ説明変数の標準化を行う\ntrain_x_scaled = (train_x - train_x_mean) / train_x_std\ntrain_x_scaled.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjDpypEEfFPD"
   },
   "source": "目的変数を標準化"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf_hxVjZIWR5"
   },
   "outputs": [],
   "source": "# 訓練データ目的変数の各列の平均を計算する\ntrain_y_mean = train_y.mean()\ntrain_y_mean"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9CzTgoaIWR-"
   },
   "outputs": [],
   "source": "# 訓練データ目的変数の各列の標準偏差を計算する\ntrain_y_std = train_y.std()\ntrain_y_std"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaaeGwfTIWR-"
   },
   "outputs": [],
   "source": "# 訓練データ目的変数の標準化を行う\ntrain_y_scaled = (train_y - train_y_mean) / train_y_std\ntrain_y_scaled.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztL5aPoDfH_n"
   },
   "source": "検証データを標準化"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIarRdO8eh5p"
   },
   "outputs": [],
   "source": "# 検証データ説明変数の標準化を行う\nval_x_scaled = (val_x - train_x_mean) / train_x_std\nval_x_scaled.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSLeXEcxejxA"
   },
   "outputs": [],
   "source": "# 検証データ目的変数の標準化を行う\nval_y_scaled = (val_y - train_y_mean) / train_y_std\nval_y_scaled.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV-NTZKhD1Mi"
   },
   "source": "## モデルの定義"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpEcp-U2DnxW"
   },
   "outputs": [],
   "source": "# ニューラルネットワークのモデルを定義する際に各パラメータの初期値が決定されます\n# その初期値が毎回異ならないように乱数シードをこのタイミングで固定します\ntf.random.set_seed(0)\n\n# モデルオブジェクトを用意し必要な層を追加していく\nmodel = Sequential()\n\n# 中間層1層目\nmodel.add(Dense(3, input_shape=(train_x.shape[1],)))\nmodel.add(Activation('relu'))\n# 中間層2層目\nmodel.add(Dense(3))\nmodel.add(Activation('relu'))\n# 出力層\nmodel.add(Dense(1)) # 回帰の場合は活性化関数なし（恒等関数）\n\n# 最適化手法としてAdam、誤差関数として平均二乗誤差を設定\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='mse')\n\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOJjLVWbfE8b"
   },
   "source": "## モデルの学習"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5neLKvaNVlW"
   },
   "outputs": [],
   "source": "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5nX2NznfKXC"
   },
   "source": "## 評価"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78E_U56bBGx4"
   },
   "source": "ニューラルネットワークの学習が順調に進んだかどうかを確認するには、エポックごとに誤差関数がどのように変化したかを確認することが有効です。\n\n訓練データに対する誤差関数と検証データに対する誤差関数を並べて表示し、二つを見比べることで誤差が順調に減少しているか、過学習を起こしていないか考察することができます。\n\nKerasではfitメソッドの戻り値のhistoryオブジェクトに学習の履歴が格納されています。今回はhistoryオブジェクトのhistoryプロパティにlossとval_lossだけが格納されているので、それを可視化してみましょう。\n\npanasのDataFrameにはmatplotlibをラッパーしたplotメソッドが用意されています。<br>\nDataFrameのplotメソッドを使用することで、DataFrameの中身を簡単にグラフ化することができます。<br>\n[DataFrameのplotメソッドAPIリファレンス](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ao1dA-2DZGFB"
   },
   "outputs": [],
   "source": "df_history = pd.DataFrame(history.history)\ndf_history.plot(figsize=(10, 6))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e6jTQGoRZEo"
   },
   "outputs": [],
   "source": "# 検証データを使用して予測精度を計算する\npred_val_y = model.predict(val_x_scaled)\nr2_score(val_y_scaled, pred_val_y)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGESjKehZJbZ"
   },
   "source": "## ニューラルネットワークモデルを改良する\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYmuSVc4iNBD"
   },
   "outputs": [],
   "source": "tf.random.set_seed(0)\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(train_x.shape[1],))) # ニューロン数を3から128に変更\nmodel.add(Activation('relu'))\nmodel.add(Dense(128)) # ニューロン数を3から128に変更\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='mse')\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ekynIlGiNBv"
   },
   "outputs": [],
   "source": "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzfmjKaea3Zv"
   },
   "outputs": [],
   "source": "df_history = pd.DataFrame(history.history)\ndf_history.plot(figsize=(10, 6))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqjd2ljxa3Zv"
   },
   "outputs": [],
   "source": "# 検証データを使用して予測精度を計算する\npred_val_y = model.predict(val_x_scaled)\nr2_score(val_y_scaled, pred_val_y)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4OTCgj5iJXw"
   },
   "source": "### 学習不足と過学習\nニューロンを増やした学習では、最初は順調に誤差が小さくなっていますが、検証データに対しては100エポックを少し過ぎたあたりから、改善するどころかどんどん誤差が大きくなっています。\n\nしかし、この付近ではニューロンが少ないモデルよりも検証データへの誤差が小さくなっています。\n\n最初のニューロンの少ないモデルは、データに対して十分に適合できておらず、まだ改善の余地を残している適合不足の状態です。対して、ニューロンを増やしたモデルでは、訓練データに適合しすぎて過剰適合の状態です。\n\nニューラルネットワークの改良では、適合不足でもなく過剰適合でもない丁度よい状態を見つけることが必要になります。\n\nまた、ニューラルネットワークは他のモデルに比べて学習に非常に多くの時間がかかります。そこで、学習を効率的に進めることも合わせて考慮する必要があります。\n\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF7W-8ynvk8E"
   },
   "source": "## 早期終了（Early Stopping）を導入する\n過学習が発生する一つの原因として、程よく訓練データに適合した状態を通り過ぎ、訓練データに過剰適合するまで学習を継続してしまったことがあります。\n\nそこで、検証データに対する誤差が大きくなる前に、早期終了(Early Stopping)を行う設定を追加します。\n\n早期終了は、学習時にfitメソッドの引数として、各エポック実行後に呼び出される、コールバック関数を設定することで実装できます。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvWkEEIPiwwi"
   },
   "outputs": [],
   "source": "# モデルには先ほど学習したパラメータがすでに設定されているので、\n# 学習状態をリセットするために再度モデルを定義します。モデルの内容は先ほどと変わりありません。\ntf.random.set_seed(0)\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(train_x.shape[1],)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='mse')\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOfqEURKlNHq"
   },
   "outputs": [],
   "source": "# EarlyStoppingの設定。patienceに指定した回数だけ連続で検証データの誤差が増えた場合に早期終了\nes = EarlyStopping(monitor='val_loss',\n                       patience=10,\n                       verbose=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMV6EmbqYlmG"
   },
   "outputs": [],
   "source": "# fitの引数にesを設定\nhistory = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1, callbacks=[es])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZqXcMMWbQ0v"
   },
   "outputs": [],
   "source": "df_history = pd.DataFrame(history.history)\ndf_history.plot(figsize=(10, 6))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDyuyAehbQ0w"
   },
   "outputs": [],
   "source": "# 検証データを使用して予測精度を計算する\npred_val_y = model.predict(val_x_scaled)\nr2_score(val_y_scaled, pred_val_y)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9fBewGBkDSo"
   },
   "source": "早期終了を入れると、学習にかかる時間も短くて済み、精度も向上しました。<br>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9qjS-1XpQR1"
   },
   "source": "## 追加の最適化を導入する\nニューラルネットワークのさらなる最適化を行い精度向上を図ります。\n今回は過学習を緩和する代表的な手法であるドロップアウトを適用してみます。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4-zUJs62Z6S"
   },
   "outputs": [],
   "source": "tf.random.set_seed(0)\nmodel = Sequential()\n# 中間層1\nmodel.add(Dense(128, input_shape=(train_x.shape[1],)))\nmodel.add(Activation('relu')) \nmodel.add(Dropout(0.2)) # ドロップアウトの追加\n# 中間層2\nmodel.add(Dense(128))\nmodel.add(Activation('relu')) \nmodel.add(Dropout(0.2)) # ドロップアウトの追加\n# 出力層\nmodel.add(Dense(1))\noptimizer = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='mse')\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW78Idxktkuy"
   },
   "outputs": [],
   "source": "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1, callbacks=[es]) # EarlyStoppingを設定"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaT8YVKBbh7M"
   },
   "outputs": [],
   "source": "df_history = pd.DataFrame(history.history)\ndf_history.plot(figsize=(10, 6))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxWyabEubh7M"
   },
   "outputs": [],
   "source": "# 検証データを使用して予測精度を計算する\npred_val_y = model.predict(val_x_scaled)\nr2_score(val_y_scaled, pred_val_y)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGZVuN_luGOB"
   },
   "source": "ドロップアウトは代表的な最適化手法ですが、扱うデータにより必ず精度が向上するわけではありません。各層のノード数や層数、学習率、ミニバッチのサイズ、などを調整することで、さらに精度が向上する可能性があります。"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlWYDAtpbnpg"
   },
   "source": "このノートブックは以上です。"
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMflwfrBjFQAj/vhEzLhB4g",
   "collapsed_sections": [],
   "mount_file_id": "1CRwlOp_Fnppk1rW-Md3T6DJqnCM2uFzA",
   "name": "6-1_TensorFlow_Kerasによる自動車の燃費予測.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}