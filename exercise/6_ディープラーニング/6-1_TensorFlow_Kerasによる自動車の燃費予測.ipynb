{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b1cd1c",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/trainocate-japan/Machine-Learning-and-Deep-Learning-Hands-on/blob/main/exercise/6_ディープラーニング/6-1_TensorFlow_Kerasによる自動車の燃費予測.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0Ic1oCRl4eU"
   },
   "source": [
    "# 6-1_TensorFlow/Kerasによる自動車の燃費予測\n",
    "このノートブックでは、TensorFlow / Kerasで回帰の予測モデルを作成します。<br>\n",
    "予測を行うテーマは1970年代後半から1980年台初めの自動車の燃費を予測することです。\n",
    "\n",
    "[Keras公式ドキュメント](https://keras.io/ja/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWo_M2swG3es"
   },
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eqzg8G6S-ZBk"
   },
   "outputs": [],
   "source": [
    "# データを処理するための基本的なライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learnから必要なライブラリをインポート\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# TensorFlow/Kerasで使用\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping # 早期終了\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8YM4qhJDxRA"
   },
   "source": [
    "## データの準備\n",
    "今回使用するデータはUCI Machine Learning Repositoryから公開されているAuto MPG データセットのコピーです。<br>\n",
    "downloaded from : https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVdGz4qH40Bd"
   },
   "source": [
    "#### データを取り込む\n",
    "- pandasのread_csvメソッドを使用して、mlho/data/auto_mpg.csvファイルを読み込みます\n",
    "- 読み込んだものは変数df_auto_mpgに代入します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Id6S1tvV1jB5"
   },
   "outputs": [],
   "source": [
    "# csvファイルを読み込みます\n",
    "df_auto_mpg = pd.read_csv(\"auto_mpg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzbVEi9b48VC"
   },
   "source": [
    "#### データを確認する\n",
    "- MPG : 燃費（目的変数）\n",
    "- Cylinders : シリンダーの数\n",
    "- Displacement : 排気量\n",
    "- Horsepower : 馬力\n",
    "- Weight : 重量\n",
    "- Acceleration : 加速度\n",
    "- Model Year : モデル年\n",
    "- USA, Europe, Japan ： 生産国のOne-Hot表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yL_ybysA1-di"
   },
   "outputs": [],
   "source": [
    "# 読み込んだデータを確認します\n",
    "df_auto_mpg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stmJSzwi4aNi"
   },
   "outputs": [],
   "source": [
    "# df_auto_mpgのデータ要約を確認\n",
    "df_auto_mpg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owaePxYS4dOm"
   },
   "outputs": [],
   "source": [
    "# df_auto_mpgの統計情報を確認\n",
    "df_auto_mpg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhL3S_Yl2qM8"
   },
   "source": [
    "今回のデータセットには、Horsepowerに値の含まれていない「欠損値」が含まれており、そのままではニューラルネットワークで学習をすることができません。<br>\n",
    "欠損値の補い方は、データによりいくつかの方法がありますが、今回は単純に削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c6AszHW-yUV"
   },
   "outputs": [],
   "source": [
    "# DataFrameのdropnaメソッドを使用して、欠損値の含まれる行を削除する\n",
    "df_auto_mpg = df_auto_mpg.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biF93jpy-nre"
   },
   "outputs": [],
   "source": [
    "# 再度、df_auto_mpgのデータ要約を確認\n",
    "df_auto_mpg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGUVHryfSJRb"
   },
   "source": [
    "#### 説明変数と目的変数を切り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBFUVORI31wK"
   },
   "outputs": [],
   "source": [
    "# 目的変数にするMPG以外をすべて説明変数にする\n",
    "x = df_auto_mpg.drop(columns='MPG')\n",
    "x.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvSJMzA1BAs3"
   },
   "outputs": [],
   "source": [
    "# 目的変数はMPG\n",
    "y = df_auto_mpg[\"MPG\"]\n",
    "y.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7tKbJFv5J-L"
   },
   "source": [
    "#### データを訓練データと検証データに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPBJe7AwBorC"
   },
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割（80%を訓練用に使用）\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzqhkwc7e1JU"
   },
   "source": [
    "### データのスケールを揃える\n",
    "回帰の場合はデータにより目的変数も標準化した方が良いケースがある為目的変数も標準化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMcQInA3fBPF"
   },
   "source": [
    "説明変数を標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5RdUSMnEHgd"
   },
   "outputs": [],
   "source": [
    "# 訓練データ説明変数の各列の平均を計算する\n",
    "train_x_mean = train_x.mean()\n",
    "train_x_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihn7BIKVEkdG"
   },
   "outputs": [],
   "source": [
    "# 訓練データ説明変数の各列の標準偏差を計算する\n",
    "train_x_std = train_x.std()\n",
    "train_x_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0l0nNthDPq6"
   },
   "outputs": [],
   "source": [
    "# 訓練データ説明変数の標準化を行う\n",
    "train_x_scaled = (train_x - train_x_mean) / train_x_std\n",
    "train_x_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjDpypEEfFPD"
   },
   "source": [
    "目的変数を標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf_hxVjZIWR5"
   },
   "outputs": [],
   "source": [
    "# 訓練データ目的変数の各列の平均を計算する\n",
    "train_y_mean = train_y.mean()\n",
    "train_y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9CzTgoaIWR-"
   },
   "outputs": [],
   "source": [
    "# 訓練データ目的変数の各列の標準偏差を計算する\n",
    "train_y_std = train_y.std()\n",
    "train_y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaaeGwfTIWR-"
   },
   "outputs": [],
   "source": [
    "# 訓練データ目的変数の標準化を行う\n",
    "train_y_scaled = (train_y - train_y_mean) / train_y_std\n",
    "train_y_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztL5aPoDfH_n"
   },
   "source": [
    "検証データを標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIarRdO8eh5p"
   },
   "outputs": [],
   "source": [
    "# 検証データ説明変数の標準化を行う\n",
    "val_x_scaled = (val_x - train_x_mean) / train_x_std\n",
    "val_x_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSLeXEcxejxA"
   },
   "outputs": [],
   "source": [
    "# 検証データ目的変数の標準化を行う\n",
    "val_y_scaled = (val_y - train_y_mean) / train_y_std\n",
    "val_y_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV-NTZKhD1Mi"
   },
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpEcp-U2DnxW"
   },
   "outputs": [],
   "source": [
    "# ニューラルネットワークのモデルを定義する際に各パラメータの初期値が決定されます\n",
    "# その初期値が毎回異ならないように乱数シードをこのタイミングで固定します\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# モデルオブジェクトを用意し必要な層を追加していく\n",
    "model = Sequential()\n",
    "\n",
    "# 中間層1層目\n",
    "model.add(Dense(3, input_shape=(train_x.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "# 中間層2層目\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('relu'))\n",
    "# 出力層\n",
    "model.add(Dense(1)) # 回帰の場合は活性化関数なし（恒等関数）\n",
    "\n",
    "# 最適化手法としてAdam、誤差関数として平均二乗誤差を設定\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOJjLVWbfE8b"
   },
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5neLKvaNVlW"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5nX2NznfKXC"
   },
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78E_U56bBGx4"
   },
   "source": [
    "ニューラルネットワークの学習が順調に進んだかどうかを確認するには、エポックごとに誤差関数がどのように変化したかを確認することが有効です。\n",
    "\n",
    "訓練データに対する誤差関数と検証データに対する誤差関数を並べて表示し、二つを見比べることで誤差が順調に減少しているか、過学習を起こしていないか考察することができます。\n",
    "\n",
    "Kerasではfitメソッドの戻り値のhistoryオブジェクトに学習の履歴が格納されています。今回はhistoryオブジェクトのhistoryプロパティにlossとval_lossだけが格納されているので、それを可視化してみましょう。\n",
    "\n",
    "panasのDataFrameにはmatplotlibをラッパーしたplotメソッドが用意されています。<br>\n",
    "DataFrameのplotメソッドを使用することで、DataFrameの中身を簡単にグラフ化することができます。<br>\n",
    "[DataFrameのplotメソッドAPIリファレンス](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ao1dA-2DZGFB"
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "df_history.plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e6jTQGoRZEo"
   },
   "outputs": [],
   "source": [
    "# 検証データを使用して予測精度を計算する\n",
    "pred_val_y = model.predict(val_x_scaled)\n",
    "r2_score(val_y_scaled, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGESjKehZJbZ"
   },
   "source": [
    "## ニューラルネットワークモデルを改良する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYmuSVc4iNBD"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(train_x.shape[1],))) # ニューロン数を3から128に変更\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128)) # ニューロン数を3から128に変更\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ekynIlGiNBv"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzfmjKaea3Zv"
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "df_history.plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqjd2ljxa3Zv"
   },
   "outputs": [],
   "source": [
    "# 検証データを使用して予測精度を計算する\n",
    "pred_val_y = model.predict(val_x_scaled)\n",
    "r2_score(val_y_scaled, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4OTCgj5iJXw"
   },
   "source": [
    "### 学習不足と過学習\n",
    "ニューロンを増やした学習では、最初は順調に誤差が小さくなっていますが、検証データに対しては100エポックを少し過ぎたあたりから、改善するどころかどんどん誤差が大きくなっています。\n",
    "\n",
    "しかし、この付近ではニューロンが少ないモデルよりも検証データへの誤差が小さくなっています。\n",
    "\n",
    "最初のニューロンの少ないモデルは、データに対して十分に適合できておらず、まだ改善の余地を残している適合不足の状態です。対して、ニューロンを増やしたモデルでは、訓練データに適合しすぎて過剰適合の状態です。\n",
    "\n",
    "ニューラルネットワークの改良では、適合不足でもなく過剰適合でもない丁度よい状態を見つけることが必要になります。\n",
    "\n",
    "また、ニューラルネットワークは他のモデルに比べて学習に非常に多くの時間がかかります。そこで、学習を効率的に進めることも合わせて考慮する必要があります。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF7W-8ynvk8E"
   },
   "source": [
    "## 早期終了（Early Stopping）を導入する\n",
    "過学習が発生する一つの原因として、程よく訓練データに適合した状態を通り過ぎ、訓練データに過剰適合するまで学習を継続してしまったことがあります。\n",
    "\n",
    "そこで、検証データに対する誤差が大きくなる前に、早期終了(Early Stopping)を行う設定を追加します。\n",
    "\n",
    "早期終了は、学習時にfitメソッドの引数として、各エポック実行後に呼び出される、コールバック関数を設定することで実装できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvWkEEIPiwwi"
   },
   "outputs": [],
   "source": [
    "# モデルには先ほど学習したパラメータがすでに設定されているので、\n",
    "# 学習状態をリセットするために再度モデルを定義します。モデルの内容は先ほどと変わりありません。\n",
    "tf.random.set_seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(train_x.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOfqEURKlNHq"
   },
   "outputs": [],
   "source": [
    "# EarlyStoppingの設定。patienceに指定した回数だけ連続で検証データの誤差が増えた場合に早期終了\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                       patience=10,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMV6EmbqYlmG"
   },
   "outputs": [],
   "source": [
    "# fitの引数にesを設定\n",
    "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZqXcMMWbQ0v"
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "df_history.plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDyuyAehbQ0w"
   },
   "outputs": [],
   "source": [
    "# 検証データを使用して予測精度を計算する\n",
    "pred_val_y = model.predict(val_x_scaled)\n",
    "r2_score(val_y_scaled, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9fBewGBkDSo"
   },
   "source": [
    "早期終了を入れると、学習にかかる時間も短くて済み、精度も向上しました。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9qjS-1XpQR1"
   },
   "source": [
    "## 追加の最適化を導入する\n",
    "ニューラルネットワークのさらなる最適化を行い精度向上を図ります。\n",
    "今回は過学習を緩和する代表的な手法であるドロップアウトを適用してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4-zUJs62Z6S"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "model = Sequential()\n",
    "# 中間層1\n",
    "model.add(Dense(128, input_shape=(train_x.shape[1],)))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.2)) # ドロップアウトの追加\n",
    "# 中間層2\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.2)) # ドロップアウトの追加\n",
    "# 出力層\n",
    "model.add(Dense(1))\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW78Idxktkuy"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x_scaled, train_y_scaled, batch_size=128, epochs=700, validation_data = (val_x_scaled, val_y_scaled), verbose=1, callbacks=[es]) # EarlyStoppingを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaT8YVKBbh7M"
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "df_history.plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxWyabEubh7M"
   },
   "outputs": [],
   "source": [
    "# 検証データを使用して予測精度を計算する\n",
    "pred_val_y = model.predict(val_x_scaled)\n",
    "r2_score(val_y_scaled, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGZVuN_luGOB"
   },
   "source": [
    "ドロップアウトは代表的な最適化手法ですが、扱うデータにより必ず精度が向上するわけではありません。各層のノード数や層数、学習率、ミニバッチのサイズ、などを調整することで、さらに精度が向上する可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlWYDAtpbnpg"
   },
   "source": [
    "このノートブックは以上です。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMflwfrBjFQAj/vhEzLhB4g",
   "collapsed_sections": [],
   "mount_file_id": "1CRwlOp_Fnppk1rW-Md3T6DJqnCM2uFzA",
   "name": "6-1_TensorFlow_Kerasによる自動車の燃費予測.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
