{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-2_Pytorchによる自動車の燃費予測.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkVeOogEDZMo5u1U+7M2pb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AwocY07Zn1wE"},"source":["# 6-2_PyTorchによる自動車の燃費予測\n","このノートブックでは、PyTorchで回帰の予測モデルを作成します。<br>\n","予測を行うテーマはKeras / TensorFlowと同じ、1970年代後半から1980年台初めの自動車の燃費を予測することです。"]},{"cell_type":"markdown","metadata":{"id":"frooxXj4k4Zv"},"source":["## ライブラリのインポート"]},{"cell_type":"code","metadata":{"id":"4S1U_mHYqzvk"},"source":["# データを処理するための基本的なライブラリ\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","# scikit-learnから必要なライブラリをインポート\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score # sklearnの決定係数を使って精度を算出する\n","\n","# データセットをダウンロードするためのライブラリ\n","import urllib.request\n","\n","# PyTorchで使用\n","import torch\n","# PyTorchのインポート設定は色々なスタイルがありますが、事前にインポートするとKerasに比べるて非常に多くのインポートを書く必要がある為、\n","# 今回は使用するたびに全て記載するスタイルをとります。"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZ-6ZutoLMUs"},"source":["Google Colaboratory上での出力のデフォルト設定"]},{"cell_type":"code","metadata":{"id":"tH0i-1XNLEbe"},"source":["# pandasのDataframeの出力\n","pd.set_option('display.max_columns', 500) # 表示列の最大\n","pd.set_option('display.max_rows', 500) # 表示行の最大\n","pd.set_option('display.unicode.east_asian_width', True) # 日本語出力時にヘッダのずれを解消\n","pd.options.display.float_format = '{:,.5f}'.format # 表示桁数の設定\n","\n","# ノートブックの表示桁数設定。この設定はprint文には作用せず、セルの最後に書いたものを出力する際に適用されます。\n","%precision 6\n","# numpy配列の指数表示禁止設定\n","np.set_printoptions(suppress=True)\n","# numpy配列の表示桁数設定\n","np.set_printoptions(precision=6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQRalXg8kzL5"},"source":["## データの準備\n","データの準備は、torchtensorへの変換以外、Keras / TensorFlowとほとんど同じです。一部異なる点として、データを訓練用（train_x、train_y）、検証用（val_x、val_y）、テスト用（test_x、test_y）にあらかじめ分けておきます。"]},{"cell_type":"code","metadata":{"id":"YbkpKaS1rFzv"},"source":["# ライブラリurllibを使用してデータをダウンロードする\n","# このデータセットはUCI Machine Learning Repositoryから公開されているAuto MPG データセットです\n","# https://archive.ics.uci.edu/ml/datasets/auto+mpg\n","\n","\n","URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n","dataset_path = \"auto-mpg.data\"\n","urllib.request.urlretrieve(URL, dataset_path)\n","dataset_path\n","\n","# データを読み込む\n","column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n","                'Acceleration', 'Model Year', 'Origin'] \n","mpg_dataset = pd.read_csv(dataset_path, names=column_names,\n","                      na_values = \"?\", comment='\\t',\n","                      sep=\" \", skipinitialspace=True)\n","\n","# 読み込んだデータを確認する\n","mpg_dataset.tail(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GqnJqZSrPTf"},"source":["# 欠損値の含まれる行を削除する\n","mpg_dataset = mpg_dataset.dropna()\n","\n","# originは産出国のカテゴリなので、ダミー変数化する。わかりやすくするために、列に名前を付ける\n","origin=mpg_dataset.pop('Origin') # pop('列名')でその列の値だけを取り出します\n","\n","mpg_dataset['USA'] = (origin == 1)*1.0 # booleanで値が帰ってきているため1.0を掛けてfloatにしています\n","mpg_dataset['Europe'] = (origin == 2)*1.0\n","mpg_dataset['Japan'] = (origin == 3)*1.0\n","\n","# 説明変数を取り出す\n","mpg_dataset_x = mpg_dataset.iloc[:,1:10]\n","mpg_dataset_x.tail(4)\n","\n","# 目的変数はMPG\n","mpg_dataset_y = mpg_dataset[\"MPG\"]\n","mpg_dataset_y.tail(4)\n","\n","# 訓練データとテストデータに分割（20%をテスト用に使用）\n","train_x, test_x, train_y, test_y = train_test_split(mpg_dataset_x, mpg_dataset_y, train_size=0.8, test_size=0.2, random_state=0) \n","# さらに訓練データを検証データに分割（訓練データの20%を検証用に使用）\n","train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, train_size=0.8, test_size=0.2, random_state=0) \n","\n","#トレーニングデータの標準化\n","\n","train_x_mean = train_x.mean(axis=0) # トレーニングデータ説明変数の平均\n","train_x_std = train_x.std(axis=0)  # トレーニングデータ説明変数の分散\n","\n","train_x_scaled = (train_x - train_x_mean) / train_x_std\n","\n","train_y_mean = train_y.mean(axis=0) # トレーニングデータ説明変数の平均\n","train_y_std = train_y.std(axis=0)  # トレーニングデータ説明変数の分散\n","\n","train_y_scaled = (train_y - train_y_mean) / train_y_std\n","\n","# 検証データの標準化\n","val_x_scaled = (val_x - train_x_mean) / train_x_std\n","val_y_scaled = (val_y - train_y_mean) / train_y_std\n","\n","# テストデータの標準化\n","test_x_scaled = (test_x - train_x_mean) / train_x_std\n","test_y_scaled = (test_y - train_y_mean) / train_y_std"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFs0k_b7ocDI"},"source":["#### データをtorchtensorに変換する\n","PyTorchで学習を行うためには、データをtorchtensor型に変換する必要があります。\n","pandasのDataFrameからは直接変換できないので、まず、numpy配列に変換してからtorchtensorへ変換します。"]},{"cell_type":"code","metadata":{"id":"fxAp2TlMuUYR"},"source":["# torchtensorに変換するためにまず、pandasのDataFrameからnumpy配列に変換しておく。\n","print(\"変換前\")\n","print(type(train_x_scaled))\n","print(train_x_scaled.dtypes)\n","print(type(train_y_scaled))\n","print(train_y_scaled.dtypes)\n","\n","# numpy配列へ変換\n","train_x_scaled = train_x_scaled.values\n","train_y_scaled = train_y_scaled.values\n","\n","# 変換後\n","print(type(train_x_scaled))\n","print(type(train_y_scaled))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLiXh1Z5RF3Z"},"source":["# 検証データとテストデータもnumpy配列へ変換\n","val_x_scaled = val_x_scaled.values\n","val_y_scaled = val_y_scaled.values\n","test_x_scaled = test_x_scaled.values\n","test_y_scaled = test_y_scaled.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6VCLmF-qloa"},"source":["## numpy配列をテンソルに変換\n","train_x_scaled = torch.Tensor(train_x_scaled).float()\n","train_y_scaled = torch.Tensor(train_y_scaled .reshape(-1, 1)).float()\n","val_x_scaled = torch.Tensor(val_x_scaled).float()\n","val_y_scaled = torch.Tensor(val_y_scaled .reshape(-1, 1)).float()\n","test_x_scaled = torch.Tensor(test_x_scaled).float()\n","test_y_scaled = torch.Tensor(test_y_scaled.reshape(-1, 1)).float()\n","## データをテンソルデータセットインスタンスにする\n","train_dataset = torch.utils.data.TensorDataset(train_x_scaled, train_y_scaled)\n","val_dataset = torch.utils.data.TensorDataset(val_x_scaled, val_y_scaled)\n","test_dataset = torch.utils.data.TensorDataset(test_x_scaled, test_y_scaled)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bo8NoGkNn5vj"},"source":["## モデルの定義\n","PyTorchのモデル構築は以下の手順で行います。\n","- torch.nn.Moduleを継承したクラスを定義する\n","- 作成したクラスにinitメソッドを定義し、モデルの構成を記述する\n","- 作成したクラスにforwardメソッドを定義し、順伝搬を記述する。"]},{"cell_type":"code","metadata":{"id":"r33EaLxOq5QD"},"source":["# 乱数シードの固定\n","torch.manual_seed(0)\n","\n","# ニューラルネットワークを定義\n","class Net(torch.nn.Module):\n","\n","  # 必要な層や活性化関数を定義する\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.l1 = torch.nn.Linear(train_x.shape[1], 3)     # 中間層1\n","    self.a1 = torch.nn.ReLU()  # 活性化関数1\n","    self.l2 = torch.nn.Linear(3, 3)   # 中間層2\n","    self.a2 = torch.nn.ReLU()  # 活性化関数2\n","    self.l3 = torch.nn.Linear(3, 1)     # 出力層\n","\n","  # 順伝搬を定義。引数のxは、説明変数。\n","  # 順番に関数を実行し、その結果を次の関数に渡していく\n","  def forward(self, x):\n","    x = self.l1(x)\n","    x = self.a1(x)\n","    x = self.l2(x)\n","    x = self.a2(x)\n","    x = self.l3(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFwqcq-On9rm"},"source":["## 学習\n","PyTorchの学習を定義・実行します。<br>\n","Kerasとは異なり、学習時の動作を詳細に定義できます（定義しなければいけません）\n","\n","学習を行いながら進行状況を出力するようにプログラミングすることが一般的です。"]},{"cell_type":"code","metadata":{"id":"6R7izz7QsTcx"},"source":["num_epochs = 700\n","\n","# データローダーの用意\n","# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n","# shuffleをTrueに設定することで、データをシャッフルして取り出します\n","batch_size = 128\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# モデルをインスタンス化\n","net = Net()\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.001) # 最適化手法の用意\n","criterion = torch.nn.MSELoss() # 誤差関数の用意。回帰なので平均２乗誤差を使う\n","\n","## 学習時に経過情報を保存する空リストを作成\n","train_loss_list = []      # 学習データの誤差関数用リスト\n","val_loss_list = []        # 検証データの誤差関数用リスト\n","\n","# エポック分の繰り返し\n","for epoch in range(num_epochs):\n","    \n","    #学習の進行状況を表示\n","    print('--------')\n","    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n","\n","    # 損失の初期化\n","    train_loss = 0        # 学習データの誤差\n","    val_loss = 0          # 検証データの誤差\n","    \n","    #=====学習パート=======\n","    # 学習モードに設定\n","    # PyTorchでは学習時と評価時でモードを切り替える\n","    net.train()\n","\n","    #ミニバッチごとにデータをロードして学習\n","    for x, y in train_dataloader:\n","        preds = net(x)                            # 順伝搬で予測を実行\n","        loss = criterion(preds, y)                # 誤差関数を計算\n","        optimizer.zero_grad()                     # 勾配を初期化\n","        loss.backward()                           # 勾配を計算\n","        optimizer.step()                          # パラメータ更新\n","        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n","    #ミニバッチの平均の損失を計算\n","    batch_train_loss = train_loss / len(train_dataloader)\n","    \n","    #=====評価パート(検証データ)=======\n","    # 評価モードに設定\n","    net.eval()\n","    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n","    with torch.no_grad():\n","        for x, y in val_dataloader:\n","            preds = net(x)                        # 順伝搬で予測を実行\n","            loss = criterion(preds, y)            # 誤差関数を計算\n","            val_loss += loss.item()               # ミニバッチごとの損失を備蓄    \n","    #ミニバッチの平均の損失を計算\n","    batch_val_loss = val_loss / len(val_dataloader)\n","    \n","    #エポックごとに損失を表示\n","    print(\"train_loss: {:.4f}\".format(batch_train_loss))\n","    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n","    #損失をリスト化して保存\n","    train_loss_list.append(batch_train_loss)\n","    val_loss_list.append(batch_val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePxTd6RMv67F"},"source":["## 評価"]},{"cell_type":"markdown","metadata":{"id":"jA8O9_mciQHK"},"source":["ニューラルネットワークの学習が順調に進んだかどうかを確認するには、エポックごとに誤差関数がどのように変化したかを確認することが有効です。\n","\n","訓練データに対する誤差関数と検証データに対する誤差関数を並べて表示し、二つを見比べることで誤差が順調に減少しているか、過学習を起こしていないか考察することができます。"]},{"cell_type":"code","metadata":{"id":"nLy1NFLsLolB"},"source":["# 誤差関数の可視化\n","fig = plt.figure() # グラフの描画領域全体のオブジェクトを取得\n","fig.set_figheight(8) # 縦の幅を指定\n","fig.set_figwidth(12) # 横の幅を指定\n","plt.plot(train_loss_list, color='b', label='train_Loss')\n","plt.plot( val_loss_list, color='m', label='val_loss')\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuVXXfCrxdPv"},"source":["net.eval() # モデルを評価モードにする\n","\n","# テストデータ用のデータローダを用意\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","y_pred = None\n","with torch.no_grad():\n","    # ミニバッチで取り出しながら、最初のバッチはy_predに設定し、\n","    # 2回目以降はそこへnumpy配列で接続していく\n","    for inputs, labels in test_dataloader:\n","        outputs = net(inputs)\n","        if y_pred is None:\n","            y_pred = outputs.data.numpy()\n","        else:\n","            y_pred = np.concatenate([y_pred, outputs.data.numpy()])\n","\n","y_pred = y_pred.reshape(-1)\n","\n","test_acc = r2_score(test_y_scaled, y_pred)\n","print('テストデータに対する予測精度：{:.6f}\\n'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FV7qqwlk2YUf"},"source":["# この後精度改善のために何度か確認するので、関数化しておきます\n","def myevaluete():\n","\n","  # 誤差関数の可視化\n","  fig = plt.figure() # グラフの描画領域全体のオブジェクトを取得\n","  fig.set_figheight(8) # 縦の幅を指定\n","  fig.set_figwidth(12) # 横の幅を指定\n","  plt.plot(train_loss_list, color='b', label='train_Loss')\n","  plt.plot( val_loss_list, color='m', label='val_loss')\n","  plt.xlabel('epoch')\n","  plt.ylabel('loss')\n","  plt.legend()\n","  plt.show()\n","\n","  net.eval() # モデルを評価モードにする\n","\n","  # テストデータ用のデータローダを用意\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","  y_pred = None\n","  with torch.no_grad():\n","      # ミニバッチで取り出しながら、最初のバッチはy_predに設定し、\n","      # 2回目以降はそこへnumpy配列で接続していく\n","      for inputs, labels in test_dataloader:\n","          outputs = net(inputs)\n","          if y_pred is None:\n","              y_pred = outputs.data.numpy()\n","          else:\n","              y_pred = np.concatenate([y_pred, outputs.data.numpy()])\n","\n","  y_pred = y_pred.reshape(-1)\n","\n","  test_acc = r2_score(test_y_scaled, y_pred)\n","  print('テストデータに対する予測精度：{:.6f}\\n'.format(test_acc))\n","\n","myevaluete()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FMLd-JERd158"},"source":["## ニューラルネットワークモデルを改良する"]},{"cell_type":"code","metadata":{"id":"iaSfT4VQeOEy"},"source":["# 乱数シードの固定\n","torch.manual_seed(0)\n","\n","# ニューラルネットワークを定義\n","class Net(torch.nn.Module):\n","\n","  # 必要な層や活性化関数を定義する\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.l1 = torch.nn.Linear(train_x.shape[1], 128)     # 中間層1\n","    self.a1 = torch.nn.ReLU()  # 活性化関数1\n","    self.l2 = torch.nn.Linear(128, 128)   # 中間層2\n","    self.a2 = torch.nn.ReLU()  # 活性化関数2\n","    self.l3 = torch.nn.Linear(128, 1)     # 出力層\n","\n","  # 順伝搬を定義。引数のxは、説明変数。\n","  # 順番に関数を実行し、その結果を次の関数に渡していく\n","  def forward(self, x):\n","    x = self.l1(x)\n","    x = self.a1(x)\n","    x = self.l2(x)\n","    x = self.a2(x)\n","    x = self.l3(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNjizazPeOE3"},"source":["num_epochs = 700\n","\n","# データローダーの用意\n","# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n","# shuffleをTrueに設定することで、データをシャッフルして取り出します\n","batch_size = 128\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# モデルをインスタンス化\n","net = Net()\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.001) # 最適化手法の用意\n","criterion = torch.nn.MSELoss() # 誤差関数の用意。回帰なので平均２乗誤差を使う\n","\n","## 学習時に経過情報を保存する空リストを作成\n","train_loss_list = []      # 学習データの誤差関数用リスト\n","val_loss_list = []        # 検証データの誤差関数用リスト\n","\n","# エポック分の繰り返し\n","for epoch in range(num_epochs):\n","    \n","    #学習の進行状況を表示\n","    print('--------')\n","    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n","\n","    # 損失の初期化\n","    train_loss = 0        # 学習データの誤差関数\n","    val_loss = 0          # 検証データの誤差関数\n","    \n","    #=====学習パート=======\n","    # 学習モードに設定\n","    # PyTorchでは学習時と評価時でモードを切り替える\n","    net.train()\n","\n","    #ミニバッチごとにデータをロードして学習\n","    for x, y in train_dataloader:\n","        preds = net(x)                            # 順伝搬で予測を実行\n","        loss = criterion(preds, y)                # 誤差関数を計算\n","        optimizer.zero_grad()                     # 勾配を初期化\n","        loss.backward()                           # 勾配を計算\n","        optimizer.step()                          # パラメータ更新\n","        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n","    #ミニバッチの平均の損失を計算\n","    batch_train_loss = train_loss / len(train_dataloader)\n","    \n","    #=====評価パート(検証データ)=======\n","    # 評価モードに設定\n","    net.eval()\n","    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n","    with torch.no_grad():\n","        for x, y in val_dataloader:\n","            preds = net(x)                        # 順伝搬で予測を実行\n","            loss = criterion(preds, y)            # 誤差関数を計算\n","            val_loss += loss.item()               # ミニバッチごとの損失を備蓄    \n","    #ミニバッチの平均の損失を計算\n","    batch_val_loss = val_loss / len(val_dataloader)\n","    \n","    #エポックごとに損失を表示\n","    print(\"train_loss: {:.4f}\".format(batch_train_loss))\n","    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n","    #損失をリスト化して保存\n","    train_loss_list.append(batch_train_loss)\n","    val_loss_list.append(batch_val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1PNh2EseWot"},"source":["myevaluete()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5MuVdVyij9x"},"source":["## 学習不足と過学習\n","ニューロンを増やした学習では、最初は順調に誤差が小さくなっていますが、検証データに対してはすぐに、改善するどころかどんどん誤差が大きくなっています。\n","\n","しかし、この付近ではニューロンが少ないモデルよりも検証データへの誤差が小さくなっています。\n","\n","最初のニューロンの少ないモデルは、まだデータに対して十分に適合できておらず、まだ改善の余地を残している適合不足の状態です。対して、ニューロンを増やしたモデルでは、訓練データに適合しすぎて過剰適合の状態です。\n","\n","ニューラルネットワークの改良では、適合不足でもなく過剰適合でもない丁度よい状態を見つけることが必要になります。\n","\n","また、ニューラルネットワークは他のモデルに比べて学習に非常に多くの時間がかかります。そこで、学習を効率的に進めることも合わせて考慮する必要があります。\n"]},{"cell_type":"markdown","metadata":{"id":"HfbkToSr1vHQ"},"source":["## 早期終了とドロップアウト\n","以下の最適化を行い、学習の効率化とさらなる精度の向上を図ります。\n","\n","- 早期終了\n","- ドロップアウト\n","\n","いずれの手法もどんな場合でも効果的とは限らず精度が悪化する場合もあります。（早期終了は有用でなくてもデメリットは少ない）\n","\n","PyTorchには標準で早期終了のための機能が用意されていないため、自作する必要があります。単純なパラメータのみで早期終了を行うためのプログラムを用意しましたので、今回はそれを使って早期終了機能を実装します。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"bHr4GqjkJZ5l"},"source":["# Early-Stopping機能を実装したクラス\n","class EarlyStopping:\n","\n","  def __init__(self, patience=0):\n","    self._step = 0              # lossが改善しなかった連続回数をカウントする。先頭の_は内部的であることを表わしています\n","    self._loss = float('inf')   # そこまでで最も改善が見られたlossの値を格納する\n","    self.patience = patience    # 引数で指定する、何回改善されなかったら早期終了するかの回数\n","\n","  def __call__(self, loss):\n","    if self._loss < loss:\n","      self._step += 1   # lossが改善しなければ_stepを1増やす\n","      if self._step >= self.patience:    # patienceの回数改善しなかったら早期終了する\n","        print('early stopping')\n","        return True\n","    else:               # lossが改善した場合は_stepを0にして_lossを更新する\n","      self._step = 0\n","      self._loss = loss\n","    return False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWU2sEgj2WwP"},"source":["# 乱数シードの固定\n","torch.manual_seed(0)\n","\n","# ニューラルネットワークを定義\n","class Net(torch.nn.Module):\n","\n","  # 必要な層や活性化関数を定義する\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.l1 = torch.nn.Linear(train_x.shape[1], 128)     # 中間層1\n","    self.a1 = torch.nn.ReLU()             # 活性化関数1\n","    self.d1 = torch.nn.Dropout(0.2)       # ★ドロップアウト層1★\n","    self.l2 = torch.nn.Linear(128, 128)   # 中間層2\n","    self.a2 = torch.nn.ReLU()             # 活性化関数2\n","    self.d2 = torch.nn.Dropout(0.2)       # ★ドロップアウト層2★\n","    self.l3 = torch.nn.Linear(128, 1)     # 出力層\n","\n","  # 順伝搬を定義。引数のxは、説明変数。\n","  # 順番に関数を実行し、その結果を次の関数に渡していく\n","  def forward(self, x):\n","    x = self.l1(x)\n","    x = self.a1(x)\n","    x = self.d1(x) # ★ドロップアウト層1★\n","    x = self.l2(x)\n","    x = self.a2(x)\n","    x = self.d2(x) # ★ドロップアウト層2★\n","    x = self.l3(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdJnAbAL3KV3"},"source":["num_epochs = 700\n","\n","# データローダーの用意\n","# PyTorchではデータローダーに格納されたデータセットをバッチサイズで区切って、学習のたびに取り出します。\n","# shuffleをTrueに設定することで、データをシャッフルして取り出します\n","batch_size = 128\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# ★早期終了のインスタンスを準備★\n","es = EarlyStopping(patience=10)\n","\n","# モデルをインスタンス化\n","net = Net()\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.01) # 最適化手法の用意\n","criterion = torch.nn.MSELoss() # 誤差関数の用意。回帰なので平均２乗誤差を使う\n","\n","## 学習時に経過情報を保存する空リストを作成\n","train_loss_list = []      # 学習データの誤差関数用リスト\n","val_loss_list = []        # 検証データの誤差関数用リスト\n","\n","# エポック分の繰り返し\n","for epoch in range(num_epochs):\n","    \n","    #学習の進行状況を表示\n","    print('--------')\n","    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n","\n","    # 損失の初期化\n","    train_loss = 0        # 学習データの誤差関数\n","    val_loss = 0          # 検証データの誤差関数\n","    \n","    #=====学習パート=======\n","    # 学習モードに設定\n","    # PyTorchでは学習時と評価時でモードを切り替える\n","    net.train()\n","\n","    #ミニバッチごとにデータをロードして学習\n","    for x, y in train_dataloader:\n","        preds = net(x)                            # 順伝搬で予測を実行\n","        loss = criterion(preds, y)                # 誤差関数を計算\n","        optimizer.zero_grad()                     # 勾配を初期化\n","        loss.backward()                           # 勾配を計算\n","        optimizer.step()                          # パラメータ更新\n","        train_loss += loss.data.numpy().tolist()  # ミニバッチごとの損失を格納   \n","    #ミニバッチの平均の損失を計算\n","    batch_train_loss = train_loss / len(train_dataloader)\n","    \n","    #=====評価パート(検証データ)=======\n","    # 評価モードに設定\n","    net.eval()\n","    # 評価時は勾配計算は不要なので、勾配計算を無効にして負荷を下げる\n","    with torch.no_grad():\n","        for x, y in val_dataloader:\n","            preds = net(x)                        # 順伝搬で予測を実行\n","            loss = criterion(preds, y)            # 誤差関数を計算\n","            val_loss += loss.item()               # ミニバッチごとの損失を格納    \n","    #ミニバッチの平均の損失を計算\n","    batch_val_loss = val_loss / len(val_dataloader)\n","    \n","    #エポックごとに損失を表示\n","    print(\"Train_Loss: {:.4f}\".format(batch_train_loss))\n","    print(\"val_loss: {:.4f}\".format(batch_val_loss))\n","    #損失をリスト化して保存\n","    train_loss_list.append(batch_train_loss)\n","    val_loss_list.append(batch_val_loss)\n","\n","    # ★早期終了判定★\n","    if es(batch_val_loss):\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzCgXwUf2OD_"},"source":["myevaluete()"],"execution_count":null,"outputs":[]}]}